<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchives%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[pandas 掏粪]]></title>
    <url>%2Farchives%2F4d0f29b5.html</url>
    <content type="text"><![CDATA[Pandas 掏粪技能 Jupyter notebook设置123456#显示所有列pd.set_option('display.max_columns', None)#显示所有行pd.set_option('display.max_rows', None)#设置value的显示长度为100，默认为50pd.set_option('max_colwidth',100)]]></content>
      <categories>
        <category>掏粪</category>
        <category>Pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pycharm 不以任何 test 形式运行]]></title>
    <url>%2Farchives%2F31f6a53f.html</url>
    <content type="text"><![CDATA[在 Pycharm 中，如果脚本名字含有 test，右键选择 run 时会默认选择 pytest 或 unittest 方式运行。 下面的方法关闭 test 模式，以 py 模式运行]]></content>
      <categories>
        <category>Bug</category>
        <category>Pycharm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python 中的 super() 函数]]></title>
    <url>%2Farchives%2F4aa4cdec.html</url>
    <content type="text"><![CDATA[Python 中的 super() 函数是用于子类调用父类的方法。在单继承和多重继承中常常用到。 任何类的继承是为了减少代码的复用。super() 是用来解决继承问题的，直接用类名调用父类方法在使用单继承的时候没问题。 单继承下面是常用到的子类单继承父类时的代码 123456789101112131415161718192021class FooParent(object): def __init__(self): self.parent = 'I\'m the parent.' print ('Parent') def bar(self,message): print ("%s from Parent" % message) class FooChild(FooParent): def __init__(self): super(FooChild,self).__init__() print ('Child') def bar(self,message): super(FooChild, self).bar(message) print ('Child bar fuction') print (self.parent) if __name__ == '__main__': fooChild = FooChild() fooChild.bar('HelloWorld') 程序输出结果： 12345ParentChildHelloWorld from ParentChild bar fuctionI'm the parent. super(FooChild,self) 首先找到 FooChild 的父类（就是类 FooParent），然后把类 FooChild 的对象转换为类 FooParent 的对象。参考来自网站。 多继承MRO 过程但是如果使用多继承，会涉及到查找顺序（MRO）、重复调用（钻石继承）等种种问题。 MRO 就是类的方法解析顺序表，其实也就是继承父类方法时的顺序表。具体的 MRO 算法可以参考博客。 1234567891011121314151617181920212223class Sneaky(object): sneaky = True def __init__(self, sneaky=True, *args, **kwargs): print('Sneaky running') super().__init__(*args, **kwargs) self.sneaky = sneakyclass Person(object): def __init__(self, human=True, *args, **kwargs): print('Person running') super().__init__(*args, **kwargs) self.human = humanclass Thief(Sneaky, Person): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) t = Thief()print(t.human)print(t.sneaky) 输出结果： 1234Sneaky runningPerson runningTrueTrue 当看到这段代码时很纳闷，为什么父类 Sneaky 和 Person 中也需要 super() 函数。 利用类的 __mro__ 方法可以来看继承顺序， 1print([i.__name__ for i in Thief.__mro__]) 输出 1[&apos;Thief&apos;, &apos;Sneaky&apos;, &apos;Person&apos;, &apos;object&apos;] 可以看到类的执行过程是 Thief -&gt; Sneaky -&gt; Person。 删除中间类的 super()当把 Sneaky 类中的 super().__init__(*args, **kwargs) 注释掉之后，改动如下时 1234567class Sneaky(object): sneaky = True def __init__(self, sneaky=True, *args, **kwargs): print('Sneaky running') # super().__init__(*args, **kwargs) self.sneaky = sneaky 输出结果 123456Traceback (most recent call last):Sneaky runningTrue File "C:/Data/BR/工作/Project/geotype/scripts/test_pys/test_super_init.py", line 27, in &lt;module&gt; print(t.human)AttributeError: 'Thief' object has no attribute 'human' Person running 不会打印，而且 human 属性不会被赋值， 删除末端类的 super()但是，当把 Person 类中的 super().__init__(*args, **kwargs) 注释掉之后， 12345class Person(object): def __init__(self, human=True, *args, **kwargs): print('Person running') # super().__init__(*args, **kwargs) self.human = human 代码仍可以正常输出。 解释在网站 中看到这样一段解释： So the kwargs dictionary we create and pack (with name=’badguy’ and weapon=’gun’), flows in to the first class in the MRO, and that class takes any applicable parameters out of that dictionary via its init function if applicable, then effectively passes what remains of kwargs (using the “super().init“ bit of code) to the next class in the MRO for that class’s init function, and so on. By the time kwargs gets to the object class, it’s empty, which is why an error is not thrown. 翻译过来： 沿 MRO 算法的路径进行传递时，super() 函数的作用将参数传递给下一个类的 init 函数，而在末端类的父类是 object ，是空的，所以不会抛出错误。 更好的理解123456789101112131415161718192021222324252627282930class A(): def __init__(self): print('enter A') print('leave A')class B(A): def __init__(self): print('enter B') super().__init__() print('leave B')class C(A): def __init__(self): print('enter C') super().__init__() print('leave C')class D(B, C): def __init__(self): print('enter D') super().__init__() print('leave D')print(D.__mro__)d = D() 在经典的菱形案例中， 123 ---&gt; B ---A --| |--&gt; D ---&gt; C --- 可以更好的理解 MRO 的传递过程 123456789(&lt;class '__main__.D'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class 'object'&gt;)enter Denter Benter Center Aleave Aleave Cleave Bleave D MIXINMIXIN 是一种组合的表现，在 python 是通过多继承来实现组合。 123456789101112131415161718192021222324252627282930313233343536373839class Document: def __init__(self, content): self.content = contentclass Word(Document): def format(self): self.content = 'i am word,my content is &#123;0&#125;'.format(self.content)class Excel(Document): def format(self): self.content = 'i am excel,my content is &#123;0&#125;'.format(self.content)class Monitor: def display(self): print('&#123;0&#125; on monitor'.format(self.content))class Printer: def display(self): print('&#123;0&#125; on printer'.format(self.content))class WordWithMonitor(Monitor, Word): passclass ExcelWithMonitor(Monitor, Excel): passclass WordWithPrinter(Printer, Word): passclass ExcelWithPrinter(Printer, Excel): pass 上述代码中，我们可以实现把 word 文档输出到显示器上、word 文档输出到打印机上，或者把 excel 文档输出到显示器、excel 文档输出到打印机上。 1234567wwm = WordWithMonitor('mix in')wwm.format()wwm.display()wwp = WordWithPrinter('mix in')wwp.format()wwp.display() 输出结果 12i am word,my content is mix in on monitori am word,my content is mix in on printer 通过继承不同的类组合，就可以实现不同的要求，中间的 Monitor 类和 Printer 类叫做 MIXIN 类，具有以下几个特征： 此类一般只包含方法，不包含数据 此类不能单独实例化，比如上边的Monitor类和Printer，类中没有content这个变量，所以不能单独实例化 此类一般只继承object类，或继承具有 MIXIN 类特性的类 __mro__ 可以看到类的执行过程 12print(([i.__name__ for i in WordWithMonitor.__mro__]))&gt;&gt; [&apos;WordWithMonitor&apos;, &apos;Monitor&apos;, &apos;Word&apos;, &apos;Document&apos;, &apos;object&apos;]]]></content>
      <categories>
        <category>Python</category>
        <category>Class</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pymysql 使用多行字符串报错]]></title>
    <url>%2Farchives%2Ff3cc21fc.html</url>
    <content type="text"><![CDATA[在使用 pymysql 操作数据库时，使用多行字符串作为 SQL 语句进行更新时，出现报错 版本原因1234567891011121314151617181920import pymysqlfrom insert_qc_report.cfg import PASSWDupdate_sql = """UPDATE `XXX` SET `COLUMN` = 'XXXXXXXXXXXXX' WHERE `RSID` = 'YYYYY'; UPDATE `XXX` SET `COLUMN` = 'XXXXXXXXXXXXX' WHERE `RSID` = 'YYYYY'; UPDATE `XXX` SET `COLUMN` = 'XXXXXXXXXXXXX' WHERE `RSID` = 'YYYYY';"""def connect(): connection = pymysql.connect(host='XXX', user='XXX', password=PASSWD, db='test', charset='utf8mb4') return connectionconnection = connect()with connection.cursor() as cursor: cursor.execute(update_sql)connection.commit() 报错内容如下 1pymysql.err.ProgrammingError: (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'UPDATE `XXX` SET `COLUMN` = 'XXXXXXXXXXXXX' at line 2") 原因： pymysql 版本太高导致 解决方法： 将 pymysql 版本从 0.9.2 降级到 0.7.9，在 python2 和 python3 中均不再报错。 语法错误如果在 INSERT 时遇到这个报错，需要检查 INOT 和 values 后的 () 最后一个元素是否多了 「，」。 1INSERT INTO `&#123;&#125;` (`polish_backgroud`, `change_tag`) VALUES (%s, %s)]]></content>
      <categories>
        <category>Bug</category>
        <category>sql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python2 与 Python3 的编码问题]]></title>
    <url>%2Farchives%2F183de7f1.html</url>
    <content type="text"><![CDATA[问题来源： 在 Python 3 中，使用 test = &#39;XXX&#39;.encode(&#39;utf-8&#39;) 后，将字符串 test 导入数据库后，是以 b&#39;XXX&#39; 的形式存储的，在 select 后会报错；而 Python 2 则以 &#39;XXX&#39; 形式存储，不会报错。 问题根源在于 Python 2 和 Python 3 对于 bytes 类型字符串的表现形式不一样。 Python 2 和 Python 3 中 str 类型的形式Python 2 中的 str 类型是 bytes 形式，所以 unicode 类型字符串以 u&#39;&#39; 形式存储； Python 3 中的 str 类型是 unicode 类型，所以 bytes 类型字符车以 b&#39;&#39; 形式存储。 unicode.encode(‘编码’) 和 bytes.decode(‘编码’)encode()encode() 的做法是将 unicode 字符串转换成 bytes 类型。 在 Python 3 中使用 encode() 得到 bytes 类型，也就是b&#39;&#39; 的形式。 12345678&gt;&gt;&gt; test = '工作'&gt;&gt;&gt; test.encode('utf-8')b'\xe5\xb7\xa5\xe4\xbd\x9c'&gt;&gt;&gt; test_2 = 'work'&gt;&gt;&gt; test_2.encode('utf-8')b'work'&gt;&gt;&gt; type(test_2.encode('utf-8'))&lt;class 'bytes'&gt; Python 2 中使用 encode() 得到 str 类型，也就是 &#39;&#39; 的形式 12345678&gt;&gt;&gt; test = u'工作'&gt;&gt;&gt; test.encode('utf-8')'\xe5\xb7\xa5\xe4\xbd\x9c'&gt;&gt;&gt; test_2 = 'work'&gt;&gt;&gt; test_2.encode('utf-8')'work'&gt;&gt;&gt; type(test_2.encode('utf-8'))&lt;type 'str'&gt; decode()decode() 做法将 bytes 字符串转换成 unicode 类型。 Python3 中，使用 decode() 将得到 unicode 字符串，也就是 str 类型 1234567&gt;&gt;&gt; test_decode = b'\xe5\xb7\xa5\xe4\xbd\x9c'&gt;&gt;&gt; test_decode.decode('utf-8')'工作'&gt;&gt;&gt; type(test_decode.decode('utf-8'))&lt;class 'str'&gt;&gt;&gt;&gt; b'work'.decode('utf-8')'work' Python2 中使用 decode() 将得到 unicode 字符串 1234&gt;&gt;&gt; '\xe5\xb7\xa5\xe4\xbd\x9c'.decode('utf-8')u'\u5de5\u4f5c'&gt;&gt;&gt; type('work'.decode('utf-8'))&lt;type 'unicode'&gt; 乱码出现使用 bytes.decode(&#39;编码方法&#39;) 时，当 bytes 的编码方式与编码方法不一致时，在解码时会出现乱码。 123456&gt;&gt;&gt; test='工作'&gt;&gt;&gt; a = test.encode('utf-8')&gt;&gt;&gt; ab'\xe5\xb7\xa5\xe4\xbd\x9c'&gt;&gt;&gt; a.decode('gbk')'宸ヤ綔' Windows 下的坑在 Windows 中文操作系统中，默认编码是 gbk，不是 utf-8，而且不同版本 Windows 系统的编码方式不同，这可能就是 Windows 坑的地方了。]]></content>
      <categories>
        <category>Python</category>
        <category>编码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10亮度无法调节]]></title>
    <url>%2Farchives%2F38aa7b8c.html</url>
    <content type="text"><![CDATA[使用「显示」中的亮度调节无法调节亮度时，可以采用下面的方法 先禁用再启用 Intel 的显示器设备即可]]></content>
      <categories>
        <category>Bug</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo deploy 时的报错]]></title>
    <url>%2Farchives%2Ffb81ec3d.html</url>
    <content type="text"><![CDATA[使用 hexo d -g 推送博客和使用 ssh -T git@github.com 验证 github 连接时，出现以下报错： 1mux_client_request_session: read from master failed: 解决方法： 将 ~/.ssh/ 下的文件删除，只保留 guojiao1.ppk，id_rsa，id_rsa.pub 文件，再重新进行验证。]]></content>
      <categories>
        <category>Bug</category>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库掏粪]]></title>
    <url>%2Farchives%2F675bc382.html</url>
    <content type="text"><![CDATA[数据库中的常用操作 操作与修改表新建增加表从同一 ip 下跨数据库复制12CREATE TABLE report_tbl LIKE vardb.report_tblINSERT report_tbl SELECT * FROM vardb.report_tbl 从文件中新建表1load data local infile "/home/light/mysql/gps1.txt" into table loadTable fields terminated by ',' lines terminated by "\n" (carflag, touchevent, opstatus,gpstime,gpslongitude,gpslatitude,gpsspeed,gpsorientation,gpsstatus); 增加列增加单列1alter table report_tbl add haplotype varchar(100) 默认时间为当前时间 1alter table report_tbl add column `insert_time` timestamp NULL DEFAULT CURRENT_TIMESTAMP; 增加多列1alter table msi add insert_time datetime, add file_time datetime, add qc enum('PASS', 'FAIL') 删除删除字段1alter table sample_tbl drop column msi_status 清空字段，不是删除1update t2 set status=null; 更新更新字段 需求：根据 Table1 的 sample_id 更新 Table2 的 status 字段 123456UPDATE Table2 , Table1 SET Table2.status = Table1.statusWHERE Table2.sample_id = Table1.sample_id 用 inner join 优化 1update Table2 inner join Table1 on Table2.sample_id=msi.sample_id set Table2.status = Table1.status 结果： 13 w 条记录优化后的速度快了 0.03s。 更新某个字段小数为百分数格式12345UPDATE report_tblSET af = concat(round(0.2484 * 100, 2), '%')WHERE Sample_ID = 'RS1823983CSF'AND pos = '55249091' 表查询显示所有字段信息1show full columns from teacher; 1desc teacher 用户管理显示当前用户权限1SHOW GRANTS For CURRENT_USER 存储过程显示所有存储过程1show procedure status; 查看存储过程或函数的创建代码12show create procedure your_proc_name;show create function your_proc_name;]]></content>
      <categories>
        <category>掏粪</category>
        <category>sql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一些生信基本概念]]></title>
    <url>%2Farchives%2F777791c6.html</url>
    <content type="text"><![CDATA[梳理一些生信的基本概念 SNV 与 SNP 的区别SNP SNP 是从群体的角度上出发，指出一个变异是否经常发生。 SNP 通常具有遗传特性，比如说一对等位基因的出现。 如果在超过 1% 群体中有这个变异，认为这个变异很常见。如果在极少数人身上发生，那可能是重要的变异位点。 A SNP does imply that there is an implication how often this variation occurs. This means that there is an inherit implication when someone refers to a SNP that this variation is common (at least 1% of population). This means that it isn’t just a fluke that we are seeing this variation, but is an observation seen more often than a few individuals. This implies there might be something important at that location. SNV SNV 指个体上一段 DNA 某个位置的碱基与其他个体不一样，并没有群体的概念，某些肿瘤的发生常常因为发生了单个碱基突变 The definition of SNV stands for single nucleotide variation, which means at one base there is a difference. This definition does not imply any kind of how often this variation occurs, just that there is at this particular nucleotide a difference. For example: ATG -&gt; CTG has a SNV at A -&gt; C 参考：https://www.quora.com/Genomics-What-is-the-difference-between-an-SNP-and-an-SNV]]></content>
      <categories>
        <category>生物信息学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EukRep 代码阅读]]></title>
    <url>%2Farchives%2F526c415b.html</url>
    <content type="text"><![CDATA[EukRep 是利用 kmer-freq 结合线性 SVM 分类模型对 input fasta 的原核或真核类型进行分类。代码地址见 https://github.com/patrickwest/EukRep。 代码结构十分清晰，每个函数作用简介明了，包括以下几点： 代码中有 setup.py 示例，已发布到 Pypi 利用 argparse 进行命令行参数解析以及 check_args。 自定义 Log 类记录标准输出和错误 利用 Biopython 中的 SeqIO.parse 解析 fasta 格式文件 利用 pkg_resource.resource_stream() 解析包中的数据文件，避免使用 open()，具体可以见 https://zhuanlan.zhihu.com/p/51377458 利用 pickle.load() 加载已经训练好的模型 结合 kpal.klib 中的 Profile 计算 kmer-freq EukRep_tests.py 有专门的测试模块和单元测试 是生信中不可多得的好代码]]></content>
      <categories>
        <category>Pretty Code</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tableau-数据可视化实战（四）]]></title>
    <url>%2Farchives%2F5a2d67dc.html</url>
    <content type="text"><![CDATA[Tableau 高级篇主要是地图绘制，高级图表类型，仪表盘和故事。 地图绘制和图像Tableau 支持使用什么数据来构建地图视图支持全球机场代码、城市、国家、地区、领地、州、省和一些邮政编码和二级行政区，也支持美国地区代码、核心基础统计区域（CBSA）、都市统计统计区域、国会选区。同时，如果有不支持的位置数据，可以自定义数据地理编码来创建地图视图。 自定义地理编码后，设置后图标会变成地球标记，度量中会自动产出经度和纬度，双击字段会产生地图。 出现未知位置的原因有两种，一是数据中的省市自治区名称不是 Tableau 要求的标准格式，二是调整后仍然不符合 Tableau 的格式要求，这时则需要给定经度/维度位置。 Tableau 中不能匹配“省/市/县”这种后缀的，比如「上海」可以识别，「上海市」就无法识别。 可以通过 GPSspg 查询经纬度。添加经纬度信息。 构建简单的地图视图调整大小和颜色 添加边界 添加背景颜色 进一步美化地图生成饼图标记 创建显示定量值的地图全球地震数据下载网站https://earthquake.usgs.gov/earthquakes/map/。 比例符号地图，可以对每个位置显示一个或两个定量值。一个用大小表示，一个用颜色表示。设置中心值可以通过颜色来区分哪些位于中心值以下，哪些位于中心值以上。 震级大的在上面显示，小的在下面显示 示例 1蜘蛛图：使用中心连接到周围的许多点时，显示一个起点和多个终点位置之间的路径。 列上如果有两个经度，就会出现两份地图。左侧会出现三个选项卡，一个控制总的，另外两个分别控制左右两个地图。 两个图合并到一起 添加筛选器 示例 2西雅图、华盛顿中的自行车共享数据 高级图表类型参考线、参考区间、参考分布在&lt;分析&gt;中拖动参考线，参考区间和参考分布，下图是针对表、区和单元格的区别。 表/区域和单元格的选择 参考线的置信区间 参考区间的情况 分布区间，下图表示出平均值的 25%、50% 和 75% 的区间 仪表盘和故事当需要呈现和分享可视化项时，甚至将可视化项排列成序列来为叙事提供支持，仪表盘和故事是分析和呈现数据的出色工具。 布局容器需要有工作表，然后新建仪表盘 调整布局容器通过拖拽的方式或者选中后显示出雪花状的图标。 原始表的修改会反映到仪表盘上，仪表盘上的修改也会反映到原始工作表上。 可以设置仪表盘格式。 可以通过拖动的方式调整布局，还可以拖动文本，网页等。拖动空白指加入一段空白，可以调整宽度和高度。 添加工作表双击或拖动 移除布局容器平铺和浮动仪表盘布局不同设备的预览效果 自动适应布局的做法： 工作表中选择整个视图，仪表盘大小选项选择「自动」 以平铺的方式添加工作表以浮动的方式添加工作表对浮动对象重新排序调整浮动对象的大小仪表盘的动作筛选动作 高亮显示动作URL 动作 故事用于创建、命名 和管理工作表和仪表盘的工作表。 仪表盘最佳实践https://onlinehelp.tableau.com/current/pro/desktop/zh-cn/dashboards_best_practices.html 目标了解目标和受众群体使用者最关注仪表盘的哪些数据针对现实世界设计添加交互功能以鼓励探索显示筛选器突出显示功能节省时间故事最佳实践]]></content>
      <categories>
        <category>可视化</category>
        <category>Tableau</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tableau-数据可视化实战（三）]]></title>
    <url>%2Farchives%2Fed007eb9.html</url>
    <content type="text"><![CDATA[Tableau 初级篇主要讲解排序与筛选，参数使用以及十大常用图表。 排序与筛选通过筛选器和参数，结合计算字段，做到与用户动态交互 排序排序是指依据字段来排序，包括排序依据和排序方式（升序和降序） 手工排序使用工具栏和工具提示进行排序 通过拖拽的方式进行排序 根据字段名称排序 计算排序使用自定义的规则进行排序 对订单日期字段进行排序 对分层结构进行排序，确定大类别产品中，哪些子类别销售额最高 筛选器（重点）度量提供的筛选器提供数字计算和比较，维度筛选器提供从列表中选择字符串或使用自定义值列表。 基本操作将字段拖到筛选器框中 度量筛选器 ​ 右击查看数据，验证筛选结果 维度筛选器 基本筛选器先创建未过滤的基本表，再创建筛选器；右击筛选器，选择显示筛选器，可以对筛选结果做互动筛选。 维度筛选器（维度字段） 包括基本文件类别，或具有大于或小于条件的逻辑表达式的数值过滤 度量筛选器（度量字段） 日期筛选器（日期字段） 通过相对于锚点的相对日期，绝对日期和日期范围进行筛选 上下文筛选器我们希望第二个筛选器只处理第一个筛选器返回的结果，更像是筛选器的组合。 通过设置分级查询加快加载速度，实际上是「并且」关系的查询 条件筛选器应用一些条件进行筛选 顶部筛选器返回 Top N 的结果 先确定类别，再在子类别中进行顶部筛选。 通配符筛选器通常应用在维度等字符串类型上，主要包括：包含、开头为、结尾为、精确匹配四种类型。 高级分析之参数创建和编辑参数参数是可在计算、筛选器和参考线中替换常量值的动态值。 可以选择从列表中取值，全部取值均可以及取值范围。选择列表代表参数只能从列表中取值；设置范围，通过控件控制步长。 右击参数，选择参数控件可以选择取值。 在计算中使用参数 在筛选器中使用参数对产品子类别应用动态筛选器（即配合参数），条件为：选择家具大类销售业绩最好的前 N 小类产品 使用参数使视图具有交互性使用参数时最重要的两个事项： 参数需要在计算字段中使用 需要显示参数控件，以便使用者能与之交互 十大常见图表（重点）条形（柱形）图条目多可以用条形图，条目少用柱形图。适用于枚举数据的比较，中小规模数据。 为堆叠条上方添加合计，通过添加参考线完成 折线图适合二维大规模数据集，一般用来反应趋势变化。横轴一般是日期字段。 设置双轴或混合轴 预测趋势线 饼图显示占比比例图，能够明确显示数据的比例情况，但是不会关注具体的数值。 散点图有两维数据是需要比较的，可以看出极值的分布情况，如果含有较多的点，散点图将是最佳的表现形式。 tableau 生成散点图至少需要一个度量。 文本表又叫交叉表或者透视表。适合于报表的场景。 可以通过表计算实现区域百分比。区（向下）代表占区域中的百分比。 热图 直方图用来显示分布形式的图表，连续度量的值分为范围或数据桶。 甘特图显示事件或活动的持续时间，每个标记代表一段持续时间。 订单日期和发货日期的持续时间，可以看出哪种邮寄方式的时间是比较长的 盒型图（箱式图）将数据解耦，取消勾选。 填充气泡图通常是一个维度和两个度量，大小和颜色分别代表一个度量。]]></content>
      <categories>
        <category>可视化</category>
        <category>Tableau</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tableau-数据可视化实战(二)]]></title>
    <url>%2Farchives%2F5b707d99.html</url>
    <content type="text"><![CDATA[本章主要讲解初级篇中的数据源深入，工作表，函数与计算。 Tableau 初级篇数据源深入基本操作超市.xls 文件中共有三张表，分别是订单表、退货表、销售人员表。订单表的信息如下： 选择数据源 选择维度和度量（数值类型），可以自己调整 应用可视化技术 数据连接方式 数据提取连结 把数据从数据源加载到本地，进行离线分析。 实时联结 与数据源同步，不会保存到本地，可以选择“立即更新”或“后继自动更新” 立即更新类似于更新；自动更新，只要数据库发生变化，就会自动更新。 数据多表联结同一个数据源中有多个字段，通过其中的一些字段组合成一张新的表。 在数据源界面分别双击两张表，就可以联结。对于重复的列，可以选择隐藏该列。 tableau 默认只能关联具有相同列名的两张表，如果两张表中的 id 字段不同，在关联时会出现感叹号 这里可以人为选择关联字段。 连结类型有内连结，左连结，右连结和并集联结。 三张表关联，再拖动另一张表进去即可，通过字段名称匹配。 数据混合数据可能存储在不同数据库中，比如关系型数据库，Excel 文件加载到一起进行分析。 可以先添加本地 excel 数据，然后再添加 mysql，填写 ip 地址，用户名和密码，选择数据库加载进来即可。 提取数据不需要数据源中的所有数据，像 SQL 数据库中的视图。避免数据传递，加载的内存消耗。 通过数据源中的筛选器，不断添加数据源来实现 工作表如何操作工作表添加、重命名、删除、重新排序和移动（鼠标拖动） Ctrl + S 保存工作表 编辑元数据abc 代表字符型，# 代表数值型，还有日期类型 可以更改数据类型 列的重命名和隐藏 在数据源和工作表中可以操作，可以勾选显示隐藏字段，显示成灰色字段。 列别名 对列里的值进行重命名，加强理解这一列的属性。 所有编辑元数据的操作都不会影响到原始数据。 字段操作组合两个字段合并两个或多个列 数据源中并没有合并到一起，但在工作表中是可以看到的。 可以删除，编辑合并字段。双击加入到右侧的视图。 合并字段后，原字段仍然存在 字段分层处理层次结构时用，比如大类别中包含小类别的情况。 能够随时向下钻取数据。 可以通过拖动或者选中加右键的方式创建。 ​ 从上到下依次代表大类到小类。 点击加号向下钻取 日期字段会自动分层 年/季度/月/日分层 字段分组 按文件夹分组，分门别类的存放和管理字段 在维度的空白位置右击 右键移除文件夹，取消分组。 按数据源分组，不同表代表不同的数据源 拆分字段将一个字段拆分为多个字段，比如将姓氏和名字分开。 数据源中 工作表中 保证需要拆分每一行的分隔符是相同的。 拆分后可以在数据源中重命名。 是否只选出拆分后的第一列 计算字段通过计算重新创建新的字段。tableau 可以配合公式来做，但是性能可能不够高。 可以拖动进去或使用 [字段] 代表字段的引用。 数据源中的标记多了一个 =，并且有了一个[计算]标记。 集集是根据某些条件定义数据子集的自定义字段，可以基于计算条件建立。 比如集成员对总额的贡献程度如何，如：至少购买 5k 元商品的销售额占总客户的比例 创建后出现交集的形式，加入到视图后，会出现内/外的形式。 内指集内的数据，外指集外的数据。 另一个用法是比较数据子集或者成批数据，例如，有多少客户在去年和今年购买过产品，或者购买过特定产品，是否购买其他产品？求两个集的交集或并集。 比如有多少客户在 2015 和 2016 年都购买过产品？ 选择 2015 年客户购买的产品集 通过筛选器筛选出来数据，在数据上右击，选择创建集 选择 2016 年客户购买的产品集 选择 [2015&amp;2016] 年客户购买的产品交集 结合筛选器进行图表显示 计数不同代表选出唯一的客户 如果要做 2015，2016，2017 三年的交集，需要拿 2017 客户与 （2015&amp;2016客户）做交集即可。 函数与计算运算符常规运算符+：添加两个数字，连接两个字符串，增加日期天数 -：减去两个数字，从日期中减去天数 算术运算符*/%^：乘/除/取余/指数运算 比较运算符=：等于，== 代表绝对相等（不常用）。 !=：不等于 &gt;：大于 &gt;=：大于等于 逻辑运算符AND/OR/NOT 优先级负号&gt;power&gt;*, /, %&gt;+, -&gt;=, !=, &gt; 运算符和表达式不区分大小写。 函数官方网站的函数列表。 数字函数CEILING(number)：向上取整 CEILING(2.145) = 3 FLOOR(number)：向下取整 POWER(5, 3)：指数函数 ROUND(数字，[小数])：控制小数点后的位数，会四舍五入。默认取到整数 DIV()：取除数 字符串函数LEN(string)：字符串长度 Contains(string, substring)：是否包含子字符串，返回布尔值，区分大小写 ENDSWITH(string, substring)：是否以子字符串作为结尾，会忽略空格；STARTSWITH(string， substring)： string 是否以 substring 开头的。返回布尔值。 FIND(string, substring, [start])：返回子串第一次出现的索引位置。索引从 1 开始，如果没有，返回 0。从 start 处开始查找。 LEFT(string, number)：返回最左侧 number 个字符串；RIGHT(string, number)，返回右侧起 number 个字符串 LOWER()UPPER()LEN() LTRIM()：移到左侧开始的空格；RTIRM()；TRIM() REPLACE(string, substring, replacement)：替换字符串 SPACE(number)：返回 number 个空格。&quot;A&quot;+SPACE(4)+&quot;B&quot; SPLIT(string, delimiter, token_number)：SPLIT(&quot;a-b-c-d&quot;, &quot;-&quot;, 2)=2, SPLIT(&quot;a-b-c-d&quot;, &quot;-&quot;, -2)，负号代表从最右侧开始取 日期函数date_part 是一个常量字符串参数，常使用的值为 “year”，”quarter”，”month”，”dayofyear”，”day”，”weekday”，”week”，”hour”，”minute”，”second”。 日期用前后的两个 # 括起来。 DATEADD(date_part, interval, date)，返回日期 date2-date1，DATEADD(&quot;month&quot;, 3, #04-13-2014#)= “2014/7/13 00:00:00” DATEDIFF(date_part, date1, date2, [start of week])，[start of week] 指定一周的开始是哪一天，默认是 “sunday”。 MAKEDATE(year, month, day)：组合日期；MAKEDATETIME() DATENAME(date_part, date, [start of week])：返回一个日期的 date_part。以字符串形式返回。date_part 是 Month，会返回 April 这种英文名称 DATEPART() 以整数形式返回 date 的 date_part。 DAY(date) 返回天数。YEAR(date) 返回年。MONTH(date) 返回月数。DAY(#2017.08.10#)=10。如果返回时/分/秒，可以使用 DATEPART() ISDATE(string) 判断是否是日期时间类型，支持多种格式。 NOW()返回当前时间；TODAY()返回当前日期 逻辑函数ISDATE() 判断是否为日期 ISNULL(expression) ：判断表达式中是否含有 NULL 值，ISNULL([profit]) 返回布尔值 IFNULL(expr1, expr2)：如果表达式 expr1 不为 NULL，返回该表达式，否则返回 expr2。例如，IFNULL([profit], 0) 将 profit 中的 NULL 值替换为 0。 IIF(test, then, else, [unknown])：检查条件如果为 True，返回一个值，如果为 Fasle，则返回另一个值。如果未知，返回第三个值或者 NULL。IIF([Profit] &gt; 0, &#39;Profit&#39;, &#39;Loss&#39;)。示例： 添加计算字段，添加字段后，在数据源中可以查看 IF &lt;expr&gt; THEN &lt;then&gt; [ELSEIF &lt;expr2&gt; THEN &lt;then2&gt;...] [ELSE &lt;else&gt;] END： CASE &lt;expression&gt; WHEN &lt;value1&gt; THEN &lt;return1&gt; WHEN &lt;value2&gt; THEN &lt;return2&gt; ... ELSE &lt;default return&gt; END： 聚合函数AVG(expression) 通常会跟一个字段。AVG([销售额]) COUNT(expression) 计数，不会去重，右键度量，会出现计数不同。 类型转换函数转换为特定数据类型。比如 STR()，DATE()，DATETIME()，INT()，FLOAT()。 DATE() 将字符串转换成日期格式 DATEPARSE(format, string) 将字符串按照某种格式转换成日期。YEAR(DATEPARSE(&quot;dd-MM-YYYY&quot;, &quot;30-12-2018&quot;))=2018，转换后就可以应用日期函数了。 计算字段函数的使用场景是什么？ 创建公式有 5 个元素，分别是函数（淡蓝色），字段（[]或者拖动方式，橙色），运算符（黑色），参数（紫色）和注释（绿色， 以 // 开头）。 在左下方会提示公式是否有效。 数值计算示例：研究不同邮寄方式的产品利润和折扣之间的差异；不同邮寄方式的聚合（AVG）计算字段。 字符串计算字符串计算，比如比较，连接和替换等。 包含”安”字城市的销售情况，结合筛选器选择出「真」的城市。 日期计算示例：每种商品的「订单日期」和「发货日期」的时间总间隔 表计算表计算可以理解为一种功能更强大的计算字段。 主要分为八种类型 差异计算 上图是每一年/季度/月的订单销售额情况。 如果更关注当前值与另一个值的差异，比如这个月比上个月增加了多少销售额。这种需求可以通过表计算来解决。 原始视图 差异计算：计算二月比一月销售额多了多少，三月比二月多了多少。B-A 横穿是横向比较，16 年 1 月与 15 年 1 月比较；向下是在列上比较，上一是只与上一个相比；第一个，只与第一个比。 编辑表计算，可以计算环比。 百分比差异计算 这个值与另一个值的百分比差异，(B-A)/A*100%。 选择百分比差异，调整成百分比格式 百分比计算 选择表计算中的「百分比」。B/A*100%。 总额百分比计算 A/(A+B)*100% 和 B/(A+B)*100%。 编辑表计算中，选择合并百分比。 排名计算 将每个值进行 RANK。选择排序，然后设置格式。下图不同的排序方式。降序指最高的数值排名第一。 百分位计算 选择分位数即可。默认情况下，最低值的百分位为 0，最高值的百分位为 100。也就是分位数。 汇总计算 选择汇总计算，选择计算类型 这里显示出的是各个月份销售额的累加。 可以在菜单中选择分析，合计，添加所有小计。 汇总计算和移动计算可以添加从属计算。比如计算基本年与上一年同期相比，每个月的累积差异百分比 移动计算 对当前值之前或之后的指定数目的值进行聚合（总和，平均值，最小值或最大值）,通常用于平滑短期数据波动，可以查看长期趋势。 查看每一年每一季度的平均销售额，而不是季度末那一个月的平均销售额。 上一个 2，下一个 0代表前两个数据加当前数据；当前值 勾选代表考虑当前月份数值；先向下再横穿 意思先向下遍历，再横向遍历，比如计算 2016 年 1 月数据时，会考虑 2015 年 11 月，2015 年 12 月和 2016 年 1 月三个月份的数据。 快速表计算为计算类型自动应用的表计算，可以理解为表计算的简化操作方式。 临时计算几种方式：双击字段，双击行或列的空白区域等实现公式和函数的计算。 多行公式计算：Shift + 回车键]]></content>
      <categories>
        <category>数据可视化</category>
        <category>Tableau</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tableau 数据可视化实战（一）]]></title>
    <url>%2Farchives%2Fd24f3c2e.html</url>
    <content type="text"><![CDATA[什么是数据可视化？本质上是设计一个最终符合业务逻辑需求的图表。 什么是数据可视化图表的作用主要是能够传达信息，关注重点，显示相关关系以及使信息表达更加生动。 如何用图表讲故事？从哪些方面去观察数据规律？主要从 3 个方面来看： 模式：即数据中的某种特定规律 关系：各影响因素直接的相关性，即各个图形间的关联 异常：即问题数据 不管图形需要表现的是什么，只要从模式、关系和异常去观察，就一定能观察到问题所在。 各个图表： 饼图：很快的观察出大致的比例，但是不关注具体的数据 Tableau 发展历程一款商业智能和可视化分析软件，帮助人们查看和理解数据。最初源于美国国防部项目，要求进一步提高人们数据分析能力，后来移交到斯坦福大学来做，并成立公司。它的诞生影响整个 BI 行业的重新构建，允许一线懂业务人员能够进行数据分析，而不是仅仅由专业技术人员实现，不需要强大的数学背景和代码能力。数据分析领域的龙头老大。 小数据用 excel，Power BI，大数据比如在 Hadoop 平台上，用 Tableau。 Tableau 家族产品 Tableau 桌面（重点学习） 几乎可以连接市面上所有的数据源 通过点击和拖拽的方式生成图表，报表，仪表盘和故事等 速度快，大数据量时延迟低 专业版可连接的数据源更多，可连接 Tableau server，区别于个人版 Tableau server 发布和管理 Tableau 制作的仪表盘 基于浏览器和 Web 的 BI 分析 快速，随时随地了解 Tableau 在线 基于 Tableau server 随时随地对数据查看和分析 Tableau 阅读器 阅读和浏览 Tableau 可视化产品 对工作簿数据进行筛选，过滤等 Tableau 公共场所 对外公开的共享的展示报表，图表等 Tableau server 和 Tableau desktop 是收费的，Tableau Reader 和 Tableau Public 是免费的。Tableau 社区版是免费，但是可连接的数据源有限，不能将文件保存在本地，只能保存和打开在 public 上的文件。 Tableau 的优势专门做大数据可视化的工具，PowerBI\SAS\SPSS 能实现的功能，Tableau 基本能实现。 什么都能连 平面数据、文件、关系型、非关系型、云端数据库的都能直连，不能直连的可以通过配置 odbc 来连接。支持 60 多种数据源。 学习门槛低，官方社区丰富 轻量级 简单的拖拽，智能推荐图形，快速的通过网页、邮件、服务器进行文件共享和发布。 花更少的时间清洗过滤数据，更多的时间在业务数据分析上 更智能 自动识别维度，经度，对维度打标，着色，自动求和，推荐展示方式，识别关联方式 亲切感 丰富的图表展示 理解和分析数据更重要，Tableau 可视化使得“人人都能成为数据分析师”。 Tableau 新手上路安装配置导航 维度和度量构成了数据源的所有列，度量是与数值有关的。 Tableau Public 只能保存到 Public 或者从 Public 上获取数据。 工作表 -&gt; 中的自动更新勾选后，如果连接到数据库，数据库中的数据更新时，图表会自动更新。 数据源：数据的来源 工作表：类似 excel 中的表 仪表盘：n 个工作表构成仪表盘 故事：包含仪表盘，文本等。 菜单中的分析是最主要用到的 格式菜单改变字体大小颜色等 服务器需要登陆服务器，将图表发布到服务器上，但是破解的专业版里没有这个菜单栏 设计流程需要知道如何设计出一个好的仪表盘 连接数据源 构建数据视图 视图更多是以图表的形式来展示 增强视图 通过使用过滤器，聚合，轴标签，颜色和边框的格式 创建工作表 创建不同张工作表，对相同或不同的数据源创建不同的视图 创建和组织仪表板 连接多个工作表，实现上下左右的互动等 创建故事 包含一系列工作表和仪表盘。让数据说话，实际就是在讲一个故事。 数据与文件类型数据每个数据分成四个类别，分别是 String，Number，Boolean，Datetime。加载数据源后，会自动分配数据类型。 String 数据类型：通常是用单引号或双引号括起来，如果想表达单引号或者双引号，那么只需要连续写两次就可以 Boolean 类型：True 和 False Date/Datetime 类型：如果需要强制将字符串识别为日期，那么需要在数据前面加一个 # 号，如 “# 3 March 2018”。但在公司里常用的前后各加一个 # 号，如 “10/28/2017 10:40:12”。 文件类型Tableau 的分析结果常常是 public 或者 server 上，也可以保存到本地，进行离线分析。 用到最多的是 .twb 和 .twbx ，分别代表工作簿和打包工作簿。其他的有 Tableau 数据源（tds），打包数据源（tdsx）和 数据提取（tde）。 twb 包含工作表和仪表盘的信息。 twbx 除了 twb 中的保存信息外，还保存了本地数据。可以在离线情况下对数据进行分析和处理。 数据源初探支持的数据源可达上百种。包括文件，关系系统，云系统，ODBC（微软提供的连接到其他数据源的接口）。 数据三大来源 实时连接（本地数据没有） 源数据发现变化时，会实时反应出源数据的变化情况。因为会负担源系统，这也是不利方面。 数据缓存 还可以将数据缓存到内存中来处理内存中的数据。根据内存的可用性，缓存的数据量将有限制。 组合数据源 组合来自不同数据源的数据，整合到一个数据源中，比如同时组合来自 excel，云端，数据库的数据 实战操作从 excel 中添加数据源]]></content>
      <categories>
        <category>可视化</category>
        <category>Tableau</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录一次面试经历 9k]]></title>
    <url>%2Farchives%2F8b2ffd6.html</url>
    <content type="text"><![CDATA[面试久谦咨询公司，提前在网上看了一下，有评价说这个公司的人都是铁打的，一天 24 小时都是工作时间，离职原因是“身体垮了”。面试约在 4.25 晚上 7 点开始，先是 DA 面，面了 90 分钟，后来说 DS 开会，需要等一下，等到 21:30，说一会要开会，明天再面，这个时间观念和工作强度，我已经提前感受到了。 笔试主要考察 pandas 的操作，比如 merge（DA 面又考察了两次），正则表达式，pandas 的基本操作，比如 append()，sort_values() 等等，其中有一个要求组合数的题目稍有难度。 笔试结束后，约在第二天晚上面试。 DA 面先是自我介绍，主要介绍毕业学校，项目经历，机器学习的经历。问了一下之前的专业，我介绍了一下转行业的经历。 Python 基本操作热身 pandas 中的 loc 和 iloc 的区别 这道题答上来了，是索引和值的区别 A = [1,2,3]，B=[3,4]，C=A.append(B)。问：A 和 C 分别等于什么？ 这道题 C 的取值答错了。A = [1,2,3,[3,4]]，C = None，C 是 append() 函数的返回值 None，这里想起来笔试中有一道，l1 = []，B = [l1.append(i) for i in range(3)]，问 B 等于什么？，也是考察 append() 返回值，B=[None, None, None]。 A = pd.Dataframe({&#39;x&#39;:[&#39;a&#39;] * 5})，B = pd.Dataframe({&#39;x&#39;: [&#39;a&#39;] * 4})，问 A.merge(B) 返回多少行？ 这道题答错，考察 merge，考察笛卡儿积，返回 4*5=20 行 set() 的交集，并集和差集的操作是什么？set 与 list 的区别是什么？ set 与 list 区别开始只答了 set 中元素不重复及 list 可以重复，以及 set 支持集合操作。后面面试官问有没有其他的了，又补充了一下查找时 $O(1)$ 和 $O(n)$ 复杂度的区别以及 set 分为可变类型和不可变类型，list 是可变类型，面完后在网上查了一下，set 不支持索引，list 支持索引和切片，以及一些常用方法的不同。 项目相关简历上的项目关于简历上的项目和每一句话，基本上每个都问到了，而且问的很详细很详细，即便是她不懂的基因组分析，也要问到很多细节出来，直到她确认这个项目真的是你在做，你在其中发挥的作用，以及里面的涉及的一些技术细节。感觉你要是编的项目，或者没有参与过的，真的会被问得哑口无言。 关于转录组分析流程，具体你是如何来实现的，以及都用了哪些技术？ 说了一下 Jinja 模板，JSON 配置文件，整个代码的结构等 别人是如何使用你这个流程的，后面谈到了流程中用到的分析软件，以及一些 R 包，问：Python 中是如何调用 R 的？说到最后，面试官还进行了一些总结，感觉她总结得比我讲得都明白。。 简历里谈到 GB 和 TB 级数据处理，是如何做到的？ 主要通过切分文件和遍历来做的。 为什么不用 spark？ 可能是集群不允许吧，因为集群的 qsub 结构还是 Sun 公司那套。（真的不清楚） 除了工作上的项目，还有其他项目可以谈谈嘛？ 说了一下匿名数据挖掘的项目 你是如何来做特征，找出哪些特征是有效的？ 从缺失值比例，方差阈值，训练集和测试集上特征是否同分布来筛选。 方差阈值如何确定的？ 我说当时就选了这个值。。 继续说做特征的过程，通过一些地理位置经纬度，找到对应的城市，对城市进行分级，发现这是个很重要的特征 如何发现这个特征有效的？ 通过在验证集上的表现，这里并没有用一些模型打分的方法（害怕当时问我模型打分的方法有哪些） 又继续说做了一些交叉特征，因为匿名，没办法通过业务理解构建新特征，根据在 label 上的表现差异来做的。 如何做模型验证的？ 交叉验证 讲讲交叉验证？ 划分数据集，在验证集上取平均 什么是过拟合和欠拟合？ 模型复杂度过高，学到了噪声，过拟合，表现为训练集上表现太好，测试集上不好且方差过大。 讲讲 Precision 和 recall？ 这个本以为是送分题，可惜变成送命题了。开始说 precision 考察所有样本的预测正确样本的比例，recall 指所有正例样本中的比例，刚开始没理清，说得不清楚，我想用混淆矩阵来表示一下的，面试官说，那你说一下混淆矩阵吧，我那时候突然就不起混淆矩阵了。怎么想不起 TP，FP，FN 和 TN 了，给自己挖坑了。 | | （预测结果）+ | - || ——————- | ——————- | —— || （实际结果）+ | TP | FN || - | FP | TN | Precison = TP/(TP+FP)\\ Recall = TP/(TP+FN) 然后又谈到之前切分文件的问题，切分文件之后做了什么工作？我又把从基因注释文件中提取 CDS，基因注释文件格式，后面做 KEGG 注释，Go 注释的过程以及 gff 文件去 overlap，感觉这块讲得太乱了，想到什么说什么，当时头已经有些懵了。 能不能再详细的讲讲你这些处理的过程 我就觉得这部分是要体现你的代码能力。就又把组装后的结果评估，lastz 比对结果中染色体上出现空白区域如何来做等等说了一下。 用到这么大的数据，如何用 Pandas 读入的？ 我说这些一般是非结构化数据，就是采取切分文件的方式来做，又详细问切分文件的方法和结果输出等等。。 业务理解面 这张表中，现在要求第一列进行降频，比如肯德基宅急送（大润发店），肯德基（沃尔玛店）都表示为肯德基，所有出现麦当劳的都归为麦当劳，表格共有几百万行，共有一千多个品牌。 我开始的回答是利用正则表达式进行匹配，那每一行都要遍历这一千品牌嘛？效率有点低，把这一列取 unique，再做正则，建立哈希，再 map 回去。可以不能循环这一千个品牌嘛？ 最后给了两个答案：1. 去除一些特有的表示地点的词语或许能做 2. 做分词，计算每个词向量的距离，再做聚类。 现在要求给这些商品打上食材标签，比如麦辣鸡块打上鸡块，西红柿炒蛋打上西红柿和鸡蛋，原味板烧鸡腿堡中署套餐打上鸡腿，薯条等，一个商品可能对应多个不同食材。 我开始想的方法，怕说错，一直没敢说，面试官一直鼓励， 就说了一下是否可以利用神经网络中已有的训练好的模型，直接找到对应的方法。 面试官给的答案是利用爬虫抓取网站上一些菜品和食材的对应关系，这是一种解决方法。 如果爬下来的数据要和 sku_name 这一列做 merge，应该用什么方法？ 开始说 leftjoin，面试官说，一个菜品对应不同的食材呢，会出现什么情况？ 后来仔细想了想，还是笛卡尔积产生数据爆炸的问题。 如果有一个大型连锁超市，在各地区都有分布，现在要做一些销售额的预测，需要做哪些分析？ 谈了一下人均消费水平，人口流量时间，单位时间出货量，客单价，客户等级与消费金额等等 如果要设计几张数据表，你该怎么来做？ 每说出一张表，面试官都会问这张表主键是什么？ 最后总结说出了四张表， 消费者个人信息，主键 customerid， 消费者订单，主键 orderid， 订单表，主键 customerid，itemid， 物品表 ，主键 itemid。 总结这次面试自己评分 65 分，有很多地方答得不好，基础也不是很扎实。但是一次很好的学习机会，也感觉到现在要进互联网做数据分析，要求真得变高了，需要你进去真正能干事的，而不是靠几个临时项目拼凑出的。这也是大环境不好的一种体现。 总结一下：Python 基础语法不扎实，项目描述比较混乱，业务理解不深。面试感觉不错，面试官很好，基本每个题都会告诉你答案和思路。 后面好好再过一下自己做过的项目和思路，熟悉再熟悉一些。做个导图，准备二面。]]></content>
      <categories>
        <category>机器学习</category>
        <category>面试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类算法笔记（四）--聚类算法的衡量指标]]></title>
    <url>%2Farchives%2Fdfe8b6.html</url>
    <content type="text"><![CDATA[在实际应用中，如何衡量一个聚类结果的好坏？ 聚类算法的衡量指标已知数据标签 均一性 完整性 V-measure 调整兰德系数（ARI） 调整互信息（AMI） 均一性和完整性有点像 Precision 和 Recall。V-measure 是对二者的折中。 ARI 和 AMI 本质上就是混淆矩阵的推广，二者都是取 1 最优，取 -1 最差。 可以参考 08 聚类算法 - 聚类算法的衡量指标 上述的这些指标（除轮廓系数外）的共性是要求有标签 y 的，但是聚类的使用场景一般是不知道标签是什么的，二者有矛盾。 一般的做法是，从中选出一小部分样本，人工打上标签，再使用 Kmeans/DBSCAN/谱聚类/AP 等方法聚类，根据衡量指标，选择较好的聚类方法，应用到完整的数据集上。 未知数据标签 轮廓系数 均方根标准误差 R 方 $Hubert \Tau$ 统计 轮廓系数不需要真实类别 y 就可以度量无监督学习好坏。 簇内不相似度 簇间不相似度 $b_i$ 等于样本 i 离其他簇平均距离的最小值。 样本 i 的轮廓系数 $s_i$ 越接近于 1，说明 $b(i)$ 越大，$a(i)$ 越小，样本 i 聚类越合理；$s_i$ 近似为 0，说明样本 i 在两个簇的边界上；$s_i$ 越接近于 -1，说明样本 i 越应该分类到另外的簇。 聚类的轮廓系数 均方根标准误差(RMSSTD) RMSSTD = (\frac{\sum_i\sum_{i\in I_j}||x_i-c_i||^2}{P\sum_i{(n_i-1)}})^\frac{1}{2}$c_i$ 是类簇的中心，$n_i$ 是类簇 i 的样本数量，P 是样本点的维度。可以看作是经过归一化后的标准差。RMSSTD 衡量了聚类结果的同质性，即紧凑程度，数值越小，聚类效果越好。 R 方 RS = \frac{\sum_{x\in D}||x-c||^2-\sum_i\sum_{i\in C_i}||x_i-c_i||^2}{\sum_{x\in D}||x-c||^2}分子上前半部分代表将数据集看作一个类簇的误差平方和，后半部分代表聚类后的误差平方和，R 方代表了聚类前后对应的误差平方和的改进程度，数值越大，聚类效果越好。 $Huber\Tau$ 统计 \Tau = \frac{2}{n(n-1)}\sum_{x\in D}\sum_{y\in D}d(x, y)d_{x\in C_i, y\in C_j}(c_i, c_j)$d(x, y)$ 表示两个点 $x$ 与 $y$ 之间的距离。$d_{x\in C_i, y\in C_j}(c_i, c_j)$ 代表 x 所属簇中心 $c_i$ 与 y 所属簇中心 $c_j$ 之间的距离，系数代表对所有 $(x, y)$ 点对的个数做了归一化处理。 理想情况下，$d(x, y)$ 越小，$d(c_i, c_j)$ 越小，$d(x, y)$ 越大，$d(c_i, c_j)$ 越大，所以， $\Tau$ 值越大，说明与样本的原始距离越吻合，聚类效果越好。]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习面试每日一问]]></title>
    <url>%2Farchives%2F409af206.html</url>
    <content type="text"><![CDATA[总结机器学习面试过程中的常见问题，主要来自于 Datawhale 学习群中的每日一问。 1. 逻辑回归与线性回归的区别与联系？区别： 一个是分类模型，一个是回归模型 自变量类型：线性回归要求自变量是连续的，逻辑回归要求自变量是离散的 因变量 y 的分布：一个基于正态分布的假设，一个基于伯努利分布的假设 逻辑回归最终得到的是属于某个类别的期望，线性回归得到的是某个预测值 损失函数：线性回归采用差的平方和，逻辑回归则是负对数似然 联系： 损失函数和梯度都是通过极大似然估计推导得出的 都属于广义线性模型 逻辑回归通过变换可以得到对数几率回归的形式，这也是逻辑回归称谓的来源。 求解参数时，都用梯度下降的形式 2. 什么是模型过拟合，请列举一下模型过拟合的原因及解决办法？过拟合是由于模型对训练集数据拟合过当，将噪声数据的特征也学习到，导致模型在训练集上的表现很好，在测试集上表现较差。 过拟合原因有以下几个方面：模型过于复杂，样本数量太少，特征多等原因 从定量角度来说，过拟合常常会使模型的方差过大 解决办法：1. 增加样本数量；2. 减少特征；3. 降低模型复杂度；4. 提高正则化系数；5. bagging 的方法。 3. 在 python 中，数组 list 和元组 tuple 的作用和区别是什么？区别： list 是可变类型，tuple 是不可变类型 list 支持动态变化的一些操作，比如 append()，pop()，insert() 等 tuple 可以作为字典的键 tuple 的存储占用的内存空间更少 tuple 在表示只有一个元素时，应使用 (1,) 避免歧义 联系： 都是容器类型对象 都支持索引，切片，迭代，连接（+）操作 4. 对特征进行挑选的方法有哪些？ Filter 对于连续变量，剔除方差太小的特征 剔除测试集与训练集分布不一致的特征，比如二分类特征的取值不一致 剔除特征之间相关性很强的特征 特征可解释性 PCA 降维 Wrapper 验证一个特征是否有效的方式之一是比较加入这个特征前后，模型的预测能力是否得到提高。可以根据模型预测的误差决定是否加入一个特征。 Embedding 主要分为两类，一是模型本身具有一些特征选择的功能，二是基于模型打分的方式 模型本身 L1 正则化，会将一些不重要的特征系数置 0 树类模型中根据gini 系数/信息增益/分裂次数/子节点样本数量等方式输出特征重要性 模型打分 permutation importance：改变特征中的数据排列，看它对模型预测的影响有多大 partial importance：持续改变某一个特征的值，观察它是如何影响预测结果的 SHAP value Null importance：制造一批随机噪声特征，将目标特征分数与随机特征分数做对比，如果不能明显超过随机特征分数，证明这个特征是个无用特征。 5. 机器学习中，为什么要经常对数据做【归一化】处理？一般适用于什么情形归一化的目的主要有两个，一是使各个特征的量纲一致，在涉及距离求解或线性变换的模型中提高精度；二是归一化可以使损失函数的等高线由椭圆形变成类圆形，加快梯度下降求最优解的速度。 涉及到归一化的模型有 KNN/logistic 回归/SVM/神经网络/PCA 等，树类模型不需要进行归一化。 6. 什么是逻辑回归算法，它的优缺点是什么？逻辑回归算法是一种处理分类问题的算法，在训练集上通过梯度下降算法得到最优参数，构建出模型之后。在面对一个新数据 $x$ 时，通过 y = \frac{1}{1+e^{-\theta x}}函数得到概率值，通过判断概率值是否大于 0.5，确定样本属于哪个分类。在处理多分类问题时，常常转换成 k 个二分类问题，在训练分类器时，将标签整理为属于第 i 类和不属于第 i 类。 优点： 简单，速度快，适合于二分类问题 可以直接查看各个特征的权重 能容易的更新模型，吸收新的数据 缺点： 对数据和场景的适应性有限，不如树类模型适应性强 7. 请简要说说一个完整机器学习项目的实践流程。（或者比赛项目） 在实际项目中，还有很重要的一步是项目解读，通过分析，把问题抽象成数学问题。 数据分析 是数据挖掘项目的必要环节，也是产出特征和验证特征的有效方式。 可以构造新的特征 可以验证特征的有效性 可以分析标签的原因 可以解释特征/模型的有效性 特征工程 特征编码：对原始数据字段进行编码 特征提取：从原始数据字段中提取特征 特征筛选：从特征集合中选择特征子集 模型训练 划分训练集和测试集 在训练集上学习模型和参数 在测试集上预测分数 在测试集上的得分是最有效的反馈 模型验证 HoldOut 检验：直接划分训练集和验证集进行评价 交叉验证：为消除样本划分随机性的影响，常用的有 K 折交叉验证和留一验证 自助法：进行 n 次有放回的采样，没有抽到的样本做验证集。 模型优化 常用的优化方法有 GridSearch/RandomSearch/基于贝叶斯的超参数调优方法 模型融合 常用的模型融合方法有 Voting，Averaging 和 Stacking 模型预测或上线运行 8. 在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为 10w 条数据,负样本只有 1w条数据。对于样本不均衡问题，怎么处理？（可以着重说下自己实践过的解决方案）一般处理样本不均衡的思路是上采样/下采样和 SMOTE 算法，工业界中常采取的方法是尽可能多地获取小类地数据。 在采样过程修改样本地权重，实际过程中做得比较少。常用的有以下几个方法 修改损失函数权重，使少数类别预测错误的损失大于多数类别预测错误的损失，使损失函数参数往少数类别偏； 大类欠采样的方法 Easy Ensemble：对大类数据进行不放回地采样，每个均建立一个模型，使用多个模型共同决策 Boosting：先通过对大类下采样产生训练集，利用 Adaboost 训练模型，对所有大类数据进行预测，从大类样本中删除预测正确地样本，重复上述两个步骤，直到大类样本数量和小类样本数量一致。 对大类样本进行聚类，根据聚类结果进行欠采样 针对大类样本，如果 k 近邻多数样本与自身类别不一样，那么删除这个样本 小类过采样 随机有放回地对小类抽样，可以加入一定地随机波动，这种方法容易过拟合 利用 SMOTE 差值算法生成新的样本 正负样本极不平衡时，可以看成异常检测问题，针对一分类建模，把所有不属于这个类别的数据看作异常值，常用的算法有 One Class SVM，IsolationForest 等。 9. 树模型需要对数据做归一化等预处理么？基于参数或距离计算的模型都需要归一化处理。 归一化目的是加快梯度下降速度，提高模型的收敛速度，同时，使各个特征维度对目标函数的权重影响是一致的，消除量纲不同引起的误差。在涉及距离或协方差计算时，使不同特征之间具有可比性，防止因为数值过大而特别注重某个特征。最优化问题中，为加快梯度下降速度，一般需要做归一化的，比如 GBDT/SVM/Linear Regression/GBDT/XGboost/Adaboost 等，涉及到距离计算的也需要归一化，比如 KNN/K-means 等。树模型属于概率模型，做的是筛选最优特征或决定当前特征是否是最佳分裂点，判定依据是概率分布或样本个数，不涉及距离，不需要做归一化。比如决策树或随机森林。 标准化和归一化一般都是对列向量进行操作的。 10. 什么是线性分类器和非线性分类器，有什么区别和优劣？线性分类器是指模型是参数的线性函数，分类平面强调的是一个平面或超平面，可以是一条直线，一个平面或一个超平面等。非线性分类器的分类平面则可以是曲面或者是多个超平面的组合。 常用的线性分类器：LR，SVM（线性核），LDA，朴素贝叶斯等； 常用的非线性分类器：决策树，KNN，SVM（高斯核），随机森林，GBDT，神经网络等 线性分类器的优缺点：编程简单，速度快，可解释性好，但是拟合能力较弱； 非线性分类器优缺点：拟合能力较强，但是计算复杂度高，容易出现过拟合。 11. 为什么朴素贝叶斯是线性分类器？假设一个二分类问题，判断一个样本 y=1 的依据是 \frac{P(y=1|X)}{P(y=0|X)}>1\\ =\frac{P(x|y=1)P(y=1)}{P(x|y=0)P(y=0)} > 1根据条件独立性假设，将 $P(x|y)$ 写成 $\prod _{j=0}^d P(x_j|y)$ 的形式，重新整理上面函数，最后可以得到 b + \sum_{i=1}^n w_j x_j \geq 0的形式。 具体可以参考 http://svivek.com/teaching/machine-learning/fall2018/slides/prob-learning/naive-bayes-linear.pdf 12. bias 和 variance 是什么？如何根据 bias 和variance 对模型进行分析和改善偏差是指通过采样得到多个训练集得到的模型输出平均值与真实值之间的偏差；方差则是多个采样得到的训练集模型输出的方差。 模型预测的偏差过大，往往由于模型复杂度过低或做出错误的假设导致的，可以通过 Boosting，使用复杂模型（添加神经网络层数，使用非线性模型），增加特征等 模型预测的方差过大，原因可能是模型复杂度过高，学到了一些噪声特征，导致模型预测结果不稳定，往往伴随着过拟合的发生，降低过拟合的方法有助于降低模型方差，比如 Bagging，增加训练样本，降低模型复杂度，增大正则化系数等 13. 什么是正则化、如何理解正则化以及正则化的作用？理解正则化最好的切入点是过拟合，当发生过拟合时，模型会拟合尽可能多的点，包括一些噪声和异常值，导致模型的参数过大，复杂度升高。 正则化的方法是给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。避免模型的参数过大，带来过拟合的风险。 常用的正则化方法有 L1 正则化和 L2 正则化，L1 正则化会使部分变量的参数为 0，达到选择特征的目的；L2 正则化只能尽可能减小参数值，使其尽量接近于 0。 14. 评判聚类好坏的指标是什么？如果知道样本标签时，可以采用均一性，完整性，V-measure，ARI 和 AMI 等进行评价。 如果未知样本标签的情况下，主要考察簇内样本的紧簇情况和簇间样本的分离情况。主要评价指标有轮廓系数，均方根标准偏差，R 方，$huber \Tau$ 统计等。 15. 当我们拿到数据进行建模时，如何选择更合适的算法？（机器学习or深度学习）说说你的经验之谈。首先需要明确的是分类，回归还是聚类问题。 如果拿到的数据是结构化数据，可以先拿 XBG 和 LGB 尝试一下，看看效果。 在其他特定的场景中，可以考虑一些特定的算法。比如在图像类，首选 CNN 或者 CNN 的变种；在时间序列分析中，可以考虑 ARIMA 时序分析，LSTM 和其他 RNN 的方法；在 CTR 预估的项目中，可以考虑深度学习或者矩阵分解模型。 16. 在求最优解的时候要求函数是凸函数？如何判断函数凸或非凸？（面试题系列）凸函数 $f(x)$ 满足不等式 f(\theta x+ (1-\theta) y) \leq \theta f(x) + (1-\theta) f(y)，0]]></content>
      <categories>
        <category>机器学习</category>
        <category>面试题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2Farchives%2F527e8d63.html</url>
    <content type="text"><![CDATA[在网上找到一篇很好的介绍 word2vec 的博客，但由于原博客中 Latex 公式显示不友好，特搬运到这里。 原博客链接彻底将word2vec拉下神坛。 word2vec是什么简单说： word2vec是一个工具。 word2vec是一个将词表示成向量的工具。 这从它的名字就能简单明了地看出来：从word（词）到（2是取英语to的谐音）vector（向量）。 word2vec在算法策略圈（无论技术还是产品都知道）很流行很轰动，原因有多方面： Google出品：品牌背书 非常实用：解决了很多实际的NLP问题，挖掘同义词 成本很小：百万词表用单机处理是不成问题的 上手很快：不用二次开发就能拿来即用 效果炫目：著名的vec(king) - vec(man) + vec(woman) = vec(queen)等式 预备知识到目前为止，这些问题都不涉及到细节。从现在开始，我开始回答一系列我猜测观众朋友可能会问的问题，从而（几乎）彻底消除大家对word2vec的神秘感。 向量是什么？好，如果看这篇文章还问这个问题，我只能说：“我的压力很大，但是我还是要尝试给你讲清楚。” 向量就是vector，当然这是废话。通常的解释是：向量是空间中的一个点。这个解释没有错，但是对于理解后面的文本算法可能有点障碍。我尝试稍微再解释得通用点，从而不给我后面的讲解添麻烦。 看下面这副刚刚出土的泼墨画： 这幅图示意了两个向量：（1, 2）和（3, 3, 3）。我们姑且给他们分别起个名字：a=(1,2)和b=(3,3,3)，a向量唯一确定了它在那个二维坐标系中的位置，一个点，独一无二：只要是这个向量就一定是在这里，只要在这个点这里就肯定是a，即使你给他起别的任何名字，它本质就是(1,2)这个点。同样(3,3,3)是在三维坐标系中的一个点。 但是我还是做得不够好，还是引入了“二维坐标系”和“三维坐标系”这样的名字，这两个名字就是前面那句“向量是空间中的一个点”中的“空间”。二维坐标系是平面，活在这里的都没有身高，脸大可能比较受欢迎，可以想象扑克牌上的卡通人物都活在二维坐标系中。我们人活在三维坐标系中，因为除了有平面坐标外，还有高度。 至此，总结： 向量是一个点； 不同空间下的向量没有比较的意义，我们常说“我们不是一个世界的人”，意思就是“我们不是同一个空间下的向量”。 说向量，叫说维度，前面说的二维，三维，就是“两个维度的空间”和“三个维度的空间”。在我的泼墨画中，我们也贴心地给维度取了名字： 在二维下，两个维度的名字分别是x和y，你取什么名字都行。 在三维下，三个维度的名字分别是x,y,z，你去什么名字都行。 带上维度名字后的向量a： 向量名字 维度x 维度y a 1 3 带上维度名字后的向量b： 向量名字 维度x 维度y 维度z b 3 3 3 所以说a这个向量因为在x和y这两个维度上都有确定的取值1和2，因此唯一确定了位置，b也是同样道理。接下里，我们可能要飞一下，请系好安全带。 要用数学来表示任何事物的存在，向量是最佳选择。 因为只有向量表示后，任何事物才可以被计算。 表示一篇文章（后面统称文档或者doc）的独一无二，用向量就是很自然的想法，那么根据“向量是空间中的点”这个定义，就需要解决两个问题： 空间是啥？以及在哪？ 如何确定它在空间中的位置？ 很遗憾，这两个问题有很多答案，也没有标准答案，很多大牛默默耕耘多年，目的就是尝试给出更好答案。这里距离说明一个常见的文档向量表示方法： unigram模型别被这个名字吓到了，你就乖乖听我说就行了。我们从前面的二维空间向量a类比过来： 我们这样来用向量表示一篇文档： 我们这个空间维度和词的数量一样，相同的词只算一个； 空间每个维度的名字都由文档中的词来承担； 每个维度上的取值就是词在文档中出现的次数。 比如这一篇文档1： Are you OK ? 不考虑标点符号，有三个词：Are you OK。那么我们选择的空间就是个三维空间，维度的名字分别叫Are， you， OK，这是规定，这样规定后，其他文档也在这个空间下，互相就可以一起玩耍了（后面再讲）。 这篇文档的向量就是： 向量名字 维度Are 维度you 维度OK 文档1 1 1 1 然后再来一篇文档2： I am OK. 同样表示成向量： 向量名字 维度I 维度am 维度OK 文档2 1 1 1 这两个文档的向量虽然都是三个维度的空间，但显然不在一个空间下，因为是不同的维度下，我们必须让两篇文档在一个空间下，才能做一些事情，比如计算文档相似度。方法是，互相补充缺少的维度，取值就是0： 向量名字 维度Are 维度you 维度OK 维度I 维度am 文档1 1 1 1 0 0 文档2 0 0 1 1 1 这样，两篇文档可以计算相似度了，计算方法就是余弦相似度，而余弦相似度背后还有一个知识，此刻要说出来，不然来不及了：点积（内积）是两个向量（当然是相同空间）最常见操作。 两个向量计算点积后会得到一个数值，这个数值可以直观理解为“反应了两个向量之间的距离”，注意，我很严谨，是“反应了”，而不是“就是”。点积是这样算的： 把两个向量在相同维度上的取值相乘 然后把这个乘积再加起来 用公式写一写，比较有些脑回路不一样的人看公式更容易看懂： \dot(a, b) = \sum_{i}{a_{i}b_{i}}\所谓余弦相似度，只是把这个点积归一化到[0,1]之间了，归一化的方式就是除以各自向量的长度。 \ cos(a, b) = \frac{dot(a,b)}{\sqrt{\sum_{i}{a_{i}^{2}}}\sqrt{\sum_{i}{b_{i}^{2}}}}向量的长度计算方式其实就是这个“点”到空间中“原点”的距离，想象一下前面泼墨画里面的二维向量的长度，就是 $\sqrt{1^2+2^2} = \sqrt{5}$ 一定要记住向量的点积运算，在机器学习里面处处可见。到现在，我们学会了把一个事物表示成一个向量的必要性和方法。 为什么要把词表示成向量？前面说到，任何事物都可以表示成向量，都要表示成向量才能计算。词为什么要表示成向量呢？好问题，那请看下面这个需求： 狗”和“狗狗”是相近意思吗？ “喵星人”和“猫咪”是相近意思吗？ 不要怀疑，他们就是相同意思，但是计算机可没那么多知识，计算机仅仅从字符匹配上看是不是同一个，所以在计算机看来“狗”和“狗狗”是不同的，”喵星人”和“猫咪”是不同的，除非…… 除非用相同的向量表示他们，这样再计算时，两个名字不同的事物其实就是相同的了。 把词表示成向量有很多办法。word2vec就是其中一个工具。表示成向量后，就可以进行下面的计算了： vec(king) - vec(man) + vec(woman) = vec(queen) 意思是：king, man, woman, queen这几个词在表示成向量后，也就表达了词的意思了，也就用数学方式计算出下面这个推理： king的意思中把其中有关性别的那些意思换成女性的话，那其实和queen是一个意思。 关键是怎么把一个词的向量算出来。这就是word2vec的任务了。 简陋版word2vec基于一个直觉：两个词，如果他们经常和相同的词一起出现，那么这两个词就很相似，也就是这两个词的向量很接近。word2vec把这个直觉用神经网络表达出来了。word2vec原始论文中，做了大量的工程优化，我们先抛去不看，从最简单的例子入手。这个例子里作如下假设： 只考虑上下文的一个词 词表很小 在这两天前提下，我们来实现word2vec中的CBOW模型，还有一个Skip gram模型，稍后再看。 模型结构为了实现word2vec，我们要搞清楚一些必要的数学运算，就是那部分要变成代码的数学运算。 现在明确以下，这个模型的任务是根据一个词前面/后面的词，预测当前这个词。模型的样子在下面这张图： 输入是一个词，输出是给整个词表每个词都计算一个概率，当然实际上输出只有一个词是正确的，那么那个正确的词的概率就要越大越好。 首先，把输入的这个词用one-hot编码方式表示成向量x，表示成的向量就是这个样子： 维度等于词表长度V 只有输入词编号位置是1，其他都是0 同样，输出词就是正确的label，也是one-hot编码而成的向量y。 然后，因为是神经网络，所以中间有一个隐藏层h，和通常的神经网络（深度神经网络更是如此）不同，这个隐藏层没有非线性激活函数。h有N个神经元，这个N就是最终给每个词学习到的向量长度，是一个超参数，所谓超参数就是开始计算只要需要人指定的参数。 h和x之间，形成了一个V*N形状的矩阵，标记为$ W_{V\times{N}}$。这个矩阵： 每一行是一个词的向量，我们称之为输入向量，也是word2vec的最终产出 隐藏层的输出就是$H{N\times{1}} = W{V\times{N}}^{T}X_{V\times{1}}$ 这里顺带提一句，如果自己编程实现机器学习模型，有一个小技巧去帮助理解：一定要搞对每一步的矩阵运算维度变化。 神经网络的隐藏层没有非线性激活函数，所以上面的H直接就要输出，输出为了给每一个词算一个概率，用到了softmax regression（理解为指数加权的轮盘赌）。从隐藏层到最终的概率输出，别看图中很简单（右半部分），中间其实经历了三步： $\mu{V\times{1}} = W^{\prime T}{N\times{V}}H_{N\times{1}}$ 对 $\mu$ 中每一个元素值都进行指数运算：$\mu{V\times{1}} = np.exp(\mu{V\times{1}})$，其中 np.exp 就是对一个向量每个元素都求指数 然后归一化，得到每个词的概率：$Y{V\times{1}} = \frac{\mu{V\times{1}}}{\sum{j}{\mu{j}}}$ 这里得到的每个词的概率就是模型的最终输出了。再提醒一遍：这里说的“模型的输出”是说的模型按照定义走完的计算流程最后一步，而我们需要的“输出”其实是这个模型的副产品：$W_{V\times{N}}$ ，不过，让我们先忘掉这个我们其实最终想拿到的副产品。 得到向量模型结构如前所述，接下来就是从数据中学到模型中的所有未知数，有两个地方需要学习：$W{V\times{N}}$ 和 $W^{\prime T}{N\times{V}}$。按照机器学习的套路，当然是先量化目标，即学到什么程度算是好？目标函数就是最大化输出词的预测概率（即最大似然）。下面涉及到的所有公式，都不是为了学术推导，而是为了编程实现，为了利用 numpy 这个矩阵运算Python 库，我们的目标是把所有的计算都用矩阵运算表示出来。 目标是最大化预测词概率： max p(W_{O}|W_{I}) = max y_{j^{* }} = max log y_{j^{* }}= \mu_{j^{* }} - log \sum_{j^{\prime}=1}^{V}{exp(\mu_{j^{\prime}})} := -E定义的损失函数就是 E ，最小化E，等价于最大化输出词的预测概率。 按照梯度下降优化方法，需要求出待学习参数的梯度即可，两大块，一个一个来。 更新输出矩阵隐藏层右边的矩阵（或称为“输出矩阵”）每个元素的梯度是： \frac{\partial{E}}{\partial{w_{ij}^{\prime}}} = \frac{\partial{E}}{\partial{\mu_{j}}}\frac{\partial{\mu_{j}}}{\partial{w_{ij}^{\prime}}}而其中： \frac{\partial{E}}{\partial{\mu_{j}}} = y_{j} - t_{j} := e_{j}第一部分偏导数表示为$e_{j}$，其中 $t_j$ 对于输出词是1，其他词是0。 \frac{\partial{\mu_{j}}}{\partial{w_{ij}^{\prime}}} = h_{i}这里说明了输出矩阵单个元素$w_{ij}^{\prime}$的梯度如何计算，用矩阵运算方式表示出该矩阵所有元素的梯度就是： \frac{\partial{E}}{\partial{W^{\prime}}} = H_{N\times{1}}(Y-T)_ {V\times{1}}^{T}其中$Y{V\times{1}}$ 和 $T{V\times{1}}$分别是：预测概率向量及输出词的 one-hot 编码向量，维度都是和词表长度一直，为 V。再次提醒，关注矩阵的下标，下标一定要对，我这里也特地标识出了每个矩阵的下标。 得到了梯度后，每次迭代更新$W^{\prime}_{N\times{V}}$ 的公式就是： W^{\prime}_{N\times{V}} = W^{\prime}_{N\times{V}} - \eta\frac{\partial{E}}{\partial{W^{\prime}}}更新输入矩阵隐藏层左边的矩阵（或称为“输入矩阵”）每个元素的梯度是： \frac{\partial{E}}{\partial{w_{ki}}} = \frac{\partial{E}}{\partial{h_{i}}}\frac{\partial{h_{i}}}{\partial{w_{ki}}}而其中： \frac{\partial{E}}{\partial{h_{i}}} = \sum_{j=1}^{V}{\frac{\partial{E}}{\partial{\mu_{j}}}\frac{\partial{\mu_{j}}}{\partial{h_{i}}}}不要被这一坨吓到了，遇到元素相乘再求和的，那就是向量的点积，计算矩阵的其中一个元素值用到点积，那显然是两个矩阵在运算，因此这一坨对应的矩阵运算就是： \frac{\partial{E}}{\partial{H}} = W^{\prime}_ {N\times{V}}(\frac{\partial{E}}{\partial{\mu}})_ {V\times{1}}再看： \frac{\partial{h_{i}}}{\partial{w_{ki}}} = x_{k}用矩阵运算表示就是： \frac{\partial{H}}{\partial{W}} = X_{V\times{1}}综合起来，输入矩阵的梯度计算就是： \frac{\partial{E}}{\partial{W}} = (\frac{\partial{H}}{\partial{W}})_ {V\times{1}}(\frac{\partial{E}}{\partial{H}})^{T}_ {N\times{1}}得到梯度后，每次迭代更新$W_{V\times{N}}$ 的公式就是： W_{V\times{N}} = W_{V\times{N}} - \eta\frac{\partial{E}}{\partial{W}}训练过程将所有计算表示成矩阵运算，有个好处就是用numpy写起来得心应手。接下来就是给训练过程准备数据，原始论文中为了提高训练速度，采用了随机梯度下降(SGD)训练模型，我们这里为了说明计算过程，所用语料是人为构造的，所以不适合随机梯度下降，直接用梯度下降，多轮迭代。 迭代之前，随机初始化两个矩阵，然后每轮迭代过程就是三步： 1. 计算输出概率 2. 计算输出矩阵的梯度，更新输出矩阵参数 3. 计算输入矩阵的梯度，更新输入矩阵参数 我们把原始语料库拆解成词对： (左边词，当前词) (右边词，当前词) 然后构造成训练样本。 Python实现读入语料库： 12345678910111213141516171819202122232425262728import numpy as npimport scipy as spimport argparsefrom feature_tools import FeatureIndeximport mathdef read_corpus(corpusfile, window = 1): vocabulary = FeatureIndex() traindata = [] with open(corpusfile) as corpusinput: for line in corpusinput: words = line.strip().split() wordid = [] for word in words: index = vocabulary[word] if index &gt;= 0: wordid.append(index) lastword = -1 for i in range(len(wordid)): # 以当前词作为预测目标，前一个词作为输入构造一条样本 if i &gt; 0: traindata.append((wordid[i - 1], wordid[i])) # 以当前词作为预测目标，下一个词作为输入构造一条样本 if i &lt; len(wordid) - 1: traindata.append((wordid[i + 1], wordid[i])) return vocabulary, traindata 初始化两个矩阵： 12345init_vectors(V, N):input_vectors = np.random.rand(V, N)output_vectors = np.random.rand(N, V)return input_vectors, output_vectors 保存我们想要的副产品（输入矩阵）： 1234def save_vectors(input_vectors, vectorfile): with open(vectorfile, &apos;w&apos;) as vectorfileoutput: for row in range(input_vectors.shape[0]): vectorfileoutput.write(&quot;%s\n&quot; % (&quot; &quot;.join([str(s) for s in input_vectors[row]]))) 计算损失函数值： 123456789101112131415def compute_loss(V, traindata, input_vectors, output_vectors): loss = 0.0 for pair in traindata: context_word_onehot = np.zeros((V, 1)) context_word_onehot[pair[0]] = 1 output_word_onehot = np.zeros((V, 1)) output_word_onehot[pair[1]] = 1 hidden_output = np.dot(np.transpose(input_vectors), context_word_onehot) output_log_prob = np.dot(np.transpose(output_vectors), hidden_output) exp_log_prob = np.exp(output_log_prob) sum_exp_prob = np.sum(exp_log_prob) output_word_prob = exp_log_prob / sum_exp_prob prob = output_word_prob[pair[1]] loss += math.log(1.0/prob) return loss 主流程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061def main(corpusfile, vectorfile, vocabulary_file, N, learning_rate, epoch): vocabulary, traindata = read_corpus(corpusfile) input_vectors, output_vectors = init_vectors(len(vocabulary), N) for t in range(epoch): print(&apos;epoch = %s, loss = %f&apos; % (t, compute_loss(len(vocabulary), traindata, input_vectors, output_vectors))) for pair in traindata: # predict context_word_onehot = np.zeros((len(vocabulary), 1)) context_word_onehot[pair[0]] = 1 #print(&quot;context_word_onehot shape = (%s, %s)&quot; % context_word_onehot.shape) output_word_onehot = np.zeros((len(vocabulary), 1)) output_word_onehot[pair[1]] = 1 #print(&quot;output_word_onehot shape = (%s, %s)&quot; % output_word_onehot.shape) #print(&quot;input_vector shape = (%s, %s)&quot; % input_vectors.shape) #print(&quot;output_vector shape = (%s, %s)&quot; % output_vectors.shape) hidden_output = np.dot(np.transpose(input_vectors), context_word_onehot) #print(&quot;hidden_output shape = (%s, %s)&quot; % hidden_output.shape) output_log_prob = np.dot(np.transpose(output_vectors), hidden_output) #print(&quot;output_log_prob shape = (%s, %s)&quot; % output_log_prob.shape) exp_log_prob = np.exp(output_log_prob) sum_exp_prob = np.sum(exp_log_prob) #print(&quot;exp_log_prob shape = (%s, %s)&quot; % exp_log_prob.shape) output_word_prob = exp_log_prob / sum_exp_prob #print(&quot;output_word_prob shape = (%s, %s)&quot; % output_word_prob.shape) # compute gradient and update output vectors d_output_log_prob = output_word_prob - output_word_onehot # d_\mu #print(&quot;d_output_log_prob shape = (%s, %s)&quot; % d_output_log_prob.shape) d_output_vectors = np.dot(hidden_output, np.transpose(d_output_log_prob)) # d_w^\prime #print(&quot;d_output_vector shape = (%s, %s)&quot; % d_output_vectors.shape) output_vectors = output_vectors - learning_rate * d_output_vectors # compute gradient and update input vectors d_hidden_to_output_log_prob = output_vectors #print(&quot;d_hidden_to_output_log_prob shape = (%s, %s)&quot; % d_hidden_to_output_log_prob.shape) d_hidden_output = np.dot(d_hidden_to_output_log_prob, d_output_log_prob) #print(&quot;d_hidden_output shape = (%s, %s)&quot; % d_hidden_output.shape) d_input_vectors = np.dot(context_word_onehot, np.transpose(d_hidden_output)) #print(&quot;d_input_vector shape = (%s, %s)&quot; % d_input_vectors.shape) input_vectors = input_vectors - learning_rate * d_input_vectors save_vectors(input_vectors, vectorfile) vocabulary.save(vocabulary_file)if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser(description = &quot;&quot;&quot;it is a word2vec simple version, with one word as context, and little vocabulary&quot;&quot;&quot;) parser.add_argument(&apos;-c&apos;, &apos;--corpus&apos;, help = &quot;&quot;&quot;corpus file, words have been splited by space&quot;&quot;&quot;) parser.add_argument(&apos;-v&apos;, &apos;--vector&apos;, help = &quot;&quot;&quot;file to save vectors&quot;&quot;&quot;) parser.add_argument(&apos;-w&apos;, &apos;--words&apos;, help = &quot;&quot;&quot;file to save words&quot;&quot;&quot;) parser.add_argument(&apos;-N&apos;, &apos;--dim&apos;, type = int, default = 5, help = &quot;&quot;&quot;vector&apos;s dimention&quot;&quot;&quot;) parser.add_argument(&apos;-e&apos;, &apos;--eta&apos;, type = float, default = 0.05, help = &quot;&quot;&quot;learn rate&quot;&quot;&quot;) parser.add_argument(&apos;-t&apos;, &apos;--epoch&apos;, type = int, default = 10, help = &quot;&quot;&quot;iteration times&quot;&quot;&quot;) args = vars(parser.parse_args()) if args[&apos;corpus&apos;] is None or args[&apos;vector&apos;] is None or args[&apos;words&apos;] is None: parser.print_help() exit() main(args[&apos;corpus&apos;], args[&apos;vector&apos;], args[&apos;words&apos;], args[&apos;dim&apos;], args[&apos;eta&apos;], args[&apos;epoch&apos;]) 简单测试构造了一个简单的语料库： 123456789101112131415161718192021吃 米饭 喝 啤酒喝 果汁吃 面条面条 吃米饭 吃果汁 喝啤酒 喝喝 汽水饮 啤酒喝 白酒干 二锅头干 啤酒吃 馒头吃 大米品 茶品 白酒品 二锅头喝 茶喂 米饭煮 米饭煮 茶 训练模型： 1python word2vec-simple.py -c test.txt -v vector.txt -w words.txt -N 2 -e 0.05 -t 100 得到的词向量保存在vector.txt中： 1234567891011121314151617-3.48655168257 2.524729879811.99419651757 0.235927228504-1.30266728484 -1.603646655920.118887402224 3.377708761912.29850687776 2.764925916733.30972447229 -0.9033453394611.75212009611 2.184794974650.297810632964 -2.226273457821.41486168408 2.008550412130.11649004134 -2.514888385070.169218016547 2.032652284652.57190896308 -0.7357798061832.56125135754 -0.739565548644-1.83965190942 -2.439769533781.60691474095 1.06507268586-2.53892144511 1.2898654145-2.50456983554 -0.612351171966 写一个程序用来查找相似词： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import argparsefrom feature_tools import FeatureIndexfrom annoy import AnnoyIndeximport sysdef read_vectors(vectorfile): vector_index = AnnoyIndex(2) with open(vectorfile) as vectorfile_input: wordid = 0 for line in vectorfile_input: vector = [float(s) for s in line.strip().split()] vector_index.add_item(wordid, vector) wordid += 1 vector_index.build(-1) vector_index.save(&apos;vector.ann&apos;) vector_index.load(&apos;vector.ann&apos;) return vector_indexdef main(vectorfile, vocabularyfile): vocabulary = FeatureIndex() vocabulary.read(vocabularyfile) vocabulary.set_flag(False) vector_index = read_vectors(vectorfile) while True: print(&quot;please input a word in vocabulary:&quot;) line = sys.stdin.readline() line = line.strip() if line == &apos;exit&apos; or line == &apos;quit&apos;: break wordid = vocabulary[line] if wordid is None: print(&apos;sorry, the word you input is not in our corpus.&apos;) continue print(&quot;your word&apos;s id is %s&quot; % wordid) similar_words, distances = vector_index.get_nns_by_item(wordid, 5, include_distances = True) print(&quot;similar words:&quot;) similar_words = [vocabulary[wid] for wid in similar_words] for i in range(len(similar_words)): print(&quot;\t%s:\t%s&quot; % (similar_words[i], distances[i]))if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser(description = &apos;query similar words from vectors&apos;) parser.add_argument(&apos;-v&apos;, &apos;--vector&apos;, help = &quot;&quot;&quot;vector file from word2vec&quot;&quot;&quot;) parser.add_argument(&apos;-w&apos;, &apos;--words&apos;, help = &quot;&quot;&quot;all words and its id(vocabulary)&quot;&quot;&quot;) args = vars(parser.parse_args()) if args[&apos;vector&apos;] is None or args[&apos;words&apos;] is None: parser.print_help() exit() main(args[&apos;vector&apos;], args[&apos;words&apos;]) 运行结果如下： 原汁原味的word2vec毕竟，我们为了找感觉，对word2vec原始论文做了很多简化。下面就开始啃真正的硬骨头，原址原文实现word2vec。相比前面的简化版，有以下变化： 层次softmax（Huffman softmax regression） 负样本采样 随机梯度下降 CBOW模型CBOW模型是word2vec原始论文中的两个模型之一，也是我们在上一小节中简化的模型原型。 我手绘了一下CBOM模型： 首先，上下文中2C个词作为输入，他们每个词的向量都是N维，word2vec的做法是把他们加起来： $V{w}=\sum{k=1}^{2C}{W_{k}}$ $V_{w}$ 就是这个相加后的向量，N维。 然后，计算输出，也就是说计算:$p(w|V_{w})$。这里用一棵Huffman树计算，就相当于把我们上一节中的一次性计算多分类变成了计算多次二分类。而与传统的把多分类转变成多次二分类不同之处在于： 传统的方式是询问“是不是篮球？”“是不是乒乓球”……对叶子节点逐一询问； 这里的Huffman 树是假设了多个隐藏的二分类节点，二分类的询问“不可告人”，出来结果时就是最终类别了。 这里从Huffman树根部逐一向下询问，每一个节点询问一次，询问的方式是用sigmoid函数计算“向左”还是“向右”的概率。向左认为是正确路径，向右是错误路径。每一次计算也就能计算出误差来，并且每个隐藏节点的正确错误恰巧就是待预测词的Huffman编码。 p(w|V_{w}) = \prod{p(L,R | V_{w}, \theta)}这里就是意思一下，你知道，输出词的概率就是沿着树的路径一路把概率乘下来。现在规定几个符号，把上面这个概率连乘表示更清楚些： 符号 含义 $p$ 当前输出词在Huffman 树中的路径 $l$ 路径 $p$经历的节点数(包括叶子节点)，比如上面的手绘图中，足球这个词的 $l=4$ $d_{j}$ 第j个叶子节点后面的编码，即左还是右, $2&lt;=j&lt;=l$，这里注意，根节点是没有编码的 $\theta_{j}$ 第j个叶子节点对应的隐藏向量，也是N维, $1&lt;=j&lt;=l-1$ 其中一个隐藏节点的概率是： p(d_{j}|\theta_{j-1},V_{w}) = \begin{cases} \sigma(V_{w}^{T}\theta_{j-1}) &d_{j} = 0 \\ 1-\sigma(V_{w}^{T}\theta_{j-1}) &d_{j} = 1 \end{cases}换个写法变成一个整体： p(d_{j}|\theta_{j-1},V_{w}) = [\sigma(V_{w}^{T}\theta_{j-1})]^{1-d_{j}}[1-\sigma(V_{w}^{T}\theta_{j-1})]^{d_{j}}那么： p(w|V_{w}) = \prod_{j=2}^{l}{p(d_{j}|\theta_{j-1},V_{w})}表示出了一个词的概率，就可以表示出整个语料的概率，无非还是连乘。按照前面说过的最大化似然（最小化负似然），分别计算出损失函数对 $\theta$ 和 $V_{w}$的梯度： \frac{\partial{L}}{\partial{\theta_{j-1}}} = -[1-d_{j}-\sigma(V_{w}^{T}\theta_{j-1})]V_{w}\\ \frac{\partial{L}}{\partial{V_{w}}} = -[1-d_{j}-\sigma(V_{w}^{T}\theta_{j-1})]\theta_{j-1}更新公式分别为： \theta{j-1}=\theta_{j-1}+\eta[1-d_{j}-\sigma(V_{w}^{T}\theta_{j-1})]V_{w}W_{k} = W_{k}+\eta\sum_{j=2}^{l}{\frac{\partial{L}}{\partial{V_{w}}}} = W_{k}+\eta\sum_{j=2}^{l}{[1-d_{j}-\sigma(V_{w}^{T}\theta_{j-1})]\theta_{j-1}}其中，由于原来公式中的$V{w}$ 是每个词的向量 $W{k}$ 求和而来，但我们要更新的是 $W_{k}$，所以上面第二个更新公式是这样式的。 由于用的是随机梯度下降优化，所以就对所有的样本运行上述更新过程即可。流程伪代码如下： e = 0 $V{w}=\sum{k=1}^{2C}{W_{k}}$ FOR j = 2, to, l DO: { $q = \sigma(V{w}^{T}\theta{j-1})$ $g = \eta(1-d_{j}-q)$ $e = e + g\theta_{j-1}$ $\theta{j-1} = \theta{j-1} + gV_{w}$ } FOR $w_{k} \in context(w)$ DO: { $W{k} = W{k} + e$ } python实现 CBOW 模型Huffman树实现一个关键的数据结构是Huffman树。先说构建这棵Huffman树完成后的结果是什么： 一棵二叉树 叶子节点是词及其词频 左边的节点词频大于右边词频 每个节点的权重是子节点权重之和 每个节点是父节点的左子节点则编码为1，否则编码为0 根节点不编码 每个节点有一个N维向量 它承担的功能是： 每一个样本，输入 $V_{w}$和词w，从Huffman树中找出词w的Huffman编码，其实就是其在树中的路径； 从顶向下逐一计算二分类概率，并计算预测错误，用于计算梯度，累加梯度用于更新词向量，然后更新当前节点的向量值 构建Huffman树的流程是： 将原始词表构建为V棵树，每棵树节点是词，和词频 循环执行下述过程，直到树列表中只剩一棵树时停止： 合并权重最小的两棵树，合并后的节点权重是原来两个节点权重之和，且为父子关系 添加合并后的节点进树列表，删除已合并的节点 注意：在合并时记录编码。 Python代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117class TreeNode(object): &quot;&quot;&quot;this is a tree node implement &quot;&quot;&quot; def __init__(self, data = &#123;&#125;): self._data = data self._left = None self._right = None def __getitem__(self, index): if index in self._data: return self._data[index] return None def __setitem__(self, key, value): self._data[key] = value @property def left(self): return self._left @property def right(self): return self._right @left.setter def left(self, node): self._left = node @right.setter def right(self, node): self._right = node def __str__(self): return &quot;%s, \n\tleft = %s, \n\tright = %s&quot; % (self._data, self.left._data if self.left is not None else &apos;&apos;, self.right._data if self.right is not None else &apos;&apos;)class HuffmanTree(object): &quot;&quot;&quot;this is huffman tree implement &quot;&quot;&quot; def __init__(self, words, N): &apos;&apos;&apos;build a huffman tree from a vocabulary with frequence. Args: words: tuple list, key is word and value is frequence N: vector dimention Returns: HuffmanTree instance &apos;&apos;&apos; tree_nodes = [] for word in words: data = &#123;&#125; data[&apos;code&apos;] = 0 data[&apos;weight&apos;] = word[1] data[&apos;word&apos;] = word[0] data[&apos;theta&apos;] = np.random.rand(N) node = TreeNode(data) tree_nodes.append(node) while len(tree_nodes) &gt; 1: tree_nodes = sorted(tree_nodes, key = lambda x: x[&apos;weight&apos;]) node1 = tree_nodes[0] node2 = tree_nodes[1] node = self.__merge_two_node__(node1, node2) tree_nodes.pop(0) tree_nodes.pop(0) tree_nodes.append(node) self._root = tree_nodes[0] # get huffman code of each word self._word_codes = &#123;&#125; def get_codes(node, codes): if node == None: return huffman_code = codes.copy() huffman_code.append(node) if node[&apos;word&apos;] is not None: self._word_codes[node[&apos;word&apos;]] = huffman_code return get_codes(node.left, huffman_code) get_codes(node.right, huffman_code) get_codes(self._root, []) def get_path(self, word): &apos;&apos;&apos;query word&apos;s path in huffman tree Args: word: a word Returns: a tree nodes list &apos;&apos;&apos; if word not in self._word_codes: return [] return self._word_codes[word] def __merge_two_node__(self, node1, node2): &apos;&apos;&apos;merge two nodes to a new node Args: node1: one of the node node2: another node Return: node, which its weight equal to node1.weight + node2.weight &apos;&apos;&apos; data = &#123;&#125; data[&apos;code&apos;] = -1 data[&apos;weight&apos;] = node1[&apos;weight&apos;] + node2[&apos;weight&apos;] data[&apos;theta&apos;] = np.random.rand(len(node1[&apos;theta&apos;])) node = TreeNode(data) if node1[&apos;weight&apos;] &gt; node2[&apos;weight&apos;]: node1[&apos;code&apos;] = 1 node2[&apos;code&apos;] = 0 node.left = node1 node.right = node2 else: node1[&apos;code&apos;] = 0 node2[&apos;code&apos;] = 1 node.left = node2 node.right = node1 return node 处理语料库将语料库按照空格分开成词，给词编号。实现一个词表类，需求如下： 给每个词编号 给每个词统计词频 可以遍历词表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114class Word(object): &quot;&quot;&quot;Word save string and count. &quot;&quot;&quot; def __init__(self, w): self._count = 1 self._word = w; def __repr__(self): return &quot;(%s, %s)&quot; % (self._word, self._count) def __str__(self): return &quot;(%s, %s)&quot; % (self._word, self._count) @property def word(self): return self._word @property def freq(self): return self._count @freq.setter def freq(self, value): self._count = valueclass Vocabulary(object): &apos;&apos;&apos;vocabulary saving many words &apos;&apos;&apos; def __init__(self): self._word_2_index = &#123;&#125; self._index_2_word = [] def __getitem__(self, index): if isinstance(index, str): if index not in self._word_2_index: self._word_2_index[index] = len(self._index_2_word) word = Word(index) self._index_2_word.append(word) return self._word_2_index[index] if isinstance(index, int): if index &lt; 0 or index &gt;= len(self._index_2_word): return &apos;&apos; return self._index_2_word[index] raise Exception(&apos;unsupport index type, should be str or int.&apos;) def __len__(self): return len(self._index_2_word) def add(self, word): &apos;&apos;&apos; add a word in vocabulary if word has being there, then make its frequence plus 1 Args: word: string word Returns: the word&apos;s index number &apos;&apos;&apos; _index = -1 if word not in self._word_2_index: w = Word(word) _index = len(self._index_2_word) self._index_2_word.append(w) self._word_2_index[word] = _index else: _index = self._word_2_index[word] self._index_2_word[_index].freq += 1 return _index @property def words(self): &apos;&apos;&apos;get words Args: None Returns: [(index, freq)] &apos;&apos;&apos; _words = [] for i in range(len(self._index_2_word)): _words.append((i, self._index_2_word[i].freq)) return _wordsclass Corpus(object): &quot;&quot;&quot;this is corpus saving many documents(sentences) &quot;&quot;&quot; def __init__(self, corpus_path): &apos;&apos;&apos;corpus reading words sequence from file, split into list, map word to index, statistic word frequence Args: corpus_path: corpus file N: vector dimention Return: corpus instance &apos;&apos;&apos; self._vocabulary = Vocabulary() self._sentences = [] with open(corpus_path, &apos;r&apos;) as corpus_file: for line in corpus_file: words = line.strip().split() words_index = [] for word in words: index = self._vocabulary.add(word) words_index.append(index) self._sentences.append(words_index) @property def sentences(self): return self._sentences @property def vocabulary(self): return self._vocabulary CBOW模型主体就是按照前面的流程实现CBOW模型了。Python代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class Word2Vec(object): &apos;&apos;&apos;this is a word2vec implement &apos;&apos;&apos; def __init__(self, corpus_file, config): &apos;&apos;&apos;initialize instance of word2vec Args: corpus_file: corpus file config: hyperparameters loaded from yaml file Returns: instance of word2vec &apos;&apos;&apos; self._config = config self._corpus = Corpus(corpus_file) words = self._corpus.vocabulary.words self._huffman_softmax = HuffmanTree(words, self._config[&apos;N&apos;]) self._word_vectors = [] for i in range(len(self._corpus.vocabulary.words)): self._word_vectors.append(np.random.rand(self._config[&apos;N&apos;])) def run_cbow(self): &apos;&apos;&apos;run CBOW model run CBOW model with SGD Args: None Returns: None &apos;&apos;&apos; sentences = self._corpus.sentences sentences_bar = Bar(&apos;Processing sentences&apos;, max = len(sentences)) for sentence in sentences: words_bar = ChargingBar(&apos;processing words&apos;, max = len(sentence)) for i in range(len(sentence)): current_word = sentence[i] left_window_start = i - config[&apos;window&apos;] left_window_start = 0 if left_window_start &lt; 0 else left_window_start right_window_end = i + config[&apos;window&apos;] + 1 right_window_end = len(sentence) if right_window_end &gt; len(sentence) else right_window_end context = sentence[left_window_start: config[&apos;window&apos;]] context += sentence[i+1: right_window_end] self.__process_a_sample__(context, current_word) words_bar.next() words_bar.finish() sentences_bar.next() sentences_bar.finish() def __process_a_sample__(self, context, current_word): e = 0.0 v_w = np.zeros(config[&apos;N&apos;]) for c in context: v_w = v_w + self._word_vectors[c] huffman_path = self._huffman_softmax.get_path(current_word) for j in range(len(huffman_path)): if j == 1: continue q = self.__sigmoid__(v_w, huffman_path[j - 1][&apos;theta&apos;]) g = config[&apos;eta&apos;] * (1 - huffman_path[j][&apos;code&apos;] - q) e = e + g * huffman_path[j - 1][&apos;theta&apos;] huffman_path[j - 1][&apos;theta&apos;] += g * v_w for c in context: self._word_vectors[c] += e def __sigmoid__(self, vector1, vector2): linear = np.dot(vector1, vector2) if linear &gt; 100: return 1.0 elif linear &lt; -100: return -1.0 else: return 1.0/(1.0 + math.exp(-1.0 * linear)) def save(self, vector_file, words_file): &apos;&apos;&apos;save vector and vocabulary into file Args: vector_file: vector file words_file: vocabulary file Returns: None &apos;&apos;&apos; with open(vector_file, &apos;w&apos;) as vector_output: with open(words_file, &apos;w&apos;) as words_output: for i in range(len(self._corpus.vocabulary.words)): vector = self._word_vectors[i] vector_output.write(&apos; &apos;.join([str(s) for s in vector])) vector_output.write(&apos;\n&apos;) words_output.write(self._corpus.vocabulary[i].word) words_output.write(&apos;\n&apos;) 主函数12345678910111213141516171819def main(config, corpus_file, vector_file, words_file): word2vec = Word2Vec(corpus_file, config) word2vec.run_cbow() word2vec.save(vector_file, words_file)if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser(description = &apos;run word2vec model&apos;) parser.add_argument(&apos;-c&apos;, &apos;--corpus&apos;, help = &apos;&apos;&apos;corpus file&apos;&apos;&apos;) parser.add_argument(&apos;--config&apos;, help = &apos;&apos;&apos;config file(yaml file)&apos;&apos;&apos;) parser.add_argument(&apos;-v&apos;, &apos;--vectors&apos;, help = &apos;&apos;&apos;vectors file&apos;&apos;&apos;) parser.add_argument(&apos;-w&apos;, &apos;--words&apos;, help = &apos;&apos;&apos;words file&apos;&apos;&apos;) args = vars(parser.parse_args()) if args[&apos;corpus&apos;] is None or args[&apos;config&apos;] is None or args[&apos;vectors&apos;] is None or args[&apos;words&apos;] is None: parser.print_help() exit() config = yaml.load(open(args[&apos;config&apos;], &apos;r&apos;)) main(config, args[&apos;corpus&apos;], args[&apos;vectors&apos;], args[&apos;words&apos;])]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LR,SVM 与 EM 算法的区别]]></title>
    <url>%2Farchives%2Fd0d273d9.html</url>
    <content type="text"><![CDATA[LR 和 SVM 与 EM 算法在思路上的不同之处 SVM 的损失函数： LR 的似然函数： EM 算法的似然函数： $l(\theta) = \sum_{i=1}^{m}log\sum_zp(x, z;\theta)$ 从似然函数角度来说，LR 和 SVM 中都含有标签 $y$，是有监督学习，可以通过对损失函数求偏导得到最优解的参数。 EM 算法损失函数中，$p(x, z;\theta)$ 代表 x 属于参数为 $\theta$ 的分布 z 的概率，因为含有隐随机变量 $z$，$z$ 的分布并不知道。不方便直接进行参数估计。 EM 算法的做法是 先通过固定参数 $\theta$，在给定 $x$ 时，求分布 $z$（E-step）， 在通过固定分布 $z$ 求取得极值的参数 $\theta$ （M-step），如此往复。 EM 算法是无监督的学习方式，通常在含有隐变量时用到。]]></content>
      <categories>
        <category>机器学习</category>
        <category>EM 算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类算法笔记（二）--PCA/SVD]]></title>
    <url>%2Farchives%2F2c759b92.html</url>
    <content type="text"><![CDATA[将原始数据通过某种聚类算法聚到 k 类后，如果将样本属于哪一类作为一个特征，并且将这个特征用 one-hot 编码表示，实际上就是将原来数据的 n 维特征降维到 k 维特征。降维的方法有很多，这次主要聊聊 PCA 和 SVD。 PCAPCA 的主要作用使降维，通过降维去噪声，构造出新的 k 维特征出来。 WHEN维度诅咒在高维空间中，数据会变得非常稀疏，常常需要指数倍的增加数据来填充整个空间，比如在二维空间中，需要 $4^2$ 个覆盖整个空间，在三维空间中，则需要 $4^3$ 个覆盖空间。 在维度很高时，点与点之间的距离会变得非常远，而且互相之间的距离几乎都一样远，所以相似性没那么高了，在进行距离计算时，也就变得没有意义了。 举个栗子，随机选取 2000 组数据，维度分别是 10， 50，100，500，1000，5000。计算一个样本与其他所有样本的余弦相似度及欧式距离，画柱状图统计。左边是余弦相似度，右边是欧式距离。 在比如图像压缩，信号处理的信噪比的场景中，会出现特征共线性情况，PCA 的作用可以去掉冗余的特征； 对数据进行可视化时，通常会先进行降维，在二维或三维上进行展示。常用的方法有 PCA 和 t-SNE。 PCA 目的在更低维度上找到一些特征，这些特征能够更好的反应原始数据。 比如在上图中，我们要寻找哪个一维变量能够更好的反映这些二维的数据点？。有两种方式去理解，一是所有样本点离直线距离和足够近，二是，样本在直线上的投影方差足够大，对应 PCA 的两种理论，最小投影距离和最大投影方差。 推导最大方差理论直线在某个方向上的投影坐标： 由余弦定理可以得知，向量 $x$ 在 $\mu$ 方向上投影等于 h = ||x|| \frac{x^T\cdot \mu}{||x||\times||\mu||}\\ =\frac{x^T\cdot \mu}{||\mu||}因为计算向量 $x$ 在 $\mu$ 方向上的投影时，与 $u$ 的方向无关。因为在描述一个向量坐标时，依据便是在基向量上的投影。我们假设 $\mu$ 是个基向量，即 $||\mu|| = 1$。 所有样本在 $u$ 上的投影假设，我们有 m 个样本，每个样本 $x$ 的维度是 n，那么样本形成的矩阵可以表示成，$x^{(j)} = (x_1^{(j)}, x_2^{(j)}, …, x_n^{(j)})$。 X=\begin{pmatrix} x^{(1)}\\ x^{(2)}\\ ...\\ x^{(m)} \end{pmatrix}假设，$u$ 就是找到的使 $X$ 投影后方差最大的方向，$\mu$ 是 n 维的。 又因为我们前面假设的 $||u||= 1$，所以，$x^{(i)}$ 在 $\mu$ 上投影的值等于 $x^{(i)}\cdot \mu$。 所有样本的投影即 X =\begin{pmatrix} x^{(1)}\\ x^{(2)}\\ ...\\ x^{(m)} \end{pmatrix} \cdot \mu= \begin{pmatrix} x^{(1)}\cdot \mu\\ x^{(2)}\cdot \mu\\ ...\\ x^{(m)}\cdot \mu \end{pmatrix}在 PCA 之前的数据预处理中，需要对数据做中心化，即使每个维度（每一列）上的均值为 0，由此可以得到，在投影后得到的值的均值为 0。 \sum_{j=1}^m x^{(j)}_i = 0;\\ (\sum_{j=1}^m x^{(j)})\cdot u = 0投影方向上的方差最大记 $x^{(i)}$ 在投影后的值等于 $y^{(i)} = x^{(i)}\cdot \mu$。 所有样本投影后的方差为 \sigma = \frac{\sum_{i=1}^n(y^{(i)} - \bar{y}^{(i)})^2}{m}因为 $\bar{y}^{(i)} = 0$，所以 \sigma = \frac{\sum_{i=1}^n(y^{(i)} )^2}{m}要求最大值，与样本个数 m 没关系 maxL(\mu)= \frac{\sum_{i=1}^n(y^{(i)} )^2}{m}\\ = \frac{\sum_{i=1}^n(x^{(i)}\cdot \mu)^2}{m}\\ = (x^Tu)^Tx^T\mu\\ =\mu^Tx^Tx\mu\\ s.t. ||\mu|| = 1拉格朗日乘数法我们把如何求 $\mu$ 转化成求带约束条件的极值问题。因为 $||\mu||=1$，构造拉格朗日函数。 L = \mu^Tx^Tx\mu - \lambda (||\mu|| - 1)\\ = \mu^Tx^Tx\mu - \lambda \mu^T\mu + \lambda利用矩阵求导公式 对 $\mu$ 求导，令导数等于 0 \frac{\partial L}{\partial \mu} = 2x^T x- 2\lambda\mu = 0$x^Tx\mu = \lambda \mu$ 选择特征向量由特征值和特征向量定义可知， $\mu$ 就是矩阵 $X^TX$ 的特征向量，$\lambda $ 就是矩阵 $X$ 的特征值，取前 k 大的特征值对应的特征向量构成的 $k\times n$ 矩阵就是降维后的结果。 PCA 步骤数据标准化（减均值） 计算原始数据的协方差矩阵 在数据做中心化处理后，矩阵 $X$ 的协方差矩阵等于 $X^TX$，维度是 $n\times n$。 计算协方差矩阵的特征值和特征向量 选择特征向量选择前 k 大的特征值对应的特征向量，作为降维后的数值 思考 为什么需要对 x 做中心化？ PCA 的中心化是针对每一维特征（每一列）来做的，防止投影到低维空间中，整个投影会去努力逼近最大的那一个特征，而忽略数值比较小的特征； 中心化是 PCA 流程里的关键一步，也是推导出计算协方差矩阵特征值和特征向量的重要条件。 为什么说直接计算原始数据的协方差矩阵？ 在做中心化后，$X^TX$ 就是原始数据 $X$ 的协方差矩阵。协方差矩阵和 $X^TX$ 都是 $n\times n$ 维的。 如何理解 PCA 降维产生的第一主成分和第二主成分？通过投影，找到原始数据投影后方差最大的方向，称为第一主成分方向。 因为对称矩阵的特征向量相互垂直，在与第一主成分方向垂直的方向中，再寻找使投影后方差最大的方向，作为第二主成分。 依此类推。 为什么特征值越大的特征向量越表现说明矩阵的特征？ 矩阵 A 乘以 x 后，即表示对向量 x 进行一次线性变换（旋转或拉伸）。 $Ax=\lambda x$ 代表方阵 A 对 x 进行线性变换后，相当于对 x 进行了拉伸，而特征向量表明矩阵可以对哪些向量进行拉伸，发生拉伸的程度则用特征值来衡量。 这样做为了看清一个矩阵可以在哪个方向上产生最大的效果，并且根据特征值评判效果的大小。 优缺点缺点得到的新特征可能是原来特征相加或相乘的结果，对于新特征的解释性不强。在不需要对特征做出解释时可以使用。 SVDSVD 分解 A = U\Sigma V^TA 是任意矩阵，可分解成正交矩阵乘以对角矩阵，正交矩阵。 对于方阵来说，SVD 相当于特征值分解；对称矩阵的特征值和特征向量相互垂直，所以 A = Q\Lambda Q^T$Q​$ 是正交矩阵。 假设 A 可以写成 A = U\Sigma V^T A^TA = V\Sigma^TU^TU\Sigma V^{T} = V\begin{pmatrix} \sigma_1^2\\ &\sigma_2^2\\ &&&...\\ &&&&\sigma_r^2 \end{pmatrix}V^T​ $U^TU​$ 是单位矩阵 AA^T = U\Sigma V^TV\Sigma^TU^{T} = U \begin{pmatrix} \sigma_1^2\\ &\sigma_2^2\\ &&&...\\ &&&&\sigma_r^2 \end{pmatrix}U^T $V$ 即 $A^TA$ 的特征向量，$U$ 是 $AA^T$ 的特征向量。 $AA^T​$ 和 $A^TA​$ 都是对称矩阵，所以，$V​$ 和 $U​$ 都是正交阵。 $AA^T​$ 和 $A^TA​$ 的特征值完全相同 $\Sigma$ 则是 $AA^T$ 的特征值矩阵的开方，剩余部分用 0 补全。 SVD 性质特征值矩阵按照从大到小排列，而且特征值减小得特别快。在很多情况下，前 10% 甚至 1% 的特征值的和就占了全部的特征值之和的 99% 以上了！我们可以利用最大 K 个特征值和对应的左右特征向量来描述矩阵。]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类算法笔记（三）--密度聚类/谱聚类]]></title>
    <url>%2Farchives%2F6edc7606.html</url>
    <content type="text"><![CDATA[密度聚类密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。 这类算法能够克服基于距离的算法只能发现“类圆形”（凸）的聚类的缺点，可以发现任意形状的聚类，且对噪声数据不敏感。但是计算密度单元的计算复杂度大，需要建立空间索引（如并查集）来降低计算量。 主要介绍 DBSCAN 和密度最大值算法。 DBSCANDBSCAN 的全称是 Desity-Based Spatial Clustering of Applications with Noise。 将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有噪声的数据中发现任意形状的聚类。 DBSCAN 的若干概念 如图，q 是核心对象，p 不是核心对象。从 q 到 p 是直接密度可达的。这种指向是单向的，$p\rightarrow q$ 时，$p$ 是核心对象，$q​$ 则不一定。 在上图 1 的密度可达链中，从 $p, p1, …, p{n-1}​$ 是核心对象，$p​$ 是否是核心对象未知。 在上图 2 中，$p$ 密度可达 $o$，$q$ 密度可达 $o$，$p$ 和 $q$ 可以称为密度相连。 当某个样本离其他样本比较远，不能由其他样本点密度可达时，有可能被定义为噪声。在实际情况下，如果一个样本是核心对象，但是形成的簇比较小，也可能会将这个簇内的所有样本认为是噪声。 超参数 $m$ 和 $\epsilon$ $r$ 和 $\epsilon$ 等价，上图表示当 $m$ 值变小时，原来的噪声可能会变成核心对象，会形成一些新的小簇，簇的样本数量很小。所以，可能需要些后处理，设定形成簇所需最小样本的阈值。 如果呈现出图 4 中的情况，大多数点聚类成一类，往往是因为判定条件太弱导致的，这时需要调小 $\epsilon$ 半径，调大 m 值。 答疑为了方便查找（Find），常常会做一些路径的压缩，比如 $3\rightarrow 7\rightarrow 8 \rightarrow 1$ 表示为 $3$、$7$、$8$ 三者均指向 $1$，$7\rightarrow1, 8\rightarrow1, 3\rightarrow1$，构成并查集，用来提高计算速度。 扫描全部样本点，直到样本点的分类不再变化为止。 DBSCAN 算法对数据分布是没有要求的。 两种情况注意区分： $1\rightarrow 2\leftarrow 3$，$1, 2, 3$ 不一定是一个簇，如果真是这样的话，$1, 3$ 都是核心对象，$2$ 属于 $1$ 或 $3$ 都是有道理的。 $1\leftarrow 2\rightarrow 3$，$1, 2, 3$ 是一个簇。 如果一个样本不是核心对象，并且没有被其他核心对象密度可达，那么这个样本可以被判定为异常值。 $\epsilon​$ 邻域的定义有多种，可以用切比雪夫距离，欧式距离，曼哈顿距离等等。 密度最大值聚类密度最大值聚类可以识别各种形状的类簇，并且参数很容易确定。 局部密度 $\rho_i$ \rho_i = \sum_{j}\chi(d_{ij} - d_c) \chi(x)= \begin{equation} \left\{ \begin{aligned} 1 && x\rho_i}(d_{ij})对于样本 i，找出所有密度高于 $\rho_i​$ 的所有对象，这些对象中距离对象 i 最近的距离称为高局部密度点距离 $\delta_i​$。 对于每个样本都有一个 $(\rho_i, \delta_i)​$： 一个样本的 $\rho_i$ 和 $\delta_i$ 都很大，$\rho_i$ 大的意思是样本 i 周围有很多样本，$\delta_i$ 大指有一个比 $\rho_i$ 还大的样本距离 i 很远，那么 i 可以被认为一个聚类中心。 $\rho_i$ 小，$\delta_i$ 大时，样本 $i​$ 周围样本很少，并且远离了大多数样本，可以认为是异常值。 $\rho_i$ 大，$\delta_i$ 小时，说明离比 $\rho_i$ 大的样本很近，大多数样本符合这种情况。 $\rho_i$ 小，$\delta_i$ 小时，这种情况很少见，因为噪声的 $\rho_i$ 会很小，这种现象可以认为是噪声中的噪声。 局部密度的其他定义为了避免采用计数确定密度时，出现二者计数相同的情况，可以采用其他方式定义密度。 $I_S{i}$ 指样本 i 的邻域。 簇中心的识别在利用比较大的局部密度 $\rho_i$ 和大的高密度距离 $\delta_i$ 确认簇中心后，其他点按照距离已知簇的中心最近进行分类，也可按照密度可达的方法进行分类。 Density Peak 决策图分别以 $\rho_i$ 和 $\delta_i$ 为横纵坐标画图 1 和 10 点的 $\rho_i$ 和 $\delta_i$ 值都非常大，可以认为是聚类中心。 26、27 和 28 的 $\rho_i$ 很小，$\delta_i$ 大，可以认为是噪声。 大多数样本的 $\rho_i$ 和 $\delta_i$ 都是很小的 算法鲁棒性的证明 谱聚类线代中的两个结论，是实对称矩阵的特征值一定是实数，实对称阵不同特征值对应的特征向量一定正交。 如果有概念关系到“谱”，往往就是求的特征值和特征向量。 谱聚类的过程相似度度量方式给定一组数据 $x_1, x_2, …, x_n$，总有办法求出任意两个点之间的相似度 $s(x_i, x_j)$，这里，借鉴 SVM 中核函数的方法度量两个样本间的相似性 s(x_i, x_j) = exp[-\frac{||x_i-x_j||_2^2}{2\sigma^2}]$\sigma$ 决定了衰减速度。 相似度矩阵任意两个样本 $i$, $j$ 都可以计算相似度，n 个样本则可以形成 $n\times n$ 的方阵 $W = (w_{ij}), i, j=1, .., n$。 对角线上指的是样本和自己的相似度，这里并不取 1，取做 0。 W = \begin{pmatrix} 0 & s(x_1, x_2) & s(x_1, x_3)...&s(x_1, x_n)\\ s(x_2, x_1) & 0 & s(x_2, x_3)...&s(x_2, x_n)\\ ...\\ s(x_n, x_1) & s(x_n, x_2) & s(x_n, x_3)...&0\\ \end{pmatrix}如果相似度的度量方式是对称的，相似度矩阵 $W$ 则是一个对称阵。 顶点的度顶点的度 $di​$，定义为相似度矩阵中每一行的加和 $d_i = \sum{j=1}^nw_{ij}​$。 于是便有对角阵 $D$ D = \begin{pmatrix} d_1 & ...&0\\ 0&d_2&... &0\\ 0&0...&d_n\\ \end{pmatrix}拉普拉斯矩阵$L = D - W$ 矩阵 $L$ 是对称阵，求特征值 $\lambda$ 和特征向量 $\mu$。将特征向量按特征值从小到大排列，$u_1, u_2, …, u_n$ 可以构成特征向量矩阵 $U$。 构造拉普拉斯矩阵有多种方式，如果 $L = W-D$ 或 $L = D^{-1}W​$，那就需要特征值从大到小排列。 事先给定聚类个数 $k$，选择前 k 个的特征值对应的特征向量，形成矩阵 $U$，$U\in R^{n\times k}​$。 聚类$U$ 是 $n\times k$ 维，将每一行看作一个样本，每个样本是 $k$ 维的，使用 k-means 算法对 n 个样本进行聚类，得到 $C_1, C_2, …, C_k$ 簇，该聚类的结果就是谱聚类的结果。 过程和 PCA 十分相似，同时，这也是一个降维的过程。 答疑 如何选择 K 将 $\lambda​$ 从小到大排列后，选择 $\lambda​$ 突然上升之前的 k 作为最终聚类的数目。比如将 $\lambda​$ 从小到大排序后，如下图所示，那选择 $k=4​$ 是比较合适的，这是谱聚类常用的选择 k 的方法。 谱聚类可以认为是切割图推导的聚类。原则上切割后的两个类中的 $w(i, j)​$ 不要太小，同时保证切割后两个类的样本数量差不多，由此也可以推出为什么 $L=D-W​$。 $L=D^{-1}W​$ 的解释。如果将 $W​$ 左乘一个矩阵 $D^{-1}​$ \begin{pmatrix} \frac{1}{d_1}\\ &\frac{1}{d_2}\\ ... &&&\frac{1}{d_n} \end{pmatrix}最终得到 \begin{pmatrix} \frac{w_{11}}{d_1}&\frac{w_{12}}{d_1}...&\frac{w_{1n}}{d_1}\\ \frac{w_{21}}{d_2}&\frac{w_{22}}{d_2}...&\frac{w_{2n}}{d_2}\\ ...\\ \frac{w_{n1}}{d_n}&\frac{w_{n2}}{d_n}...&\frac{w_{nn}}{d_n}\\ \end{pmatrix}这个矩阵的第 i 行可以表示样本 i 转移到相邻样本的转移概率。 由此，拉普拉斯矩阵可以认为任一样本在其对应的转移概率下到相邻样本的随机游走。 未正则拉普拉斯矩阵 $L=D-W$ 随机游走拉普拉斯矩阵 $L=D^{-1}(D-W)$ 或 $L=D^{-1}W$ 对称拉普拉斯矩阵 $L=D^{-1/2}(D-W)^{-1/2}$ 随机游走拉普拉斯矩阵是使用最多的。 谱聚类，k-means，层级聚类都需要事先指定 k 值；密度聚类，AP 算法，Canopy 算法都不需要事先指定 k 值。 W 矩阵的建立方式有多种，比如全连接图，$\epsilon$ 近邻图，k 近邻图等 但是，不见得非要做 K-Means，可以试试其他聚类方法，比如密度聚类，层次聚类等。 为什么谱聚类是有效的拉普拉斯矩阵 $L=D-W$ 是对称半正定矩阵，最小特征值是 0，相应的特征向量是全 1 向量。 证明过程如下 L \begin{pmatrix} 1\\ 1\\ .\\ .\\ 1 \end{pmatrix}= D \begin{pmatrix} 1\\ 1\\ .\\ .\\ 1 \end{pmatrix}- W \begin{pmatrix} 1\\ 1\\ .\\ .\\ 1 \end{pmatrix}= \begin{pmatrix} d_1\\ d_2\\ .\\ .\\ d_n \end{pmatrix}- \begin{pmatrix} w_{11}+w_{12}+...+w_{1n}\\ w_{21}+w_{22}+...+w_{2n}\\ .\\ .\\ w_{n1}+w_{n2}+...+w_{nn} \end{pmatrix}\\ = \begin{pmatrix} d_1\\ d_2\\ .\\ .\\ d_n \end{pmatrix}- \begin{pmatrix} d_1\\ d_2\\ .\\ .\\ d_n \end{pmatrix}=0 =0 \begin{pmatrix} 1\\ 1\\ .\\ .\\ 1 \end{pmatrix}假如共有 n 个样本，我们知道有 r 个样本是一个簇，另外 n-r 个样本是一个簇，而且，假设这两个簇间的相似度为 0。 相似度矩阵 $W$ \begin{pmatrix} W_{r\times r}&0\\ 0&W_{n-r\times n-r} \end{pmatrix}矩阵 $D​$ \begin{pmatrix} D_{r\times r}&0\\ 0&D_{n-r\times n-r} \end{pmatrix}拉普拉斯矩阵 $L​$ \begin{pmatrix} L_{r\times r}&0\\ 0&L_{n-r\times n-r} \end{pmatrix}= \begin{pmatrix} D_{r\times r}-W_{r\times r}&0\\ 0&D_{n-r\times n-r}-W_{n-r\times n-r} \end{pmatrix}根据 $L{r\times r}​$ 和 $L{n-r\times n-r}​$ 分别能得到特征值为 0 的两个特征向量，选择前 2 小的特征值对应的特征向量如下： \begin{pmatrix} 1\\ .\\ 1\\ 1\\ .\\ 0\\ \end{pmatrix} \begin{pmatrix} 0\\ .\\ 1\\ 1\\ .\\ 1\\ \end{pmatrix}第一个向量有 $r$ 个 1，第二个向量有 $n-r$ 个 1，共有 n 行。 这样，每个样本变成了 (1, 0)，(1, 0)，… (0, 1) 的形式，用 k-means 进行聚类自然能聚成两类。一类样本数量是 r，一类样本数量 n-r。在现实中可能不会这么理想，形成的两个向量可能是小数形式。 从切割图，随机游走和举特例的方式都可以尝试着解释谱聚类。 标签传递算法 假设一个样本的邻域内有 7 个样本，5 个绿色，2个红色，那这个样本有 $\frac{5}{7}$ 的概率标记为绿色，$\frac{2}{7}$ 的概率标记为红色。上述图是遍历所有样本次数差异得到的聚类结果。可以应用在社区发现中。]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[什么是迭代器和生成器]]></title>
    <url>%2Farchives%2F97e11c7c.html</url>
    <content type="text"><![CDATA[关于什么是可迭代对象，什么是迭代器，什么是生成器，以及 yield 是生成器的重要关键字，为 Python 协程的学习打基础。 例子关于迭代器的理解，先看几个例子 12numbers = [1, 2, 3, 4, 5, 6]squares = (i ** 2 for i in numbers) 12print(tuple(squares))(1, 4, 9, 16, 25, 36) 12print(sum(squares))0 1234print(9 in squares)print(9 in squares)FalseFalse 1234counts = &#123;'apples': 2, 'oranges': 1&#125;x, y = countsprint(x, y)apples oranges 你应该会奇怪，为什么 sum(squares)=0 ？解包操作是否对于所有可迭代对象都可以？ Python 中的 for 循环for 循环使用Python 的 for 循环和其他语言不同，没有索引变量，没有索引变量的初始化，没有边界检查，也没有索引递增。 对于可迭代对象，for 循环都是可以遍历的，比如以下几种形式： 12345678numbers = [1, 2, 3, 5, 7]coordinates = (4, 5, 7)words = "hello there"my_set = &#123;1, 2, 3&#125;my_dict = &#123;'k1': 'v1', 'k2': 'v2'&#125;my_file = open('test_iter.py') # 文件句柄squares = (n**2 for n in my_set) # 生成器fruits = &#123;'lemon', 'apple', 'orange', 'watermelon'&#125; # 集合 但是对于非序列类型（比如集合/字典/文件句柄/生成器等），并不支持索引，用索引手动对这些可迭代对象遍历时，便会报错。 123456fruits = &#123;'lemon', 'apple', 'orange', 'watermelon'&#125;i = 0while i &lt; len(fruits): print(fruits[i]) i += 1&gt;&gt;TypeError: 'set' object does not support indexing 由此得出，任何可以用 for 循环遍历的东西都是可迭代的，序列类型只是一种可迭代的类型。 迭代器是如何工作的利用 iter() 可以使可迭代对象变成迭代器，有了迭代器，可以通过将它传递给内置的 next() 函数来获取它的下一项。 12345678numbers = &#123;1, 2, 3, 5, 7&#125;my_iterator = iter(numbers)next(my_iterator)&gt;&gt;1next(my_iterator)&gt;&gt;2next(my_iterator)&gt;&gt;3 一旦迭代器空了，将抛出 StopIteration 异常。而且，元素一旦被取出，就不能放回去了。 for 循环的本质 从给定的可迭代对象中获得迭代器 反复从迭代器中获得下一项 如果我们成功获得下一项，就执行 for 循环的主体 如果我们在获得下一项时得到了一个 StopIteration 异常，那么就停止循环 12345678910def fucky_for_loop(iterable, action_to_do): iterator = iter(iterable) done_looping = False while not done_looping: try: item = next(iterator) except StopIteration: done_looping = True else: action_to_do(item) 迭代器迭代器协议 迭代器可以传递给 next 函数，它将给出下一项，如果没有下一项，那么它将会引发 StopIteration 异常 可以传递给 iter 函数，它会返回一个自身的迭代器 比如 for 循环，可迭代对象的解包，* 型表达式，工厂函数（list(), tuple()等）都是使用迭代器协议。 12345678910111213141516171819In [15]: numbers = &#123;1, 2, 3, 5, 7&#125; ...: coordinates = (4, 5, 7) In [16]: for n in numbers: ...: print(n) 1 2 3 5 7 In [17]: x, y, z = coordinates In [18]: print(x, y, z) 4 5 7 In [19]: a, b, *rest = numbers In [20]: print(*rest) 3 5 7 当我们在迭代器上调用 iter() 时，它会给我们返回它自己。 12345678In [22]: numbers = [1, 2, 3]In [23]: iterator1 = iter(numbers)In [24]: iterator2 = iter(iterator1)In [25]: print(iterator1 is iterator2)True 迭代器的特点 迭代器没有长度，它们不能被索引。 12345678910111213In [27]: numbers = [1, 2, 3, 5, 7]In [28]: iterator = iter(numbers)In [29]: len(iterator)-------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-29-be9266db3dfd&gt; in &lt;module&gt;()----&gt; 1 len(iterator)TypeError: object of type 'list_iterator' has no len() 迭代器是惰性的，一次性使用，它们的值只能循环遍历一次。 12345678In [30]: next(iterator)Out[30]: 1In [31]: list(iterator)Out[31]: [2, 3, 5, 7]In [32]: list(iterator)Out[32]: [] 其他的可迭代对象除了上述列出的可迭代对象外，以下函数的返回值也都是，其他可迭代对象在标准库 itertools 也可以找到。 12345enumerate()reversed()zip()map()filter() enumerate 123456789101112In [34]: letters = ['a', 'b', 'c']In [35]: e = enumerate(letters)In [36]: eOut[36]: &lt;enumerate at 0x163a4bcf288&gt;In [37]: next(e)Out[37]: (0, 'a')In [38]: iter(e) is eOut[38]: True zip 1234567891011In [39]: numbers = [1, 2, 3, 5, 7]In [40]: letters = ['a', 'b', 'c']In [41]: z = zip(numbers, letters)In [42]: zOut[42]: &lt;zip at 0x163a4ac2ec8&gt;In [43]: next(z)Out[43]: (1, 'a') 惰性抓取文件的前 10 行 1234from itertools import islicefirst_ten_lines = islice(log_file, 10)for line in first_ten_lines: print(line) 创建自己的迭代器定制类构造了一个迭代器接受一个可迭代的数字，并在循环结束时提供每个数字的平方 12345678910111213class square_all: def __init__(self, numbers): self.numbers = iter(numbers) def __next__(self): return next(self.numbers) ** 2 def __iter__(self): return self 123# 无限长的可迭代对象 countfrom itertools import countnumbers = count(5) 1numbers count(5) 12squares = square_all(numbers)next(squares) 25 1next(squares) 36 利用 yield 定制迭代器通常，当我们想要一个定制的迭代器时，我们会生成一个生成器函数 yield。它等价于我们上面所做的类，它的工作原理是一样的。yield 允许我们在调用 next 函数之间暂停生成器函数。在下一次调用时，从上次停止的位置开始。 123def square_all(numbers): for n in numbers: yield n**2 1generator = square_all(numbers) 1next(generator) 49 生成器表达式另一种实现相同迭代器的方法是使用生成器表达式 12def square_all(numbers): return (n**2 for n in numbers) 使用 yield 生成序列中连续值之间的差值列表 1234567def with_next(iterable): """Yield (current, next_item) tuples for each item in iterable.""" iterator = iter(iterable) current = next(iterator) for next_item in iterator: yield current, next_item current = next_item 1%timeit differences = [(next_item - current)for current, next_item in with_next(readings)] 73.2 µs ± 2.66 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) 解释例子出现 sum(squares)=0 因为迭代器被耗尽了 1&gt;&gt;&gt; squares = (n**2 for n in numbers) 1tuple(squares) (1, 4, 9, 25, 49) 1list(squares) [] 解包操作可以应用于可迭代对象 123counts = &#123;'apples': 2, 'oranges': 1&#125;for key in counts: print(key) apples oranges 1x, y = counts 解包一个字典与在字典上循环遍历是一样的，两者都使用迭代器协议，所以在这两种情况下都得到相同的结果。 如果你想在代码中做一个惰性迭代，请考虑迭代器，并考虑使用生成器函数或生成器表达式。]]></content>
      <categories>
        <category>Python</category>
        <category>迭代器和生成器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类算法笔记（一）--K-means/层次聚类]]></title>
    <url>%2Farchives%2F73fe1944.html</url>
    <content type="text"><![CDATA[如果在样本集中 $T={x_1, x_2, …, x_m}$ 中标记 $y_i$ 不存在，将样本集 $T​$ 根据相似性分成若干个簇，我们就称做了 K 个簇的聚类，无监督模型里谈到的几乎都是聚类。 对应于有监督学习，$y$ 是带有标签的。 直观理解 $D_1, D_2, …, D_m$ 是 m 篇语料库，词典 $V$ 包含了 m 篇文档中利用分词工具（NLTK 或 jieba）得到的不重复的词，长度是 n。根据词在文档中是否出现（0/1）或者词频或者 TF-IDF（逆文档频率），文档 1 便可以用特征向量 $D_1$ 表示。m 篇文档便可以用矩阵 $X = m\times n$ 表示。 文档的维度是 $n​$，通常词数量很多，维度很高。现在将原始的 n 维向量通过聚类后，用 k 维向量表示，$k&lt;n​$，m 个样本可以用低维空间的矩阵 $Y=m\times k​$ 表示。 下图中的 $C_{n\times200}​$ 代表了某种线性映射，将 $X​$ 中从高维空间映射到低维空间，得到 $Y​$。 X_{m\times n} \cdot C_{n\times k} = Y_{m\times k}一定程度上，矩阵乘法，降维，无监督学习和聚类之间是有一定关系的。 k-Means 聚类便可以看作是一种矩阵分解，根据 $X$ 和 $C$ 求出 $Y$。 聚类可以作为某个大问题中的小模块存在，比如预处理中的聚类，在样本不均衡（数量级差别）问题中，可以将大样本做聚类，根据聚类结果进行降采样。比如后处理中的聚类，业务上做推送时，预先通过某种分类方法（xgboost/CNN）等将用户分成推送和不推送两大类，然后根据业务逻辑，将需要推送的用户聚类成高价值/中价值/低价值的用户，进行精细化推送等。 在实际应用中，用聚类做主算法不合适，但是作为 CNN/XGboost 等的后算法可能是合适的。 距离和相似度的衡量在聚类分析中，对使用距离还是使用相似度并没有明确的要求。二者皆可。 常用的距离和相似度说明见文章。 欧式距离/曼哈顿距离/切比雪夫距离都是闵科夫斯基距离的变形。 聚类算法聚类基本思想 K-meansK-means 过程 目标是一堆数据聚成两类，随机给定两个中心，计算任何一个点到聚类中心的距离（一般用欧式距离），将样本标记为距离聚类中心最近的类别。取各类的均值得到新的聚类中心，继续计算距离，划分样本，重复以上步骤，直至达到终止条件为止。 终止条件有迭代次数/簇中心变化率/最小平方误差 MSE 变化等。MSE 的计算是每个类别减去该类别均值的平方和，MSE 是逐渐减小的。 聚类中心点的随机初始化 从样本中随机选择两个点作为聚类中心 根据经验值来做，比如对男女身高聚类时，经验上的男女身高平均值 Kmeans++ 先随机选择一个样本作为初始聚类中心，计算每个样本与当前已有聚类中心之间的最短距离，根据距离加权采样，样本里已有聚类中心的距离越远，被选中的概率越大，按照轮盘法选择出下一个聚类中心。 K 值如何选择最优值？纵轴指衡量指标 MSE。 当 k=1 时，MSE 就是方差值。当 k=2 时，意味着各个点离聚类中心更近了，MSE 更小了，依次来看，k 逐渐增大，MSE 逐渐变小，k 等于样本个数时，MSE = 0。 所以，MSE 的变化曲线上不存在极值，K 值得选择通常是在曲线得拐点上，下降比较快与下降比较慢的拐点处（elbow-method）。 但是，K 值的选择更多的依赖先验知识，然后再参考 elbow-method。 样本分布是混合高斯模型（GMM）记样本中有 K 个簇中心，均值分别是 $\mu_1$, $u_2$，…$u_K$，每个簇的样本数目记为 $N_1$, $N_2$, …$N_K$。假设每个簇内的样本符合高斯分布，那么簇 $j$ 内所有的样本概率为 P(j) = \prod_{i=1}^{N_j} \frac{1}{\sqrt{2\pi}\sigma_{j}}e^{-\frac{(x_j^{(i)}-u_j)^2}{2\sigma_j^2}}共有 K 个聚类中心，所有整个样本的概率为 P = \prod_j^{K}\prod_{i=1}^{N_j} \frac{1}{\sqrt{2\pi}\sigma_{j}}e^{-\frac{(x_i^{(j)}-u_j)^2}{2\sigma_j^2}}假设 $\sigma_1$，$\sigma_2$ … $\sigma_K$ 都相等，对上述式子的对数似然取最大值，去掉常数项后。 maxL(u_1, u_2, u_K) = -\sum_j^{K}\sum_{i=1}^{N_j}(x_i^{(j)}-\mu_j)^2\\ =minL(u_1, u_2, u_K)= \sum_j^{K}\sum_{i=1}^{N_j}(x_i^{(j)}-\mu_j)^2对 $\mu_j$ 求偏导，令导数等于 0，可以得到驻点的值： \frac{\partial L}{\partial \mu_j} = -\sum_{i=1}^{N_j}(x_i - \mu_j) = 0\\ u_j = \frac{1}{N_j}\sum_{i=1}^{N_j}x_i所以，K-means 模型做出的假设是样本呈混合高斯分布而且每个簇内的方差相等。如果方差不相等，那就是 EM 算法。EM 算法和 K-means 思路是相同的，找到所有样本的期望最大分布，用这个分布估计参数，再用参数求最大期望分布，如此迭代求出最终的参数。从参数更新的公式看，不是梯度下降算法中的负梯度更新公式。 K-means 算法对所有样本计算距离和均值，本质上，就是对它的目标函数做了批量梯度下降。 随机梯度下降每次不选择所有样本计算距离和均值，而是随机选择部分样本，由此，也由批量梯度下降转换成随机梯度下降。 K-means 适用范围图示 对于非混合高斯分布的数据，效果就差一些了。 思考 K-means 是对初值敏感的，不同的初始化方式产生的聚类结果不同 K-means 认为样本分布是呈混合高斯分布的。因为对正态分布的参数 $\mu$ 进行极大似然估计后，得到的结果就是 $\mu = \frac{1}{n}\sum_{i}^nx_i​$。 异常点对于均值的计算影响严重，这时候采用 k-中值聚类比较稳妥。类似的，k-中值聚类默认样本满足混合拉普拉斯分布。 使用范围：如果某些数据不具有可加性，比如一些类别变量，如果这种类别变量具有数值上的意义，比如及格，不及格，优秀等，可以考虑 K-中值聚类，否则要考虑别的聚类方法，比如密度聚类。 注：在用聚类算法对特征选择时，比如 PCA 算法，得到的新特征可能是原来特征相加或相乘的结果，对于新特征的解释性不强。在不需要对特征做出解释时可以使用。 层次聚类层次聚类对给定的数据集进行层次的分解，直到满足某种条件为止。可以分为 AGNES 算法和 DIANA 算法。 凝聚的层次聚类 AGNES 算法自底向上（常用） 关于簇间距离的计算 以 ward 为例，未合并前两个集合的 MSE 分别是 $MSE_1$，$MSE_2$，合并之后集合为 $MSE$ 。$MSE&gt;MSE_1+MSE_2$，要找到的最好的合并使得合并后的 $MSE$ 不要太大。 分裂的层次聚类 DIANA 算法自顶向下 答疑 经典时间序列分析 如果已知用户最近（转评赞）的数量，已知高速公路前几天的车流量情况，已知网站的点击量等，要求预测后面几天的值 模型可以选 ARIMA 时序分析，LSTM 等方法]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯为什么是线性分类器]]></title>
    <url>%2Farchives%2F35b1965.html</url>
    <content type="text"><![CDATA[线性分类器直观地来说，是在高维样本空间中找到一组超平面，将样本空间划分成两个区域。每个区域对应于不同的类别。数学上来说，线性分类器能找到权值向量 $w​$，使得判别公式可以写成特征值的线性加权组合。 \sum_i^n w_ix_i - w_0 > 0如果上面的不等式成立，则样本属于正类；反之，则样本属于负类。 什么是线性模型线性模型可以用曲线拟合样本，但是分类的决策边界一定是直线的，例如 logistics 模型。 区分是否为线性模型的依据，主要是看式子中自变量 $x$ 前的系数 $w$，如果 $w$ 只影响一个 $x$（ $w$ 和 $x$ 是向量）。那么此模型为线性模型，或者判断决策边界是否是线性的。 y=\frac{1}{1 + e^{-\sum w_ix_i}}$x$ 与 $y$ 是曲线关系，但是 $x_1$ 只被 $w_1$ 影响，所以是线性模型。 y=\frac{1}{1 + w_4(e^{-(w_1x_1 + w_2x_2 + w_3x_3)})}这个便是非线性模型，$x_1$ 被 $w_1$ 影响的同时，还被 $w_4$ 影响。 最简单的判断就是判别决策边界是否是非线性的。 朴素贝叶斯是线性分类器吗朴素贝叶斯公式中，只要 $P(x_i|c)​$ 是服从指数分布簇的，就可以写成是线性分类器的形式。 具体可以参考： 假设 $x_i$ 只能为 0 或 1 的二分类情况 判断一个样本 y=1 的依据是 \frac{P(y=1|X)}{P(y=0|X)}>1\\ =\frac{P(x|y=1)P(y=1)}{P(x|y=0)P(y=0)} > 1根据条件独立性假设，将 $P(x|y)$ 写成 $\prod _{j=0}^d P(x_j|y)$ 的形式，重新整理上面函数，最后可以得到 b + \sum_{i=1}^n w_j x_j \geq 0的形式。 具体参考https://svivek.com/teaching/machine-learning/fall2017/slides/prob-learning/naive-bayes-linear.pdf $p(x_i|c)​$ 是服从指数分布簇的普遍情况 https://stats.stackexchange.com/questions/142215/how-is-naive-bayes-a-linear-classifier 并不是所有的朴素贝叶斯分类器都是线性分类器。如果连续特征的朴素贝叶斯分类器中方差不相同，那我们就会发现判别公式不能写成特征值的线性加权组合。 http://www.algorithmdog.com/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8-%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8-2 参考： https://www.jianshu.com/p/46ac61a312d4]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[指数族分布与广义线性模型]]></title>
    <url>%2Farchives%2F2551ba75.html</url>
    <content type="text"><![CDATA[指数族分布是广义线性模型提出的假设之一，同时也可以解释为什么 logistic 回归中用的是 sigmoid 函数，以及为什么朴素贝叶斯分类器是线性分类器等问题。 指数族分布 伯努利分布是指数族分布 高斯分布是指数族分布 同样，下面的分布都是指数族分布 多项式分布 泊松分布，对计数类数据进行建模，比如网站访问量的统计，商店顾客数量等 $\gamma$ 分布和指数分布，对连续的、非负的随机变量进行建模，比如公交车的到站时间等 $\beta$ 和 $Dirichlet$ 分布，用于概率的分布，分别是二项分布和多项式分布的共轭分布，也是先验分布。 广义线性模型构建广义线性模型的三个假设 最小二乘法为什么是 $y=\theta x$ logistics 回归为什么是 sigmoid 函数logistic 回归假设给定 x 后，y 符合伯努利分布，伯努利分布的广义线性模型的假设中， \eta = log(\frac{\phi}{1-\phi})\\ e^{-\eta} = \frac{1-\phi}{\phi}\\ \phi = \frac{1}{1+e^{-\eta}} ​ $g(\eta)$ 又叫规范响应函数，$g(\eta)$ 的反函数叫规范连接函数，在 logistics 回归中，连接函数就是 sigmoid 函数。 参考 https://github.com/jiangxiaoshaui/Algorithm-of-ML/tree/master/Logistic_Regression cs 229 讲义 https://github.com/jiangxiaoshaui/Algorithm-of-ML/blob/master/Logistic_Regression/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F.md]]></content>
      <categories>
        <category>机器学习</category>
        <category>线性回归</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集成算法（二）--GBDT/XGB/LGB]]></title>
    <url>%2Farchives%2F7c0afd4b.html</url>
    <content type="text"><![CDATA[GBDT/XGBoost/Lightgbm 是在工业中最常用的集成方法。与 Adaboost 样本权重和分类器权重的更新方式不同，这三种方法的权重更新都是采用梯度更新的方法。但这几个模型的共性都是利用上一个弱分类器的结果对当前分类器的进行修正。 Lightgbm 是 17 年微软研究院发表出来的，也是当前比较火的机器学习框架。 XGBoost目标函数样本 $xi \in R^d​$ ，线性模型本质上就是学习参数 $w_i​$，通过 $\hat{y_i} = \sum_j w_j x{ij}​$ 得到预测值 $\hat{y_i}​$。 机器学习中的目标函数由训练误差和正则项组成，训练误差则衡量模型在训练集上的表现，正则项 $\Omega(w)​$ 则衡量了模型的复杂度，控制模型不要太复杂，防止过拟合。 Obj(\theta) = L(\theta) + \Omega(w) 损失函数：$L = \sum_{i=1}^n l(y_i, \tilde y_i)​$ 正则项： $L_2​$ norm：$\Omega(w) = \lambda ||w||^2​$ $L_1$ norm(lasso)：$\Omega = \lambda ||w||_1$ 优化训练误差可以让模型具有预测性 优化正则项可以让模型简单一些，使其预测更加稳定 欠拟合的模型常常导致高偏差，过拟合模型常常导致高方差。 回归树模型（CART） 决策规则和决策树一样，通过信息增益比或基尼系数决定分裂节点 在每个叶节点都有一个预测分数 集成模型 最终的预测结果是 K 棵树的加和，第 k 棵的预测结果称为 $f_k(x_i)$。 k 棵树集成模型的目标函数 样本 $x_i$ 在集成模型上的预测值 \hat y_i = \sum_{k=1}^K f_k(x_i) 总的代价函数是每次预测的损失加和，正则项也是每棵树 $f_k​$ 正则项的加和。 Obj = \sum_{i=1}^n l(y_i, \hat y_i) + \sum_{k=1}^K \Omega(f_k) Gradient Boosting在上述的代价函数中，我们并没有办法用 SGD 去计算 $f_k​$。于是，就有了 Gradient Boosting 的方法，运用了 Additive Training 的思想。 每一轮的预测结果 $\hat y_i^{(t)}​$ 等于上一轮的预测结果 $\hat y_i^{(t-1)}​$ 加上当前函数的预测结果 $f_t(x_i)​$。 \hat y_i^{(0)} = 0\\ \hat y_i^{(1)} = f_1(x_i) = \hat y_i^{(0)} + f_1(x_i)\\ \hat y_i^{(2)} = f_1(x_i)+f_2(x_i) = \hat y_i^{(1)} + f_2(x_i)\\ ...\\ \hat y_i^{(t)} = \sum_{k=1}^tf_k(x_i) = \hat y_i^{(t-1)} + f_t(x_i)\\$\hat yi^{(t)} = \sum{k=1}^K f_k(x_i)​$ 指训练 $t​$ 轮时的模型的预测结果，也就是函数 $f_t(x_i)​$ 的累加，$f_t(x_i)​$ 则是 $t​$ 轮预测时的新函数，是需要确定的。 Additive training问题在于在 $t​$ 轮预测时，如何确定函数 $f_t(x_i)​$ 呢？ 那么，在第 $t​$ 轮预测（并非总的代价函数）时的损失 $Obj^{(t)}​$ 可以得到 Obj^{(t)} = \sum_{i=1}^n l(y_i, \hat y_i^{(t)}) + \Omega(f_t) + constant\\ = \sum_{i=1}^n l(y_i, \hat y_i^{(t-1)}+f_t(x_i)) + \Omega(f_t) + constant以平方误差为例： Obj^{(t)}= \sum_{i=1}^n [y_i-(\hat y_i^{(t-1)}+f_t(x_i))]^2 + \Omega(f_t) + constant泰勒展开式二阶泰勒展开式 f(x+\Delta x) = f(x) + f'(x)\Delta x+ \frac{1}{2}f''(x)\Delta x^2$t​$ 轮预测的损失为 Obj^{(t)} = \sum_{i=1}^n l(y_i, \hat y_i^{(t-1)}+f_t(x_i)) + \Omega(f_t) + constant令 f(x) = l(x)\\ x = y_i, \hat y(t-1) \\ \Delta x = f_t(x_i) \\ g_i = f'(x) = f'(y_i, \hat y^{(t-1)}) = \partial_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)})\\ h_i = f''(x) = f''(y_i, \hat y^{(t-1)}) = \partial^2_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)})由泰勒展开可以得 Obj^{(t)} = \sum_{i=1}^n [l(y_i, y^{(t-1)})+g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)] + \Omega(f_t) + constant$l(y_i, y^{(t-1)})$ 是上一次预测的损失函数，是个常数项。 以平方误差损失为例， g_i = \partial_{\hat y^{t-1}}(\hat y^{(t-1)} - y_i)^2 = 2(\hat y^{(t-1)} - y_i)\\ h_i = 2新的目标函数去掉常数项后 Obj^{(t)} = \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)] + \Omega(f_t)\\ g_i = \partial_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)})\\ h_i = \partial^2_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)})利用泰勒展开式，使原先难以用 SGD 求解的目标函数转换成了类似多项式的目标函数。 关于树的定义 $q$ 代表一棵树结构，$w_{q(x)}$ 代表树中叶子节点 $q(x)$ 的权重。 一个决策树的核心是树结构和叶权值。 叶子节点包括两个东西： 样本实例集 样本实例的权重 $I_i = {i|q(x_i) = i}​$ 代表在叶子节点 $j​$ 上的数据集合 正则项正则项代表了模型的复杂度。对叶子节点个数 $T$ 和每个叶子节点权重做出惩罚。 \Omega(f_t) = \gamma T + \frac{1}{2}\sum_{j=1}^Tw_j^2$T$ 代表叶子节点的个数，$w_j$ 代表叶子节点 $j$ 的权重。 重回目标函数决策树的学习过程，就是构造如何使用特征得到划分，从而得到这些权值的过程。对于样本 $x​$ 最终会落在叶节点 $q​$ 上，所以样本 x 在决策树 t 上的预测可以用叶节点 q 的权值表示：$ft(x) = w{q(x)}​$。 $i\in I_j$ 代表属于叶子节点 $j$ 的样本，在第 4 行从遍历样本转换成遍历样子节点个数。 $J(f_t)$ 便可以评估第 $t$ 棵树的质量。 举个例子 $Obj$ 代表了第 $f_t$ 棵树的评价指标，也是树的损失函数。 寻找最佳树结构借鉴 ID3/C4.5/CART 的做法，使用贪心法进行划分。针对每个可行划分点，选择增益最大的划分，继续同样的操作，直到满足阈值或得到纯节点。 具体来说， 在每个节点，遍历所有的特征。对连续特征来说，对每个特征按值大小进行排序，使用线性搜索方法搜寻最佳切割点，计算哪个切割点的信息增益最大。 这样会带来一些问题，对特征的值排序十分耗时，如果某个特征有 m 个值，那么会产生 m-1 个切割点，计算十分缓慢。针对每个特征分割点的计算上，XGBoost 实现了并行。 总结 在每一次迭代增加一个新树 •在每次迭代的开始，在每个样本上，利用泰勒展开式，计算 g_i = \partial_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)})\\ h_i = \partial^2_{\hat y^{t-1}}l(y_i, \hat y^{(t-1)}) 以贪心法寻找最佳树结构 $f_t(x)​$ 将 $f_t(x)​$ 增加到模型上 常用的方式是 \hat y_i^{(t)} = \sum_{k=1}^tf_k(x_i) = \hat y_i^{(t-1)} + \epsilon f_t(x_i)$\epsilon​$ 叫做步长或者 shrinkage，经常设置在 0.1 附近的值。$\epsilon​$ 越小，计算越慢，产生的树的数量越多，有点像学习率。 优点 计算速度快，准确率高。 支持分布式，不同于 Bagging，Boosting 产生的每棵树都是依赖于上一棵树的预测结果，所以在每棵树的计算并不能实现并行，并行思路是体现在求每个特征的最佳分割点上。 和 GBDT 不同，基分类器不一定是 CART 树，可以是线性回归等其他分类器。 GBDTGBDT 中，无论分类还是回归都是 CART 树。和 Additive training 一样，假设在前一轮的学习器是 $f{t-1}(x)​$，本轮迭代的目标是找到一个 CART 树模型的弱学习器 $h_t(x)​$，使本轮的损失函数 $L(y, f_t(x)) = L(y, f{t-1}(x) + h_t(x))​$ 最小。 损失怎么拟合？ 用损失函数的负梯度拟合本轮损失的近似值 选择什么样的损失函数？ 分类问题中常用交叉熵损失，回归问题中常用 MSE 损失。 算法核心：用损失函数的负梯度拟合本轮损失的近似值。 目标是求近似函数 $fm(x)$ 使损失函数 $\sum{i=1}^nL(yi, f_m(x))$ 最小。$f_m(x)$ 是一组基函数的加权和 $f_m(x) = \sum{i=0}^Mf_i(x)​$。 假设第 m 棵树的参数是 $\theta_m$，也就是求 \theta_m = argmin_{\theta_m}\sum_{i=1}^NL(y_i, f_{m-1}(x_i)+T(x_i;\theta_m)) 初始化常数函数 $f_0(x)​$，估计使损失函数极小化的常数值，这个常数值不是随意初始化的，如果使用最小二乘构建损失函数时，即 L(y_i, f_m(x)) = \frac{1}{2}(y_i-f_m(x))^2$f_0(x)​$ 值应该使所有 y 值均值，这样第一步的损失函数 $f_0(x)​$ 才会最小。如果是绝对值损失，应该取 y 值中位数。 遍历 M 轮。 遍历每个样本，求出每个样本的负梯度。$r_{im}​$ 指损失函数的负梯度方向，对于平方损失函数，通常就是所说的残差；对于一般损失函数，就是残差的近似值。 根据样本数据 x 和损失函数负梯度（残差）作为 y，通过线性搜索叶节点区域 $R{jm}​$，使在每个叶节点的损失加和最小，生成一棵新的回归树 $T_m=\sum{j=1}^{Jm}\gamma{jm}I(x\in R_{jm})​$，作为第 m 轮的决策树。$J_m​$ 代表第 m 棵回归树的叶子节点个数。 更新回归树 得到最终模型 $f_m(x)$。 参数《统计学习方法》 XGBoost 与 GBDT 的区别链接：https://www.zhihu.com/question/41354392/answer/98658997 传统 GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器，这个时候 xgboost 相当于带 L1 和 L2 正则化项的 logistic 回归（分类问题）或者线性回归（回归问题）。 传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是 xgboost 优于传统 GBDT 的一个特性。 Shrinkage（缩减），相当于学习速率（ xgboost 中的eta）。xgboost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把 eta 设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）。xgboost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是 xgboost 异于传统 gbdt 的一个特性。 对缺失值的处理。对于特征的值有缺失的样本，xgboost 可以自动学习出它的分裂方向。 xgboost 工具支持并行。boosting 不是一种串行的结构吗？怎么并行的？注意 xgboost 的并行不是 tree 粒度的并行，xgboost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。xgboost 的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost 在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 （新）可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 xgboost 还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。 Lightgbm最大的特点：训练速度快，更节省内存。 XGBoost 一个瓶颈是针对每个特征，它需要对每一个可能的分裂点扫描全部的样本来进行计算基尼系数，这是非常的耗时的。 对于 XGboost 来说，如果有 m 个连续值的样本，需要先进行排序，然后分别选择 m-1 个分裂点计算基尼系数。Lightgbm 使用直方图算法（而非每个样本作为分裂点）在牺牲一定精度的条件下换取计算速度的提升和内存消耗的降低。直方图中有多少个 bins，就计算多少次分裂点，bin 的数量是远小于样本数量的。 主要基于以下两种方法： Gradiant-based One-side Sampling (GOSS) GOSS 随机排除一些小梯度样本，用剩余的样本评估信息增益。抽样的方法保证计算速度提升的同时，也能够防止过拟合。 Exclusive Feature Bundling (EFB) Lightgbm 采用 leaf_wise 策略，xgboost 采用 level_wise 策略 Lightgbm 直接支持分类变量，而 xgboost 需要转换成 one-hot 编码 分裂方法GOSS 和 EFB 两种分裂方法的出发点不同，GOSS 主要从样本角度除法，而 EFB 则通过 bundle 的方式降低特征数量，加快计算速度。 GOSSLightGBM 和 XGboost 同样是一种 Gradient Boosting 的方法，根据当前模型损失函数的负梯度训练新加入的基分类器，将训练好的基分类器以累加的形式加入到现有模型中。不同地方之一在于寻找分裂点的方式不一样。 Histogram-based Algorithm 将连续型样本划分成不同的直方图，构成 usedRows。遍历每个特征和 usedRows 中的每个元素，寻找最佳分裂点。 LGB 寻找最佳分裂点从两个方面进行了改进，一是采用直方图的算法，二是在训练模型数据集的选择上不同，先对每个样本上的梯度进行排序，分成大梯度和小梯度两部分，大梯度样本全部取出，小梯度样本随机进行抽样，然后组合到一起，对随机抽样的样本权重 $w​$ 乘以某个系数，用来训练新的模型。 EFB(了解)高纬度数据通常是稀疏的，许多特征之间是互斥的，不会同时拥有非零值（假设）。我们可以把互斥的特征进行 bundle，通过算法可以对同一个 bundle 里的特征做相同的直方图，最后将 bundle 后的数据 merge。采用 bundle 的形式降低了特征的维度，从而加快了计算速度。复杂度从原来 $O(data\times feature)$ 降低到 $O(data \times bundle)​$。 leaf-wise 与 level-wiseXGboost 对每一层的所有节点进行无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是 xgboost 也对其进行分裂，这便浪费了资源。 LGB 对当前叶子节点选择分裂收益最大的节点进行分裂。但是这样容易造成过拟合，因为容易陷入较大的深度，所以必须设置树的深度。所以 LGB 中的 max-depth 和 num_leaves 参数很重要。 Categorical Feature对于类别变量，XGboost 会转换成 one-hot 编码，耗用内存，并且引起树的不平衡，需要较大的树深才能达到较好的精度。 一个可以替代的优化方法是对分类特征进行二分，比如一个类别变量有 $a_1, a_2, a_3, a_4$，可以有 $a_1|a_2, a_3, a_4$，$a_2,|a_1,a_3,a_4$ ，$a_3|a_1,a_2,a_4$，$a_4|a_1,a_2,a_3$，$a_1, a_2|a_3, a_4$，$a_1,a_3|a_2,a_4$，$a_2, a_3|a_1, a_4$ 七种分裂方式，如果有 k 类，会有 $2^{k-1}-1$ 个分裂点。 LGB 的做法则是根据分类特征与目标变量的相关性进行重新排序。更详细的说，是根据分类特征的累积值（sum_gradient/sum_hessian）对其直方图进行重排，然后在排序好的直方图上寻找最佳分裂点。最终的时间复杂度是 $O(k\times log_k)​$。 总结： LGB 在分类效果上和 XGboost 差不多，但是计算速度非常快，尤其是存在很多分类特征时，XGBoost 需要将分类特征转换成 one-hot 编码，内存很容易会爆掉，而 LGB 自身的机制便可以出来分类特征。 LightGBM GBDT原理及利用 GBDT 构造新的特征-Python 实现 GBDT+LR算法解析及Python实现]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[KNN 与 KD 树]]></title>
    <url>%2Farchives%2F7c10b52f.html</url>
    <content type="text"><![CDATA[KNN 不具有训练和学习的过程，没有参数，不会最终形成模型。拿到新的实例后，从训练数据集中，通过计算距离，选择 k 个最近邻的训练类别实例，通过多数表决或平均值等方式进行决策。 K 近邻法（KNN） 输入 训练集 $T={(x_1, y_1), (x_2, y_2), … (x_m, y_m)}$ 输出 实例 x 所属的类别 y。 根据给定的距离度量，在训练集 T 中找出与 x 最近邻的 k 个点，这个邻域记为 $N_k(x)$。 在 $N_k(x)$ 中根据分类决策决定 x 的类别 y。 由此产生三个问题： K 的取值如何确定？ 距离如何度量？ 是否真的遍历所有的样本计算距离？（线上推荐速度太慢） K 值选择通过交叉验证选择 K 值越小，模型越复杂，容易发生过拟合 K 值较大，模型简单，学习的误差增大，欠拟合 距离度量 闵科夫斯基距离 欧式距离 曼哈顿距离 参考文章 http://www.hotheat.top/archives/af94264b.html KD 树建树对于新的实例，如果遍历所有的样本，这样速度太慢，KD 树是一种对 k 维空间中的实例进行存储以便对其进行快速检索的树形数据结构。 选择方差最大的特征 $k_i$ 选择特征 $k_i$ 的中位数作为分裂点，将特征值小于 $k_i$ 的数据划分到左子树，大于 $k_i$ 的划分到右子树。 针对左子树和右子树重复分裂操作，构建一棵二叉树。 搜索搜索的过程涉及两次递归操作。 第一，先根据二叉树，从根节点出发，比较左右子节点的值，直到抵达叶子节点，目标节点为叶子节点 第二，递归向上回退 ​将当前叶子节点记为“最近节点”，记录新实例与当前节点的距离为最近距离 ​回溯到父节点，计算新实例与父节点的距离，更新最近距离和最近节点 找到父节点的另一个子节点，更新最近距离和最近节点，更新目标节点为父节点 判定以新实例为中心，最近距离为半径形成的超球体与目标节点的兄弟节点是否相交 如果相交，意味着在兄弟节点区域可能存在与新实例更近的点，继续递归近邻搜索 如果不相交，向上回退，更新目标节点。 第三，回退到根节点时，搜索结束，“当前最近点”即为 x 的最近邻点。 新实例点为 $(3, 4.5)$，查找与新实例最近邻的点。 如上图 抵达叶子节点的路径依次是 $(7, 2)\rightarrow (5, 4) \rightarrow (4, 7)$。 回溯递归查找形成的圆依次是 $红色圆（叶节点）\rightarrow 绿色圆（父节点）\rightarrow 蓝色圆（兄弟节点）$ 蓝色圆的半径最小，回退到根节点（7, 2），蓝色圆与 $x=7$ 不相交，不用进入右子空间查找，搜索结束 最近节点为（2, 3），最近距离为蓝色圆半径。]]></content>
      <categories>
        <category>机器学习</category>
        <category>KNN</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[样本不均衡的处理方法]]></title>
    <url>%2Farchives%2F10cd09cb.html</url>
    <content type="text"><![CDATA[造成样本不均衡的原因 由采样造成的样本不均衡，比如医院的病人数据，餐厅的意见簿等。 样本自身原因。信用卡欺诈，地震，点击率等发生概率很低的事件 样本不均衡的数据所造成的后果比如有一个样本集，正负例比例是 9：1，如果有一个模型，不论接受什么样本，只输出正例，那么模型的准确率也会达到 90%。但是模型没有任何意义。 样本不均衡的解决思路如果样本不均衡，坚持用原始样本喂给模型，那么就会出现模型的预测大多数都是大类的情况。 主要有以下几个做法： 样本中有正例 A 990 个，负例 B 类 10 个。 大类欠采样 随机对 A 类进行欠采样，建立模型 $t_i$，重复 m 次，对这 m 个模型用随机森林，得到最终的决策； 或者将 A 类分成若干子类，分别与 B 类进入 ML 模型； 或者用基于聚类对 A 类分割。 大类降采样是经常采用的方法。 小类过采样 比如将 B 类样本升至 10 倍 200 个，将 A 类与 B 类用 SVM/LR/RF 等算法预测。 二者结合 A 降采样到 90 个，B 过采样到 50 个，选择某个模型预测。 数据合成 通过随机插值或 SMOTE 等方法进行数据合成 代价敏感学习（权重的更迭） 降低 A 类权值，提高 B 类权值。 ​ 这是 SVM 中分别用线性核和高斯核对不均衡数据进行分类的结果。当采用线性核，权重为 1: 1 时，样本全部预测为绿色，当权重变成 1:99 后，右上图中有一部分样本被预测为红色。 设置损失函数的权重，使得少数类别数据判断错误的损失大于多数类别数据判断错误的损失，即当我们的少数类别数据预测错误的时候，会产生一个比较大的损失值，从而导致模型参数往让少数类别数据预测准确的方向偏。 参考： 02 特征工程 - 数据不平衡]]></content>
      <categories>
        <category>机器学习</category>
        <category>预处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集成算法（一）--Bagging/Adaboost]]></title>
    <url>%2Farchives%2F9fdd0500.html</url>
    <content type="text"><![CDATA[组合多个基础模型的预测值，提高模型的泛化能力。 主要有两个方法得到最终的预测值： Averaging：独立的建立多个模型对预测结果做平均或投票，又叫 Bagging。 Boosting：有序列的，依赖多个基模型，用后一个模型修正前一个模型的 bias。 BaggingBagging 集成规则有平均值、少数服从多数或其他投票机制。 每个基分类器的数据集是随机抽取得到的，通过引入随机性来减少方差。 Boostrap 采样方法： 有放回的随机采样，每次采集一个样本，然后放回，一轮随机的采样次数与样本集数目 m 相同，当 $m \rightarrow \infty​$ 时，一轮随机采样中，样本不被采集到的概率是 \lim_{m\rightarrow \infty}(1 - \frac{1}{m})^m \\ = (\lim_{m\rightarrow \infty}(1 + \frac{1}{-m})^{-m})^{-1}\\ = \frac{1}{e} \approx 0.368这 36.8% 的数据，没有落在袋子中，作为 Out of Bag (OOB)，可以作为测试集，验证模型的泛化能力。这部分数据是没有被模型学到的。落在袋中的数据作为训练数据。所以，Bagging 是不需要划分训练数据和测试数据。 Bagging 对分类器没有限制，LR，DT，KNN，朴素贝叶斯，SVM 等基础模型都可以作为基本分类器。 随机森林属于 Bagging 的一种模型，在样本和特征上都做随机。基分类器最常用的是 CART 决策树。SVM 和 LR 作为强分类器，常常不放在随机森林中。 样本和特征上的随机选择保证了生成决策树的多样性。 假设训练集有 m 个样本，一次决策树共做 m 次有放回采样，得到 m 个样本的采样集； 每建一棵树时，特征是随机选择的，选择最优（信息增益或基尼系数）特征作为分裂点进行左右划分；实际操作中经常用随机数发生器来实现选择。 共做 T 次决策，如果是分类决策，采用投票最多的样本类别；如果是回归算法，则取 T 个弱分类器结果的平均值作为输出。 每建一棵树时，都可以计算一下 OOB Score，如果共做 T 次决策，则可以计算这 T 次 OOB Score 的平均值作为在测试集上的表现。 举例 蓝色点是样本点，灰色线是用弱分类器做的回归曲线，红色线是所有灰色曲线的平均结果作为最终的回归曲线。 投票机制简单投票机制－ 一票否决（一致表决） － 少数服从多数 ​ 有效多数（加权） － 阈值表决 贝叶斯投票机制 m 是个超参数，是个有意义的固定值。 $\frac{m}{v+m}C$ 作为先验知识，做一个初步的预判。新电影没有人看过，v = 0，WR = C；电影是个热门电影，v 远大于 m，WR 接近该电影用户投票的平均分。 利用 RF 建立样本间相似度 这种相似度判定，绕过样本特征的情况。 利用RF 计算特征重要度 在 scikit-learn 中的 model.featureimportances AdaboostBoosting：下一个分类器的表现会依赖于上一次分类的结果。 Adaboost，是一种比较经典的 boosting 思路，像 GBDT，XGboost，LGB 是基于梯度的 boosting 思路。 主要思路同时给予分类器和每个样本权重，假设开始时每个样本的权重相同，每轮预测结束后，预测错误的样本权重上升，预测正确的样本权重下降。下一轮预测基于上一轮中预测错误的样本来进行，也就是权重变高的那些样本。最终的结果基于分类器的权重系数得到的。 假设共有 m 个样本，迭代次数 T 次，第一次预测初始化每个样本的权值 $D^1(i)$ 分布为 $\frac{1}{m}$ for t=1, …, T do 拿一个基本分类器做一次预测，比如 SVM，DT，LR 等做一次二分类 计算这个分类器的误差，也就是预测错误的部分，估计值 $f_t(X_i)$ 不等于 $y_i$ 的加和。 误差率的计算将样本权值 $D^t(i)$ 也考虑进去，第二步公式等同于下式，$I$ 是指示函数。 \epsilon_t = \sum_{i=1}^m \omega_{ti}I\{f_t(X_i)\neq y_i\} 计算系数 $\alpha_t$，这个系数会被认为是分类器 $f_t(X)$ 的权重，同时 $\alpha_t$ 也会用来计算下一次样本 $i$ 的权重。 更新每个样本的权重 $D^{(t+1)}(i)$，如果预测错误，$y_if_t(X_i) = -1$，$e^{-\alpha_t y_if_t(X_i)}$ 将大于 1，下一次该样本的权重将变大；同理，如果预测正确，$e^{-\alpha_t y_if_t(X_i)}$ 将小于 1，下一次预测该样本权重将变小；归一化因子 $Z^{(t)}​$ 保证权重在变化后的加和仍然是 1，满足一个概率分布。 最终的分类器将是 T 个基本分类器 $f_t(X)$ 加上权重 $\alpha_t$ 加和的结果。]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python 协程的实现机理]]></title>
    <url>%2Farchives%2F68a4c345.html</url>
    <content type="text"><![CDATA[在看别人写的二叉树遍历的代码时，看到了yield 和 yield from 的用法，不是很理解，发现 yield 与 Python 协程十分相关，在网上找到两个很好的谈协程的系列。 从 generator 和 yield 表达式说起关于生成器、迭代器、yield 关键字可以参考文章什么是迭代器和生成器，我们在写一个简单的生成器的时候有很多东西没有用到，特别是生成器的三个重要方法，一个是 next()、一个是 send()，一个是 throw()。 send()方法的使用 （4）在用 g.send() 后，yield 表达式执行，在下一次运行 next() 时，会直接输出 3，原来的 2 去哪里了？ （5）如果再执行一次 next(g)，my_generator() 中的元素耗尽，会引发 StopIteration 异常。 yield 语句的用法总结 迭代器（生成器）的 send() 方法详解原来的 2 实际上就是 send() 的返回值。 生成器 throw 的方法用法g.throw() 括号中的参数一个异常，作用是向生成器抛出一个异常，使这个生成器终止，后面的 yield 语句将不会执行。 g.throw(StopIteration) 会直接中断迭代器的执行。如果传入其他异常，这个异常被 except 接收，那后面的 yield 不会再执行，生成器终止，如果执行 g.throw() 会引发 StopIteration 异常。 生成器的启动与关闭 close 生成器启动有两种方式 直接使用 next() 使用 g.send(None)，第一次启动的时候只能传入 None，如果传入其他具体的指则会报错。 使用 g.close() 关闭，关闭后再启动将抛出 StopIteration 异常。 生成器的终止迭代——StopIteration除了 g.close() 方法外，遇到 return 语句也会抛出 StopIteration 异常。生成器没有办法使用 return 来返回值。因为 return 返回的那个值是通过 StopIteration 的异常信息返回的。 抛出 StopIteration 异常的几种情况： generator 耗尽时，再调用 next() 或 send() 使用 g.close() 关闭迭代器 使用 g.throw(StopIteration) 向生成器抛出异常 生成器函数中遇到 return 语句 获得 return 返回值的方法 使用 yield from 语句 使用 StopIteration 类的 value 属性 12345678910111213141516def g3(): yield 'a' return '这是错误说明' yield 'b'g=g3()try: print(next(g)) #a print(next(g)) #触发异常except StopIteration as exc: result=exc.value print(result)'''运行结果为：a这是错误说明''' python 协程的通俗理解以及使用 yield 关键字实现协程线程、进程和协程进程 操作系统进行资源调度和分配的基本单位，多个进程之间相互独立，进程间数据不共享，一个正在运行的应用程序在操作系统中被认为是一个进程。 进程可以包括一个或多个线程 稳定性好，如果一个进程崩溃，不影响其他进程，但是进程资源消耗大，开启的数量有限制。 线程 CPU 进行资源调度和分配的基本单位，线程是进程的一部分，是比进程更小的能够独立运行的基本单位。 线程主要是由 CPU 寄存器、调用栈和线程本地存储器（Thread Local Storage，TLS）组成的。CPU 寄存器主要记录当前所执行线程的状态，调用栈主要用于维护线程所调用到的内存与数据，TLS 主要用于存放线程的状态信息。 线程自己不拥有系统资源，只拥有一点在运行中必不可少的资源，但是一个进程下的所有线程可以共享该进程下的所有资源。 线程本质上是操作系统提供的一段逻辑功能，是进程中一段并发运行的代码，所以线程需要操作系统投入 CPU资源来运行和调度 进程与线程的区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 1) 简而言之，一个程序至少有一个进程，一个进程至少有一个线程。 2) 线程的划分尺度小于进程，使得多线程程序的并发性高。 3) 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 4) 线程在执行过程中与进程还是有区别的。每个独立的进程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 5) 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 如果 IO 操作密集，那么多线程效率会高，缺点是如果一个线程崩溃，会造成进程的崩溃。 GILPython 官方解释器（CPython 解释器）认为在内部不是线程安全的，因此有一个全局解释器锁（Global Interpreter Lock，简称 GIL），它使得在任何时刻都只有一个线程在执行 Python 字节码。简单来说，一个 Python 进程永远不能在同一时刻使用多个 CPU 核心。 所以，Python 能够使用线程，而且这个特性使得 Python 特别适合 IO 密集的任务（这些任务不依赖于多核）。 如果想要发挥多核的能力，规避 CPython GIL 带来的问题，可以用以下两种解决方案： 用原生 C 代码编写，某些库（比如 Numpy）不会有 GIL 带来的问题。 用多进程代替多线程。 GIL 并不能保证编写的 Python 代码线程安全，但会保证某些 Python 基本操作原子性。其他一些 Python 实现，比如 Pypy 不会有相同的保证。 协程协程有点像多线程，但协程的特点在于一个线程执行，协程看上去像函数，但在执行过程中，在函数内部可以中断，不一定要一次性执行完才行，然后转而执行别的函数，在适当的时候返回来执行。 12优势一：最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。优势二：就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 比如说在等 IO 时，CPU 就不分配时间了，而是由编程者自己控制各个任务的执行顺序，从而最大可能的发挥 CPU 性能。 为什么 yield 可以实现协程 Python 实现协程的代码 c.send(None) 启动生成器，相当于调用 consumer()，但是如果 consumer() 是个普通函数，会等待 consumer 执行完后，才会重新回到 producer 手中，现在，consumer 是生成器，所以在第一次遇到 yield 时暂停，直接执行 producer() 中的代码。 进入 producer 的 while 循环体，n=1，打印生产者正在生产 1，调用 consumer()，打印消费者正在消费 1，赋值 r，又遇到 yield，暂停，并返回 变量 r 的值，回到 producer，打印返回值 r。 准备进入下一个循环。 协程的状态查看 yield 实现协程的不足之处 协程函数的返回值（return）不是特别方便获取，可以参见第一章节，只能够通过发出 StopIteration 异常，通过该异常的 value 属性获取。 Python 的生成器是协程 coroutine 的一种形式，但它的局限性在于只能向它的直接调用者每次 yield 一个值。这意味着那些包含 yield 的代码不能想其他代码那样被分离出来放到一个单独的函数中。这也正是 yield from 要解决的。 总的来说，协程是一个可以暂停执行的函数，并且可以恢复执行。yield 已经可以暂停执行，如果在暂停后如果有办法将一些 value 发回到暂停执行的函数中，那么 Python 就有了协程。而这个“把东西发送到已经暂停的生成器中的”方法就是 send()。 yield from详解yield 是每次“惰性返回”一个值，yield from 是 yield 的改进升级版，可以理解成从“什么（迭代器）中返回”，这也构成了 yield from 的一般语法，yield from generator。 12345678910111213141516171819In [3]: def generator1(): ...: for i in range(10): ...: yield i ...:In [4]: def generator2(): ...: yield 'a' ...: yield 'b' ...: yield 'c' ...: yield from generator1() ...: yield from [11, 22, 33, 44] ...: yield from (12, 23, 34) ...: yield from range(3) ...:In [5]: for i in generator2(): ...: print(i, end=',') ...:a,b,c,0,1,2,3,4,5,6,7,8,9,11,22,33,44,12,23,34,0,1,2, yield from 后面可以跟的可以是“ 生成器 、元组、 列表、range（）函数产生的序列等可迭代对象”简单地说，yield from generator 。实际上就是返回另外一个生成器。yield from iterable 的本质是 for item in iterable: yield item 的缩写版。 yield from 的高级应用针对 yiled 无法获取生成器 return 的返回值在不使用 yield from 时，如果想要得到生成器函数中的返回值，需要通过捕捉 StopIteration 异常来做到。 但是现在有了 yield from，需要多一个包装函数，通过调用方-&gt;生成器包装函数-&gt;生成器函数来实现 yield from iteration 结构会在内部自动捕获 iteration 生成器的 StopIteration 异常。这种处理方式与 for 循环处理 StopIteration 异常的方式一样。而且对 yield from 结构来说，解释器不仅会捕获 StopIteration 异常，还会把return 返回的值或者是 StopIteration 的 value 属性的值变成 yield from 表达式的值，即上面的 result。 这里的做法也是通过一个包装函数变成了 yield from iteration 的形式。 yield from 所实现的数据传输通道yield 中涉及调用方和生成器两者，而 yield from 则涉及到委派生成器（yield from 构成），子生成器（yield）构成，调用方（main 函数）。 yield from 的用法实例其实 yield from 相当于一个数据管道，这个数据管道的作用负责将任务传给子生成器，以及将子生成器的值返回给调用方，而且可以处理子生成器产生的 StopIteration 异常。 Python协程系列（一）——从 generator 和 yield 表达式说起 python协程系列（二）——协程的通俗理解及yield关键字实现协程 python协程系列（三）——yield from详解 python协程系列（四）——详解『同步|异步』『并发|并行』『线程|进程』 Python的协程真的有那么难吗？ 深入理解Python的yield from语法 Python的GIL是什么？ 【意译】Python3中的线程，GIL，线程安全（上）]]></content>
      <categories>
        <category>Python</category>
        <category>协程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常见的数据可视化图表]]></title>
    <url>%2Farchives%2F51ddfe42.html</url>
    <content type="text"><![CDATA[可视化的四类 比较：比较各类别的关系或随时间的变化趋势，折线图 联系：两个或两个以上变量的关系，比如散点图 构成：部分占整体的百分比或随时间变化的百分比，比如饼图 分布：单个变量或多个变量的分布情况，比如直方图 散点图1234import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pd 1234N = 1000x = np.random.randn(N)y = np.random.randn(N)plt.scatter(x, y, marker='x') 12df = pd.DataFrame(&#123;'x': x, 'y':y&#125;)sns.jointplot(x='x', y='y', data=df, kind='scatter') 折线图12x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35] 1plt.plot(x, y) 1[&lt;matplotlib.lines.Line2D at 0x1f8de66c7f0&gt;] 12df = pd.DataFrame(&#123;'x':x, 'y':y&#125;)sns.lineplot(x='x', y='y', data=df) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8de695b70&gt; 直方图12a = np.random.randn(100)s = pd.Series(a) 1plt.hist(s) 12345(array([ 1., 1., 12., 14., 19., 19., 18., 13., 1., 2.]), array([-2.32016662, -1.86680799, -1.41344936, -0.96009073, -0.5067321 , -0.05337347, 0.39998516, 0.85334379, 1.30670242, 1.76006105, 2.21341968]), &lt;a list of 10 Patch objects&gt;) 1sns.distplot(s, kde=False) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8de798780&gt; 1sns.distplot(s, kde=True) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8de7fa518&gt; 条形图12x = ['cat1', 'cat2', 'cat3', 'cat4', 'cat5']y = [5, 4, 8, 12, 7] 1plt.bar(x, y) 1&lt;BarContainer object of 5 artists&gt; 1sns.barplot(x, y) 箱式图1data = np.random.normal(size=(10, 4)) 1labels = ['A', 'B', 'C', 'D'] 1plt.boxplot(data, labels=labels) 12df = pd.DataFrame(data, columns=labels)sns.boxplot(data=df) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8de8c7cc0&gt; 12# 均值是 10 方差是 1 和均值是 1 方差是 10 的二元正态分布data = np.random.normal([10, 1], [1, 10], size=(4, 2)) 1data 1234array([[10.62703289, 4.45110371], [ 9.38855947, 0.0385639 ], [10.18634894, -4.37851528], [10.58392971, 10.66448511]]) 饼图12nums = [25, 37, 33, 37, 6]labels = ['High-school', 'Bachelor', 'Master', 'Ph.d', 'Others'] 1plt.pie(x=nums, labels=labels) 热力图关于 seaborn 中更多热力图的使用参考文章。 1flights = sns.load_dataset('flights') 1flights.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; year month passengers 0 1949 January 112 1 1949 February 118 2 1949 March 132 3 1949 April 129 4 1949 May 121 1data=flights.pivot('year','month', 'passengers') 1sns.heatmap(data) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8deb0ef28&gt; 雷达图123# 中文和负号的正常显示plt.rcParams['font.sans-serif'] = 'SimHei'plt.rcParams['axes.unicode_minus'] = False 1234567891011121314151617181920212223242526272829303132# 使用ggplot的绘图风格plt.style.use('ggplot')# 构造数据values = [3.2,2.1,3.5,2.8,3]feature = ['个人能力','QC知识','解决问题能力','服务质量意识','团队精神']N = len(values)# 设置雷达图的角度，用于平分切开一个圆面angles=np.linspace(0, 2*np.pi, N, endpoint=False)# 为了使雷达图一圈封闭起来，需要下面的步骤values=np.concatenate((values,[values[0]]))angles=np.concatenate((angles,[angles[0]]))# 绘图fig=plt.figure()# 这里一定要设置为极坐标格式ax = fig.add_subplot(111, polar=True)# 绘制折线图ax.plot(angles, values, 'o-', linewidth=2)# 填充颜色ax.fill(angles, values, alpha=0.25)# 添加每个特征的标签ax.set_thetagrids(angles * 180/np.pi, feature)# 设置雷达图的范围ax.set_ylim(0,5)# 添加标题plt.title('活动前后员工状态表现')# 添加网格线ax.grid(True) ! 二元变量分布单样本双变量密度图12tips = sns.load_dataset('tips')sns.jointplot(x='total_bill', y='tip', data=tips, kind='scatter') 1&lt;seaborn.axisgrid.JointGrid at 0x1f8dfe73518&gt; 1sns.jointplot(x='total_bill', y='tip', data=tips, kind='kde') 1&lt;seaborn.axisgrid.JointGrid at 0x1f8dff93080&gt; 1sns.jointplot(x='total_bill', y='tip', data=tips, kind='hex') 1&lt;seaborn.axisgrid.JointGrid at 0x1f8e0052c18&gt; 两样本双变量密度图12345678910111213141516171819202122sns.set(style="darkgrid")iris = sns.load_dataset("iris")# Subset the iris dataset by speciessetosa = iris.query("species == 'setosa'")virginica = iris.query("species == 'virginica'")# Set up the figuref, ax = plt.subplots(figsize=(8, 8))ax.set_aspect("equal")# Draw the two density plotsax = sns.kdeplot(setosa.sepal_width, setosa.sepal_length, cmap="Reds", shade=True, shade_lowest=False)ax = sns.kdeplot(virginica.sepal_width, virginica.sepal_length, cmap="Blues", shade=True, shade_lowest=False)# Add labels to the plotred = sns.color_palette("Reds")[-2]blue = sns.color_palette("Blues")[-2]ax.text(2.5, 8.2, "virginica", size=16, color=blue)ax.text(3.8, 4.5, "setosa", size=16, color=red) 1Text(3.8,4.5,&apos;setosa&apos;) 多行多列密度图成对关系1iris = sns.load_dataset('iris') 1sns.pairplot(iris) 1&lt;seaborn.axisgrid.PairGrid at 0x1f8e05748d0&gt;]]></content>
      <categories>
        <category>可视化</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[低维度特征可视化与强特征构造]]></title>
    <url>%2Farchives%2Fa543d014.html</url>
    <content type="text"><![CDATA[做比赛时，每个维度的分析都非常重要，有时还需要像不太懂数据分析的人解释，这时候可视化就变得非常重要。以下文章主要参考科赛网上的文章，seaborn 官方教程翻译。 连续型变量对于一些类别特征，常常采用 count 或 mean 的统计分析基本可以满足分析的需求，但是如果是数值特征，也就是连续变量时，需要数据可视化来观察数据的分布特点，相关性等。 连续单变量特征最常见的单变量特征分析是在回归问题中，可以了解数据的异常值和分布情况。 123456import numpy as npimport pandas as pdfrom scipy import stats, integrateimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore') 12import seaborn as snssns.set(color_codes=True) 12# 设置随机数种子，ord() 返回字符的 ASCII 码值np.random.seed(sum(map(ord, "distributions"))) plt hist单变量最简单的方法是绘制直方图，默认 bins 是 10 个。 1x = np.random.normal(size=100) 1plt.hist(x) 12345(array([ 4., 13., 12., 22., 20., 15., 9., 1., 3., 1.]), array([-1.85860798, -1.38515154, -0.91169511, -0.43823868, 0.03521775, 0.50867418, 0.98213061, 1.45558704, 1.92904348, 2.40249991, 2.87595634]), &lt;a list of 10 Patch objects&gt;) seaborn kdeplotkdeplot 采用核函数的方式对离散型数据进行了平滑处理，默认是高斯核函数。关于核密度估计可以参考文章。 1sns.kdeplot(x,shade=True) 双样本的双变量密度图可以通过两次调用 sns.kdeplot() 来实现 12345678910111213141516171819202122sns.set(style="darkgrid")iris = sns.load_dataset("iris")# Subset the iris dataset by speciessetosa = iris.query("species == 'setosa'")virginica = iris.query("species == 'virginica'")# Set up the figuref, ax = plt.subplots(figsize=(8, 8))ax.set_aspect("equal")# Draw the two density plotsax = sns.kdeplot(setosa.sepal_width, setosa.sepal_length, cmap="Reds", shade=True, shade_lowest=False)ax = sns.kdeplot(virginica.sepal_width, virginica.sepal_length, cmap="Blues", shade=True, shade_lowest=False)# Add labels to the plotred = sns.color_palette("Reds")[-2]blue = sns.color_palette("Blues")[-2]ax.text(2.5, 8.2, "virginica", size=16, color=blue)ax.text(3.8, 4.5, "setosa", size=16, color=red) 1Text(3.8,4.5,&apos;setosa&apos;) distplotdistplot 可以绘制出拟合的近似分布。 1sns.distplot(x) 可以通过 kde=Fasle, hist=False, rug=False, bins=num 来控制关闭 kde, 关闭直方图，关闭坐标竖条，关闭分布观测条。 比赛中，常常会通过观察 bins 进行切分，并做成 one-hot 编码形成新的特征。 fit 可以用其他的分布函数近似拟合数据，例如用 gamma 分布拟合。 12x = np.random.gamma(6, size=200)sns.distplot(x, kde=False, fit=stats.gamma) 用高斯分布拟合 gamma 分布生成的数据。 1sns.distplot(x, kde=False, fit=stats.gausshyper) 连续二元变量特征实际问题中，我们常常需要对两个连续变量之间进行相关性分析，进行预测等。主要用到 matplotlib 中的 scatter plot 和 jointplot。 在应用散点图时，数据比较多时，建议使用采样观察。通过散点图可以很容易发现簇的存在，以及涉及到经纬度时，可以考虑聚类等操作。 通过多元高斯分布生成数据 123mean, cov = [0, 1], [(1, .5), (.5, 1)]data = np.random.multivariate_normal(mean, cov, 200)df = pd.DataFrame(data, columns=["x", "y"]) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; x y 0 2.190873 2.902961 1 0.387901 3.441322 2 -1.304909 0.586173 3 -0.016867 0.907323 4 0.284953 1.189304 plt scatter1plt.scatter(df['x'].values,df['y'].values) jointplotjointplot 不仅可以方便的绘制散点图，还可以进行一些简单的模型拟合，比如线性回归等。 默认的 Jointplot 1sns.jointplot(x="x", y="y", data=df) Hexbin 图类似于直方图的二维展示。一般适用于较大数据集，最好使用白色背景，可以用 with 来控制背景。 12with sns.axes_style('white'): sns.jointplot(x="x", y="y", data=df, kind='hex') KDE 图是 kde plot 的二维展示 1sns.jointplot(x="x", y="y", data=df, kind="kde") cubehelix 是具有线性增加或降低亮度和色调变化顺序的调色板，可以使得在各种变化中都保持良好的亮度线性梯度。reverse 可以对调结果顺序。as_cmap 参数用来更改显示的颜色范围是离散的还是连续的。关于更多的调色板知识请参考文章。 n_levels 控制轮廓级数 123f, ax = plt.subplots(figsize=(6, 6))cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)sns.kdeplot(df.x, df.y, cmap=cmap, n_levels=60, shade=True); plot_jointplot_joint 可以将很多绘图形式放在同一张图表中 12345g = sns.jointplot(x="x", y="y", data=df, kind="kde", color="m")g.plot_joint(plt.scatter, c="w", s=30, linewidth=1, marker="+")g.ax_joint.collections[0].set_alpha(0)g.set_axis_labels("$X$", "$Y$") pairplot绘制多个变量两两之间的关系。 12iris = sns.load_dataset("iris")sns.pairplot(iris) pairplot 是建立在 pairgrid 之上的，可以用很多中间函数进行变换。在对角线和非对角线上分别应用不同的函数。关于更多的 pairgrid 的方法可以参数文章。 123g = sns.PairGrid(iris)g.map_diag(sns.kdeplot)g.map_offdiag(sns.kdeplot, cmap="Blues_d", n_levels=6) 一些连续数据可视化更详细的参数可以参考文章 类别变量对于类别变量绘制，散点图和回归模型将不起作用。可视化的目的为了观察分布的抽象表示及应用统计估计权重趋势或置信区间。主要包括以下三类： swarmplot() 和 stripplot() boxplot() 和 violinplot() barplot() 和 pointplot() 还有，通过 Factorplot 和 Pairgrid 集成函数展示更多的变量间的关系。 12345sns.set(style="whitegrid", color_codes=True)np.random.seed(sum(map(ord, "categorical")))titanic = sns.load_dataset("titanic")tips = sns.load_dataset("tips")iris = sns.load_dataset("iris") 分类散点图stripplot最简单的方式是使用 stripplot()。 1sns.stripplot(x="day", y="total_bill", data=tips, jitter=False) 上述图因为覆盖太过严重，很难看到某个 label 上点的实际分布情况，可以使用随机抖动(jitter)将同一位置的点随机分开，抖动也可以在数据集上用 numpy 实现。sns.stripplot() 默认 jitter = True。 1sns.stripplot(x="day", y="total_bill", data=tips, jitter=True) swarmplot即使随机抖动还是有重叠的可能，所以 swarmplot() 使用使用了避免重叠点的算法，这种方式更好。 1sns.swarmplot(x="day", y="total_bill", data=tips) 可以通过传入 hue 参数添加多个分类变量。 1sns.swarmplot(x="day", y="total_bill", hue="sex",data=tips) 当要计算的类别是数值类别时，将会被排序后显示。如果类别名称很长或者有很多类别时，可以使用 orient=’h’ 或交换 x 和 y 变量的赋值。 1sns.swarmplot(x="total_bill", y="day", hue="time", data=tips) stripplot VS swarmplot 当两个变量都是类别变量时，swarmplot 更加明显。 swarmplot 缺点非常耗时，数据量大时不适用。 123_, [ax0, ax1] = plt.subplots(nrows=2, ncols=1, figsize=[12, 8])sns.stripplot(x="size", y="size", data=tips, jitter=True, ax=ax0)sns.swarmplot(x="size", y="size", data=tips, ax=ax1) 类别内的分布分类散点图在有些情况下提供的分布信息十分有限，在两个类别十分相近时，会变得不清晰，这时需要在类别之间进行简单比较或汇总统计。 boxplot箱式图给出了均值，四分位数及极值的信息。上下两条晶须分别对应第一四分位数 + $1.5 \times IQR$ 和第三四分位数 + $1.5 \times IQR$，IQR=第三四分位数-第一四分位数。 1sns.boxplot(x="day", y="total_bill", data=tips) 可以通过 hue 显示不同变量的箱式图。但是这种情况下箱子会很难看。可以通过 dodge 控制箱子的位移。默认 dodge 为 True。更多控制箱子的参数参考官网。 1sns.boxplot(x="day", y="total_bill", hue="size", data=tips, dodge=False) 1sns.boxplot(x="day", y="total_bill", hue="size", data=tips) violinplot小提琴图结合了箱式图和 kde 图，使用核密度估计更好的描述值的的分布。 1sns.violinplot(x="total_bill", y="day", hue="time", data=tips) 可以通过控制一些参数，比如 scale, scale_hue 等进行美化。当 hue 只有两个类别时。可以传入 split 参数更好的利用空间。 1sns.violinplot(x="day", y="total_bill", hue="sex", data=tips, split=True) 控制 inner 参数显示框内的信息 12sns.violinplot(x="day", y="total_bill", hue="sex", data=tips, split=True, inner="stick", palette="Set3") 与 swarmplot 的结合violinplot 的形状与 swarmplot 形状类似，可以将二者结合。 但是并不推荐这样做，过多的信息除了炫技没什么其他用处。 12sns.violinplot(x="day", y="total_bill", data=tips, inner=None)sns.swarmplot(x="day", y="total_bill", data=tips, color="w", alpha=.5) 同样 swarmplot 也可以与 boxplot 结合。 12sns.boxplot(x="day", y="total_bill", data=tips)sns.swarmplot(x="day", y="total_bill", data=tips, color="w", alpha=.5) boxenplot一种新的箱式图展示，对于含有较多异常值时展示更合理。 类别的统计信息有时，不能仅仅只看数据的一个外在的表现，我们需要一些工具将它的一些统计信息（均值，个数等）反应出来。 barplot默认会使用对应类别的均值。hue 可以控制更精细的类别。黑色竖线代表置信区间。 1sns.barplot(x="sex", y="survived", hue="class", data=titanic) countplot纵轴计算的是均值，在泰坦尼克号数据中指的是存活下来的比例，有时我们想看一下总个数，以确认这种均值是否具有统计学意义。 1sns.countplot(hue="sex", x="survived", data=titanic, palette="Greens_d") pointplot只给出点估计（概率形式）和置信区间，直线连接相同 hue 类别的点，很容易通过斜率的差异看出主要关系如何随第二个变量的变化而变化。 1sns.pointplot(x="sex", y="survived", hue="class", data=titanic) 使用不同的标记和线条样式 123sns.pointplot(x="class", y="survived", hue="sex", data=titanic, palette=&#123;"male": "g", "female": "m"&#125;, markers=["^", "o"], linestyles=["-", "--"]) 通过 grid 展示更多的分类变量factorplot()factorplot() 与 facetgrid() 结合使用，与 jointplot 一样，配合 kind 关键字指定不同的类型，默认情况下是 kind = “pinpoint”，seaborn 0.9.0 中改为 catplot()，默认为 stripplot()。具体见网址。 1sns.factorplot(x="day", y="total_bill", hue="smoker", data=tips) factorplot bar 1sns.factorplot(x="day", y="total_bill", hue="smoker", data=tips, kind="bar") 与 sns.barplot 效果相同 1sns.barplot(x="day", y="total_bill", hue="smoker", data=tips) factorplot 的优点是可以通过 col 再展开一个特征。这样可以查看 x,y,hue,col 四个变量的分布情况。 1sns.factorplot(x="day", y="total_bill", hue="smoker",col="time", data=tips, kind="swarm") 可以通过 size 和 aspect 更改图形的大小和形状 12sns.factorplot(x="time", y="total_bill", hue="smoker", col="day", data=tips, kind="box", size=4, aspect=.5) 与 pairgrid 结合也可以与 parigrid 结合来实现 violinplot，多个不同变量间的分类关系，下面这张图便展示出了五个变量个分布关系情况。 12345g = sns.PairGrid(tips, x_vars=["smoker", "time", "sex"], y_vars=["total_bill", "tip"], aspect=.75, size=3.5)g.map(sns.violinplot, palette="pastel") 12g = sns.PairGrid(tips,x_vars=["smoker", "time", "sex"],y_vars=["total_bill", "tip"],aspect=.75, size=3.5)g.map(sns.barplot, palette="pastel") 其他技巧使用”宽格式” 数据宽格式数据一个特征的不同类别用不同的列表示，而并不像长格式数据一样一个特征即一列。宽格式数据和长格式数据可以通过 stack() 与 unstack() 相互转换。 1iris.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa 转换为长格式数据 1iris.set_index('species').stack().reset_index(1).head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; level_1 0 species setosa sepal_length 5.1 setosa sepal_width 3.5 setosa petal_length 1.4 setosa petal_width 0.2 setosa sepal_length 4.9 1sns.boxplot(data=iris,orient="h") 接受 pandas 或 numpy 的向量1sns.violinplot(x=iris.species, y=iris.sepal_length) 可以通过 matplotlib 中的命令控制图形大小和形状等12f, ax = plt.subplots(figsize=(7, 3))sns.countplot(y="deck", data=titanic, color="c") 更多的类别变量资料请参考文章 变量关系的可视化一般用于回归问题，同时也可以更好的进行特征构造。Seaborn 的目的是通过可视化快速轻松的探索数据集。 12345sns.set(color_codes=True)np.random.seed(sum(map(ord, "regression")))tips = sns.load_dataset("tips") 不存在异常值的 线性关系lmplot 和 regplotlmplot 和 regplot 的区别： lmplot 必须接受 dataframe 结构中的长格式数据，必须传入 data 参数，x 和 y 参数需是字符串。 regplot 则可以接受 numpy arrays, pandas series 和 dataframe 中的变量。 lmplot()将 regplot() 与 FacetGrid 结合在一起，可以在“faceted”图上显示线性回归，从而探索多达三个或更多类别变量的交互。总结来说，观察两个变量时， regplot 的数据形式比 lmplot 更灵活，但是，lmplot 可以通过 hue 或 col 参数来展示更多维度变量的线性关系。 12x = list(range(1, 50))y = np.sin(x) 1sns.regplot(x, y) 1sns.lmplot(x, y) 123456-------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-131-e787e6bb2080&gt; in &lt;module&gt;()----&gt; 1 sns.lmplot(x, y) 1TypeError: lmplot() missing 1 required positional argument: &apos;data&apos; 二者都返回回归直线和置信区间 1sns.regplot(x="total_bill", y="tip", data=tips) 1sns.lmplot(x="total_bill", y="tip", data=tips) 存在异常值的线性回归1anscombe = sns.load_dataset("anscombe") 1anscombe.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; dataset x y 0 I 10.0 8.04 1 I 8.0 6.95 2 I 13.0 7.58 3 I 9.0 8.81 4 I 11.0 8.33 12sns.lmplot(x="x", y="y", data=anscombe.query("dataset == 'III'"), ci=None, scatter_kws=&#123;"s": 80&#125;) 可以通过传入 robust=True，使用不同的损失函数减小相对较大的误差 1sns.lmplot(x="x", y="y", data=anscombe.query("dataset == 'III'"),robust=True, ci=None, scatter_kws=&#123;"s": 80&#125;) 通过 residplot 判断相关性在拟合一个线性回归后，理想情况下，这些值应该分布在 y=0 上下的 线性相关情况 12sns.residplot(x="x", y="y", data=anscombe.query("dataset == 'I'"), scatter_kws=&#123;"s": 80&#125;) 非线性相关 1sns.residplot(x="x", y="y", data=anscombe.query("dataset == 'II'"),scatter_kws=&#123;"s": 80&#125;) 局部加权线性回归（LOWESS）在实际例子中，全部数据建模通常不是一条直线，LOWESS 主要思想是选取一定比例的局部数据，在这部分数据中拟合多项式回归曲线。 1sns.lmplot(x="total_bill", y="tip", data=tips,lowess=True) 一些二阶和高阶的关系探索在实践中，还会存在一些二次和其他关系中的数据存在，文章中说可以通过直接线性拟合预测得到的结果融入到 model 中.例如作为 xgb 或者其他模型的一个特征，这点还不是很理解。可以通过设置 order=N 来控制。 1sns.lmplot(x="x", y="y", data=anscombe.query("dataset == 'II'"),order=2, ci=None, scatter_kws=&#123;"s": 80&#125;) 更高维度上的线性回归如果我们想看这两个变量受第三或更多个变量线性关系的影响，可以利用 lmplot()，像前面 factorplot 一样加入 hue 或 col 参数来展示。 1sns.lmplot(x="total_bill", y="tip", hue="smoker", data=tips) 调节散点的形状 1sns.lmplot(x="total_bill", y="tip", hue="smoker", data=tips, markers=["o", "x"], palette="Set1") 加入 hue, row 和 col 参数，展示 5 个变量的线性关系 1sns.lmplot(x="total_bill", y="tip", hue="smoker", col="time", row="sex", data=tips) jointplot 与 pairplot 上的线性回归jointplotjointplot 可以通过传入 kind=’reg’ 显示线性回归 1sns.jointplot(x="total_bill", y="tip", data=tips, kind="reg") pairplotpariplot 是建立在 pairgrid 基础上的，展示变量的不同配对的结果，也可以通过传入 hue 参数调节第三个变量的线性关系。 1sns.jointplot(x="total_bill", y="tip", data=tips, kind="reg") 1sns.pairplot(tips, x_vars=["total_bill", "size"], y_vars=["tip"],hue="smoker", size=5, aspect=.8, kind="reg") 主要参考文章：https://zhuanlan.zhihu.com/p/27593869https://www.kesci.com/home/project/59f687e1c5f3f511952baca0]]></content>
      <categories>
        <category>可视化</category>
        <category>seaborn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas 中 cut 后最小区间的开区间的取值]]></title>
    <url>%2Farchives%2F23f0bd8b.html</url>
    <content type="text"><![CDATA[Pandas 对 Array 进行 cut 操作时，默认情况下，产生的每个区间范围都是左开右闭的，在最小区间的开区间会出现比最小值小的情况，防止在计数时最小值取不到。 比如下面最小区间里的 -0.009 。 1234567891011In [1]: import numpy as npIn [2]: import pandas as pdIn [3]: arr = np.arange(0, 10)In [5]: pd.cut(arr, bins=10)Out[5]:[(-0.009, 0.9], (0.9, 1.8], (1.8, 2.7], (2.7, 3.6], (3.6, 4.5], (4.5, 5.4], (5.4, 6.3], (6.3, 7.2], (7.2, 8.1], (8.1, 9.0]]Categories (10, interval[float64]): [(-0.009, 0.9] &lt; (0.9, 1.8] &lt; (1.8, 2.7] &lt; (2.7, 3.6] &lt; ... &lt; (5.4, 6.3] &lt; (6.3, 7.2] &lt; (7.2, 8.1] &lt; (8.1, 9.0]] -0.009 这个值通过 $arr.min() - (arr.max() - arr.min()) / 1000​$ 计算得到的。$0 - (9 - 0) / 1000​$ 得到的。与 Bins 数量无关，只与 array 的最小和最大值有关。 下面测试 Bins 数量分别是 10, 100 和 1000 时的情况： 123456789101112131415In [6]: np.random.seed(1) ...: a = np.random.randint(5000, size=1000) ...: bins = [10, 100, 1000]In [7]: for b in bins: ...: temp = pd.cut(a, bins=b) ...: print(temp.categories[0]) ...: print(a.min() - (a.max() - a.min()) / 1000)(-2.995, 501.5]-2.995(-2.995, 51.95]-2.995(-2.995, 6.995]-2.995 类似，如果加入 right=False 参数，最大区间开区间会比最大值还大。 计算公式为 $arr.max() + (arr.max() - arr.min()) / 1000$ 1234567891011In [8]: for b in bins: ...: temp = pd.cut(a, bins=b, right=False) ...: print(temp.categories[-1]) ...: print(a.max() + (a.max() - a.min()) / 1000) ...:[4497.5, 5001.995)5001.995[4947.05, 5001.995)5001.995[4992.005, 5001.995)5001.995]]></content>
      <categories>
        <category>Python</category>
        <category>Pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的距离计算和相似性度量]]></title>
    <url>%2Farchives%2Faf94264b.html</url>
    <content type="text"><![CDATA[在机器学习中，常常需要计算样本在空间中的距离或者不同概率分布下的随机变量的相似性，用来评价差异的大小，在聚类算法/评价文本间相似性/CNN 中目标检测的评价指标中发挥作用。在网上发现一篇总结较好的文章 机器学习中距离和相似性度量方法，结合邹博机器学习课中对距离的讲解，做一下总结。 定义距离函数需要满足以下的准则： 注：KL 散度并不满足对称性，即 $KL(p|| q)\neq KL(q||p)$。 闵科夫斯基距离$P$ 和 $Q$ 代表空间 $R^n$ 中的样本，$i$ 代表不同的维度。闵科夫斯基距离用来度量空间中样本的相似度。 闵科夫斯基距离又可以看作是向量 $P$ 和 $Q$ 之间的 $p$ 范式。 特例曼哈顿距离当 $p=1$ 时，闵科夫距离退化为曼哈顿距离，也就是 $L1$ 距离。$d = \Sigma_i^n|x_i - y_i|$，假设两个点 $x= (1, 3)$，$y = (4, 6)$，曼哈顿距离 $d = |1-4| + |3-6|$，在坐标系上体现为先沿 $x$ 走，再沿 $y$ 轴走一段。于是有了下图。 欧式距离我们是生活在欧式空间中的，所以对欧式距离十分熟悉了。 当 $p = 2​$ 时， $d = \sqrt{\Sigma_i^n(x_i - y_i)^2}​$，即欧式距离，$L2​$ 距离。关于 $L1​$ 和 $L2​$ 距离在这篇文章中 L1 L2 正则化理解 有详细说明。另外。$L1​$ 距离会随着坐标系旋转而发生变化，但是 $L2​$ 距离则不会。旋转后，$x​$ 与 $y​$ 的 $L1​$ 距离由 $2a​$ 变成 $\sqrt{2}a​$。 切比雪夫距离当 $p$ 趋近于无穷大时，闵科夫斯基距离演变成切比雪夫距离。 证明方式，假设 $max|x_i - yi| = |x_1 - y_1|$ (\Sigma_{i=1}^n{(x_1-y_1)^p+ (x_2-y_2)^p+...(x_n-y_n)^p})^\frac{1}{p}\\ =(\Sigma_{i=1}^n{(x_1-y_1)^p(1 + (\frac{x_2-y_2}{x_1-y_1})^p+...(\frac{x_2-y_2}{x_n-y_n})^p})^\frac{1}{p}\\因为 p 趋近于无穷大，$(\frac{x_2-y_2}{x_n-y_n})^p$ 等于 0，最终式子的结果等于 $|x_1 - y_1|$。 切比雪夫距离表征了样本最大的维度差距有多大。 等值线在平面上，距离某个点欧式距离相等的所有点构成一个圆，下面则是 p 取其他数值时，等值线的情况。当 p 取无穷大时，等值线构成正方形。 小技巧： 在面对两类物品 A 和 B 时，A 类物品的衰减速率慢，B 类物品的衰减速率快。现在有新物品 C，需要判断 C 相对于 A 或者 B 是否是异常值？如果采用闵科夫斯基距离度量时，与衰减慢的物品 A 计算距离时，幂次方 p 往往取的小一些，衰减快的幂次方 p 往往取得更大（邹博机器学习第 7 课聚类（上）56’）。 评价直观，无关数据分布，维度间幅值过大时，需要进行 z-transform 处理，在表示相关性数据时可能不合适。 马氏距离马氏距离利用 Cholesky 分解对协方差矩阵分解 $\Sigma = LL^T$，得到矩阵 L，对样本点 $x$ 做转换，$z = L^{-1}(x - u)$，转换后的欧式距离实际就是原样本的马氏距离。 协方差矩阵的由来方差如果样本只有一个随机变量 X，经过采样后得到的值为 $X={x_1, x_2, …, x_n}$，可以用方差来评价数据间的离散程度。$\sigma = E[(X-\mu)^2]$ 协方差如果样本有两个随机变量 X, Y，比如说身高和体重，经过采样后得到 $X={x_1, x_2, …, x_n}$，$Y={y_1, y_2, …, y_n}$，用协方差表示这两个维度间是否存在关联，如果协方差为正值，代表正相关。协方差计算公式为 $Cov[X, Y] = E[(X-\mu_x)(Y-\mu_y)]$。 协方差矩阵如果样本有三个或三个以上变量，我们用协方差矩阵来表示多变量间的相关性。 X Y Z X $Cov(X, X)$ $Cov(X, Y)$ $Cov(X, Z)$ Y $Cov(Y, X)$ $Cov(Y, Y)$ $Cov(Y, Z)$ Z $Cov(Z, X)$ $Cov(Z, Y)$ $Cov(Z, Z)$ $Cov(X, X)$ 也就是 X 的方差。 对称正定矩阵的 LU 分解LU 分解由高斯消元法演化而来，对称矩阵的 LU 分解又叫 Cholesky 分解，如果矩阵 A 对称而且正定，那么存在一个下三角矩阵使得 $A = LL^T​$ 成立。 假设有一个对称正定矩阵 \begin{pmatrix} 1&2\\2&5 \end{pmatrix}经过 Cholesky 分解后可以得到下式： \begin{pmatrix} 1&2\\2&5 \end{pmatrix} = \begin{pmatrix} 1&0\\2&1 \end{pmatrix} \begin{pmatrix} 1&2\\0&1 \end{pmatrix}计算过程对协方差矩阵做 Cholesky 分解后，$\Sigma = LL^T$，对样本 x 做变换 $z = L^{-1}(x - u)$，对 z 求欧式距离 $d = \sqrt{z^Tz}$ 即原来样本的马氏距离。 解释 从马氏距离上看，绿黑的距离要小于红黑距离，而欧式距离则恰好相反。马氏距离适用于存在明显相关性的两组数据之间求距离。 向量内积/余弦相似度/皮尔逊相关系数向量内积 直观上来说，不考虑长度时，向量内积越大，越相似。x 高的地方 y 也比较高，x 低的地方，y 也比较低。但是内积的结果没有界限，在除以向量长度之后就变成了余弦相似度。 余弦相似度 两个向量越靠近，夹角越小，余弦值越大，因此，余弦相似度可以作为相似度度量的标准。余弦相似度消除向量幅值的影响，如果构成的夹角（方向）不变，但是某条边特别长，也是可以说明二者相似。 余弦相似度常常用在比较两个文本之间的相似度，余弦值越接近于 1，夹角越小，越相似。 比如在比较 A 和 B 两个文本时，所有词的词典记为 $dict = {t_1, t_2, …t_k}$，在表示 A 和 B 时，将词 $t_i$ 在 A 中的出现次数记为 $n_i$， 词 $t_i$ 在 B 中的出现次数记为 $m_i$，A 向量可以表示为 ${n_1, n_2, … n_k}$，B 向量可以表示为 ${m_1, m_2, m_k}$。这样便可以计算两篇文档的相似度了。假设 A 比 B 文档的字数多很多，但内容相似，在空间上表现为二者夹角很小，但是其中有一条边很长，二者的欧式距离通常会很大。欧式距离从空间距离去衡量相似性，而余弦相似度则从两个向量的夹角衡量。 余弦相似度会受到平移的影响，如果将向量中的每个维度都加 1，那么余弦相似度便会受到影响。 余弦相似度与欧式距离的关系如果用 $cos(A, B)$ 表示两个向量的余弦相似度，用 $1-cos(A, B)$ 表示余弦距离。在对向量做归一化后，余弦距离和欧式距离呈单调关系。二者效果是一致的，在 word2vec 中，会对向量做归一化。 ||A-B||_2 = \sqrt{2(1-cos(A, B))}$||A-B||_2​$ 表示欧式距离。在求余弦相似度复杂度过高时，可以利用 KdTree 或 BallTree 算法（对高维向量友好）求欧式距离，转换成余弦相似度 皮尔逊相关系数如果我们要比较的不是向量间的距离，而是通过采样产生的随机变量间的相似性，这时就要用到皮尔逊相关系数。随机变量个数对应向量空间中的维数。 上述式子可以理解成，两个变量的协方差除以标准差的乘积，得到的数值范围是 [-1, 1]，数值 0 代表两个变量不相关。 Corr(x, y) = \frac{E[(X-\mu_x)(Y-\mu_y)]}{\sigma_x\sigma_y}如果把 x 看成是 n 维空间中的一个向量，y 也是一个向量，那么可以通过计算欧式距离来表示相似度；如果 X，Y 看成是随机变量，那么可以通过计算相关系数来得到相似性，实际上，这也就是如何解释样本的问题，选择相应的算法，得到某种合适的度量。 皮尔逊相关系数度量了随机变量 $X​$ 和 $Y​$ 之间的一阶线性关系。皮尔逊相关系数等于 0 只能说明两个变量不相关，但是不能说明二者独立。 如图，在 [-1, 1] 上的二次函数 $y=x^2​$，皮尔逊相关系数等于 0，但二者并不独立。 皮尔逊相关系数与余弦相似度的关系当两个随机变量的期望 $\mu_x = \mu_y = 0​$ 时，皮尔逊相关系数即代表余弦相似度。 余弦相似度在文档相似度上用得比较多，常常认为文档之间是相互独立的。 皮尔逊相关系数和余弦相似度描述的都是线性关系，在比如树模型这类的非线性模型的特征选择时，就不能使用了。 皮尔逊相关系数的数据要求和局限性 为避免这些局限性，可以采用 Spearman Correlation Coefficient（斯皮尔曼相关系数），可以参考文章。 Spearman Correlation Coefficient 指 X、Y 两个变量次序的皮尔逊相关系数，无论两个变量的数据如何变化，符合什么样的分布，我们只关心每个数值在变量内的排列顺序。 Jacard 相似度$J(A, B) = \frac{A\cap B}{A\cup B}$。两个集合之间的交并比。 比如评价一个推荐系统时，用真实商品列表与预测商品列表的交集数量除以并集数量作为评价标准。或者在 fastRCNN 中，常常用用户标签位置与算法识别的区域二者之间的交集除以二者之间并集（面积比）作为指标。 在 Jacard 相似度中，可以指定样本的权重，权重与流行度呈反比，流行度越低，权重越高，越说明样本间相似。 KL 散度KL 散度和卡方检验常常用来衡量两个样本分布之间的距离。 熵的直观理解熵度量了事物的不确定性。如果有一个句子，如何衡量词语给予的信息量呢？直觉上，我们认为，一个词语出现概率越低，越可以根据这个词推测这篇文章的类别，比如，根据“黑洞”我们可以推测这是篇科普文章，而“吃什么”这种常见的词，蕴含的信息量是很少的。 所以，如果想要定义一个新的物理量，叫事物的不确定性。经验上讲，事情发生的概率越小，事物的不确定性越大，带来的信息量越大，与概率 p(x) 呈反比；第二点，假设 x 和 y 相互独立，$p(x, y) = p(x)p(y)​$，我们希望 x 和 y 的不确定性是可以累加的，$h(x, y) = h(x) + h(y)​$，而 log 函数恰好可以满足这种累加性，所以有了新的度量 $-lnp(x)​$；最后我们希望获得的将是平均信息量，也就是求 $-lnp(x)​$ 的数学期望：$H[x] = -\Sigma_x p(x)ln p(x)​$，这就是熵的概念，代表了随机事件 X 的信息量。 总结来说，log 函数使熵变得可加，负号保证与经验相符，最终得到的是数学期望。 交叉熵交叉熵的定义： H(p, q) = -\Sigma_x p(x)log q(x)p(x) 为机器学习中的样本 label，q(x) 为模型的评估，分别代表训练样本和模型预测的分布。代表的意义是根据预测结果来衡量训练样本的不确定性（熵）。 KL 散度用 $H(p, q)​$ 减去训练样本的熵值 $H(p)​$ 得到两个熵之间的差值，称为相对熵 $D(p||q)​$，用来评估两个分布的差异性。 相对熵又称 KL 散度，设 p(x)、q(x) 是两个概率分布，则 p 对 q 的相对熵是： D(p||q) = H(p, q) - H(p) = -\Sigma_x p(x)logq(x) + \Sigma_x p(x)logp(x) = \Sigma_x p(x)log\frac{p(x)}{q(x)} =E_{p(x)}log\frac{p(x)}{q(x)}相对熵一定是大于等于 0 的，可以用 Jensen 不等式证明。也可以这样理解，用预测样本 q(x) 衡量 p(x) 的不确定性是要大于 p(x) 本身的不确定性，不确定性又可以用熵度量，$H(p, q) &gt;= H(p)​$，当且仅当两个分布相等时，等号成立。 离散变量的解释假设两个随机变量 $X={3, 4, 3, 2, 2, 4}$，$Y = {5, 2, 3, 4, 2 ,3 ,5}$，分布服从 $p(x)$ 和 $q(x)$ 的分布。相似性的度量方式可以用 Jacard 相似度，也可以用下面的方法。 统计样本中所有元素的频数，计算概率 $All = {2, 3, 4, 5}$，$X = {1/3, 1/3, 1/3, 0}$，$Y= {2/7, 2/7, 1/7, 2/7}$，计算 X 对 Y 的相对熵，度量两个随机变量间距离的远近。$KL(p||q) = \Sigma_ip(X=x_i)ln (\frac{p(X=x_i)}{q(Y=y_i)})$ 连续变量上的解释假设有一个未知分布 p(x)，q(x) 是对 p(x) 的一个近似，按照 q(x) 对该随机变量的各个值进行编码，会比真实分布 p(x) 额外长一些，长出来的部分就是 KL 散度。 在 logistic 回归或 Softmax 回归中，最后输出节点上的值表示这个样本分到该类的概率，我们能做的就是尽量减少样本的交叉熵之和，也就是交叉熵损失，即真实分布 $y^{(i)}​$ 与预测分布 $h_\theta(x^{(i)})​$ 的交叉熵损失。 l(\theta) = -(\sum_{i=1}^m y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))性质 不满足对称性，一般情况下，$KL(p||q)$ 不等于 $KL(q||p)$，当且仅当 p 和 q 完全相等，p 和 q 的相对熵为 0，二者才相同。 Hellinger 距离既然 $p$ 和 $q$ 两个分布的 KL 散度不满足对称性，于是就有了下面的定义 D_{\alpha}(p||q) = \frac{2}{1-\alpha ^2}(1-\int p(x)^{\frac{1+\alpha}{2}}q(x)^{\frac{1-\alpha}{2}}dx)当 $\alpha=0$ 时，通过推导可以得到下式，下面的式子本质上就是 $\sqrt{p(x)}$ 和 $\sqrt{q(x)}​$ 的欧式距离（差值的平方和）。 利用随机森林建立计算样本间相似度 这种相似度的判断绕过了样本特征，直接用随机森林的结果进行相似性度量。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用 python -m module]]></title>
    <url>%2Farchives%2F1a9e9ddf.html</url>
    <content type="text"><![CDATA[最近在看 MCscan 代码时，发现对 python -m module 的使用不是很了解，参考 充分理解 python -m mod 总结如下。 Python —helppython --help 给出的说明 -m mod : run library module as a script (terminates option list) 按照脚本的方式执行模块。这里把执行脚本和 python -m module 做比较 模块导入机制sys.path 返回一个列表，存储模块的搜索路径；sys.modules 返回一个字典，存储所有已经加载到内存中的模块；&lt;module&gt;.__dict__ 则以字典的形式返回模块的属性，比如，__name__，__doc__ 等等。 当需要 import 一个模块时，首先会从 sys.modules 中查找，如果没有，则从 sys.path 中查找，并加入到内存，常见的 ImportError: No module named &#39;xxx&#39; 就是在 sys.path 中没有找到路径。 将某一路径加入 sys.path 方法有两种： export PYTHONPATH=DIR:$PYTHONPATH sys.path.append(DIR) path.py 内容如下 123$ cat path.pyimport sysprint(sys.path) 执行 python -m path 后，并没有在 sys.path 中发现当前路径。 12$ python -m path[&apos;&apos;, &apos;C:\\Users\\hotheat\\Anaconda3\\python36.zip&apos;,...] 执行 export PYTHONPATH=/c/Data/Test/python-m/pythonpath/:$PYTHONPATH，再执行 python -m path，则在输出中发现当前路径。 12$ python -m path[&apos;&apos;, &apos;C:\\c\\Data\\Test\\python-m\\pythonpath\\:&apos;, &apos;C:\\Users\\hotheat\\Anaconda3\\python36.zip&apos;,...] 这里之所有没用 python path.py 是因为直接运行脚本，会将当前路径加入到 sys.path 中。 有无 -m 参数的比较modules.py 脚本内容如下 123456import mathimport sysprint(sys.path)print(sys.modules)print(sys.argv)print(math.__dict__[&apos;name&apos;]) 12python modules.py # 直接执行脚本python -m modules # 当作模块方式执行 注意，在执行 python -m modules 时，需要去掉 .py 后缀。 执行 python modules.py 的输出 1234[&apos;C:\\Data\\Test\\python-m&apos;, &apos;C:\\Users\\hotheat\\Anaconda3\\python36.zip&apos;,...]&#123;...,&apos;sys&apos;: &lt;module &apos;sys&apos; (built-in)&gt;, &apos;__main__&apos;: &lt;module &apos;__main__&apos; from &apos;modules.py&apos;&gt;, &apos;math&apos;: &lt;module &apos;math&apos; (built-in)&gt;, ...&#125;[&apos;modules.py&apos;]math 执行 python -m modules 的输出 1234[&apos;&apos;, &apos;C:\\Users\\hotheat\\Anaconda3\\python36.zip&apos;,...]&#123;..., &apos;sys&apos;: &lt;module &apos;sys&apos; (built-in)&gt;, &apos;__main__&apos;: &lt;module &apos;modules&apos; from &apos;C:\\Data\\Test\\python-m\\modules.py&apos;&gt;, &apos;runpy&apos;: &lt;module &apos;runpy&apos; from &apos;C:\\Users\\hotheat\\Anaconda3\\lib\\runpy.py&apos;&gt;, &apos;pkgutil&apos;: &lt;module &apos;pkgutil&apos; from &apos;C:\\Users\\hotheat\\Anaconda3\\lib\\pkgutil.py&apos;&gt;, &apos;math&apos;: &lt;module &apos;math&apos; (built-in)&gt;&#125;[&apos;C:\\Data\\Test\\python-m\\modules.py&apos;]math 不同之处在于： python modules.py 默认会将脚本所在路径加入 sys.path 中，当作模块执行时则不会。 执行 python -m module 的 sys.modules 中的 __main__ 是绝对路径，而直接执行脚本中 sys.modules 中的 __main__ 是当前目录下脚本所在位置的相对路径。 同样，python modules.py 中的 sys.argv 产生的也是相对路径，而 python -m modules 中的 sys.argv 输出的是绝对路径。 同时，也可以发现，在 import math 后，在 sys.modules 中发现了已经导入的 math 包，利用 __name__ 属性可以返回包的名称。 参考： https://segmentfault.com/a/1190000012672874]]></content>
      <categories>
        <category>Python</category>
        <category>Base</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[np.nan/float(nan)/None 区别]]></title>
    <url>%2Farchives%2F17d2768e.html</url>
    <content type="text"><![CDATA[np.nan/float(‘nan’)Typenp.nan 的返回类型是 float，float(‘nan’) 的返回类型是 nan。这里对 float(‘nan’) 的理解是一个不可变的没有值的数。 12print(type(np.nan))print(float('nan')) &lt;class &#39;float&#39;&gt; nan 判断是否是 nan 的方法，有下面三种。 1234import mathprint(math.isnan(float('nan')))print(np.isnan(float('nan')))print(pd.isnull(float('nan'))) True True True id 是否相同既然不可变，每次 float(‘nan’) 都生成不同的 id。 123nans = [float('nan') for i in range(2)]print(list(map(id, nans)))print(nans) [163139752, 163045336] [nan, nan] math.nan 和 numpy.nan 是库中 unique 的 nan，每次的 id 都是相同的。 123nans = [np.nan for i in range(2)]print(list(map(id, nans)))print(nans) [78245056, 78245056] [nan, nan] 123nans = [math.nan for i in range(2)]print(list(map(id, nans)))print(nans) [3942944, 3942944] [nan, nan] 但是每次产生的 float(‘nan’) 与 np.nan 的 id 并不相同。 1list(map(id, [float('nan'), np.nan])) [163139752, 78245056] 比较操作均返回 False因为 nan 的值不存在，== 判断对于 float(‘nan’) 和 np.nan 均返回 False。 12print(float('nan') == float('nan'))print(np.nan == np.nan) False False is 对二者返回值不同因为 float(‘nan’) 每次返回的 id 不同，is 操作自然返回 False；np.nan 返回的 id 相同，is 操作返回 True. 12print(float('nan') is float('nan'))print(np.nan is np.nan) False True in 操作可以看出二者不同12nans = [np.nan for _ in range(2)]print(np.nan in nans) True 12nans = [float('nan') for _ in range(2)]print(float('nan') in nans) False 总结一下，判断一个值是否是 nan 时，有三种方法，分别是 np.isnan(), math.isnan(), pd.isnull()；判断 np.nan 是否相等时用 is；== 对于 np.nan 和 float(‘nan’) 都返回 False.网上也有解释， np.nan 重载了“==” 运算符，dir(np.nan)就能看到”_eq_“ 更具体的参考链接1，链接2。 不产生 np.nan 的方法series.astype(str)12s = pd.Series([np.nan, float('nan')])s = s.astype(str) 12for i in s: print(type(i), i) &lt;class &#39;str&#39;&gt; nan &lt;class &#39;str&#39;&gt; nan StringIO12from pandas.compat import StringIOdf = pd.read_csv(StringIO(temp), names=[0,1,2,3,4], keep_default_na=False) 通过 na_values 参数指定特定字符为 nan 值df = pd.read_csv(StringIO(temp), names=[0,1,2,3,4], keep_default_na=False, na_values=[&#39;NA&#39;]) 参考链接 np.nan 与 None 的比较这篇文章介绍的很详细。 数据类型None 是一个 python 特殊的数据类型， 但是 NaN 却是用一个特殊的 float。 np.nan 和 None 都能作为字典的 keyfloat(‘nan’) 同样也可以作为 key，但新建的 float(‘nan’) 值与 id 与原来的并不相同，在原字典中查找不到 key，而 np.nan 和 None 在原字典中可以查找到。 12a, b, c = float('nan'), np.nan, Noned = &#123;a: 1, b:2, c:3&#125; 1234print(d)print(d[np.nan])print(d[None])print(d[float('nan')]) {nan: 1, nan: 2, None: 3} 2 3 --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-17-d4f95daf40c7&gt; in &lt;module&gt; 2 print(d[np.nan]) 3 print(d[None]) ----&gt; 4 print(d[float(&#39;nan&#39;)]) KeyError: nan Series 函数中的表现 用 Series.map 对 None 进行替换时，会“顺便”把 NaN 也一起替换掉；NaN 也会顺便把 None 替换掉。 在 Series.replace 中的表现与 map 类似。 对函数的支持numpy 和 pandas 有不少函数可以自动处理 np.NaN，但是不支持 None。如 np.nansum()，pd.cut() 等。 对容器数据类型的影响 混入 None，会使 dtype 变成 object； 混入 np.nan, np.NaN 会将原本的 int 类型转成 float；pandas 会自动将 None 替换成了 np.NaN，而且，Series 只能用 object 类型容纳，都是 None 及都是 bool 类型时，不会进行转化。 12345print(pd.Series([1, None]).dtype)print(pd.Series([1, np.nan]).dtype)print(pd.Series(['a', np.nan]).dtype)print(pd.Series([True, np.nan]).dtype)print(pd.Series([None, np.nan]).dtype) float64 float64 object object float64 12print(np.array([1, None]).dtype)print(np.array([1, np.nan]).dtype) object float64 等值性比较np.nan == np.nan 返回 FalseNone == None 返回 True 1None == None True None 在 Series 中的基准会不一致 1pd.Series([None, 1]) == pd.Series([None, 1]) 0 False 1 True dtype: bool 专门的基准判断函数numpy.testing.assert_equals()Series.equals(Series) df.merge()df.merge() 中，如果一列中不一致，会以左侧列为准。 df.groupby()DataFrame.groupby会忽略分组列中含有 None 或者 NaN 的记录 12df = pd.DataFrame(&#123;'A':[None, np.nan, 1, 1, 2], 'B':['a', 'a', 'a', 'b', 'b']&#125;) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle A B 0 NaN a 1 NaN a 2 1.0 a 3 1.0 b 4 2.0 b 1df.groupby(['A', 'B']).apply(len) A B 1.0 a 1 b 1 2.0 b 1 dtype: int64 导入数据库需要将 np.nan 替换成 Nones.where(s.notnull(), None) 1s = pd.Series([np.nan, 1]) 1s 0 NaN 1 1.0 dtype: float64 1s.where(s.notnull(), None) 0 None 1 1 dtype: object s[s.isnull()] = None 和 s.replace([NaN], None) 会自动把 None 替换成了 NaN。使用时需要注意。 1s[s.isnull()] = None 1s 0 NaN 1 1.0 dtype: float64 1s.replace([np.nan], None) 0 NaN 1 1.0 dtype: float64]]></content>
      <categories>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[合并列表的方法]]></title>
    <url>%2Farchives%2F96be41c1.html</url>
    <content type="text"><![CDATA[Python 中列表合并（扁平化）的方法有多种，哪种写法更优雅以及性能如何？ 123from functools import reduceimport operatorimport itertools 1l = np.random.randint(0, 10, size=(5, 2)).tolist() 1l 1[[6, 0], [0, 4], [6, 6], [4, 6], [9, 4]] 列表解析1[item for sublist in l for item in sublist] 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] Sum 函数sum(iterable, start) The sum() function adds the items of an iterable and returns the sum.sum() Parametersiterable - iterable (list, tuple, dict etc) whose item’s sum is to be found. Normally, items of the iterable should be numbers.start (optional) - this value is added to the sum of items of the iterable. The default value of start is 0 (if omitted) 第一个参数是迭代器，如果迭代器中的是 string，使用 &#39;&#39;.join(iterable) 1sum([2.5, 3, 4, -5], 10) 114.5 1sum(l, []) 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] Recucelambda1reduce(lambda x,y:x+y, l) 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] operator.add1reduce(operator.add, l) 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] map1new=[] 1_ = list(map(new.extend, l)) 1new 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] 需要重新声明变量 new，map 函数的返回值不起作用，代码上不够简洁。 itertools常见的也是官方推荐的做法。 1list(itertools.chain(*l)) 1[6, 0, 0, 4, 6, 6, 4, 6, 9, 4] 利用迭代器的解包利用迭代器的解包可以合并两个列表或者两个字典 1234567891011In [1]: l1 = ['a', 'b', 'c']In [2]: l2 = [1, 2, 3]In [3]: [*l1, *l2]Out[3]: ['a', 'b', 'c', 1, 2, 3]In [4]: l2 = [1, 2, [3]]In [5]: [*l1, *l2]Out[5]: ['a', 'b', 'c', 1, 2, [3]] 合并字典 123456In [6]: d1 = &#123;'a':1, 'b':2&#125;In [7]: d2 = &#123;'c':3, 'd':4&#125;In [8]: &#123;**d1, **d2&#125;Out[8]: &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125; Numpyconcatenate1np.concatenate(l) 1array([6, 0, 0, 4, 6, 6, 4, 6, 9, 4]) flatten1np.array(l).flat # 返回迭代器，支持索引 1&lt;numpy.flatiter at 0x46df750&gt; 1np.array(l).flatten() 1array([6, 0, 0, 4, 6, 6, 4, 6, 9, 4]) Perplot 性能对比12345678910111213141516171819202122232425262728293031323334353637383940414243444546from itertools import chainfrom functools import reducefrom collections import Iterable # or from collections.abc import Iterableimport operatorfrom iteration_utilities import deepflattendef nested_list_comprehension(lsts): return [item for sublist in lsts for item in sublist]def itertools_chain_from_iterable(lsts): return list(chain.from_iterable(lsts))def pythons_sum(lsts): return sum(lsts, [])def reduce_add(lsts): return reduce(lambda x, y: x + y, lsts)def pylangs_flatten(lsts): return list(flatten(lsts))def flatten(items): """Yield items from any nested iterable; see REF.""" for x in items: if isinstance(x, Iterable) and not isinstance(x, (str, bytes)): yield from flatten(x) else: yield xdef reduce_concat(lsts): return reduce(operator.concat, lsts)def iteration_utilities_deepflatten(lsts): return list(deepflatten(lsts, depth=1))from simple_benchmark import benchmarkb = benchmark( [nested_list_comprehension, itertools_chain_from_iterable, pythons_sum, reduce_add, pylangs_flatten, reduce_concat, iteration_utilities_deepflatten], arguments=&#123;2**i: [[0]*5]*(2**i) for i in range(1, 13)&#125;, argument_name='number of inner lists')b.plot() 从结果来看，itertools.chain.from_iterable, iteration_utilities.deepflatten 是耗时最短的。 参考：https://www.chenyudong.com/archives/python-make-flat-list-of-list.htmlhttps://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists]]></content>
      <categories>
        <category>Python</category>
        <category>List</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于排序的代码]]></title>
    <url>%2Farchives%2F87a31189.html</url>
    <content type="text"><![CDATA[排序算法按平均情况下的时间复杂度来分： $O(n^2)$ 三种排序具体代码见 https://github.com/hotheat/JiKeExcercise/blob/master/python-code/11_sorts/sort_On2.py 冒泡排序 每次冒泡都使未排序区间中的最大元素移至已排序区间，直至没有数据交换为止。 插入排序 将元素插入到已排序区间中，通过元素比较和数据搬移（交换）来实现。 选择排序 每次寻找最小的元素放到已排序区间的末尾。 $O(nlogn)$ 两种排序方法的代码见 https://github.com/hotheat/JiKeExcercise/blob/master/python-code/12_sorts/sortOnlogn.py 归并排序 由下到上，先处理子问题，再合并（merge 函数），合并时需要额外的空间。无法原地排序。 代码中用两种不同方法表示了 merge 函数。 快速排序 从上到下，先分区（partition 函数），再处理子问题。可以通过交换的方法搬移数据，实现原地排序。 非原地排序示意图 原地排序示意图 代码中分别是： 有 partition 分区函数的非原地排序，QuickSortNoinplace_1； 无 partition 分区的非原地排序，QuickSortNoinplace_2； 每次选取最后一个元素作为 pivot 的原地排序，QuickSortInplace； 随机选取元素作为 pivot 的原地排序，QuickSortInplace_random。 利用快排的思想可以实现在 $O(n)$ 时间复杂度内找到无序序列中第 k 大元素，通过分区后，判断 pivot 位置是否与 k 相等，如果相等，返回该位置，否则，在理想情况下，会将查找范围缩小 1/2，直至查找范围区间等于 1。 代码实现：https://github.com/hotheat/JiKeExcercise/blob/master/python-code/12_sorts/largest_k_number.py $O(n)$ 计数排序 要排序的数组是 2，5，3，0，2，3，0，3。 第一步，先计数 第二步，计数累加 第三步，根据累加结果做索引，填充数组。 具体代码 https://github.com/hotheat/JiKeExcercise/tree/master/python-code/13_sorts 参考网址： https://time.geekbang.org/column/126]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV 源码编译安装]]></title>
    <url>%2Farchives%2Faa090cce.html</url>
    <content type="text"><![CDATA[在没有特殊情况下，使用 pip install opencv-python 安装是最简单的方式。 但是，在做 Face Morpher 项目时，安装人脸检测开源库 stasm 时要求从源码安装 Opencv，在按照 windows 下的安装教程安装失败后，从 Vultr 租的服务器，新加坡节点，用干净的系统重新安装，系统版本 Ubuntu 16.04，Python3 版本 3.5.2。下面是安装过程。 1234567891011121314151617181920212223242526272829303132333435363738394041sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade &amp;&amp; sudo apt-get autoremovesudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-devsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-devsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libxvidcore-dev libx264-devsudo apt-get install libgtk-3-devsudo apt-get install libatlas-base-dev gfortransudo apt-get install python2.7-dev python3.5-dev# download opencvcd ~wget -O opencv.zip https://github.com/opencv/opencv/archive/3.3.0.zipapt install unzipunzip opencv.zip# download opencv_contribcd ~wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/3.3.0.zipunzip opencv_contrib.zipsudo apt-get install python-pip &amp;&amp; pip install --upgrade pipsudo pip install virtualenv virtualenvwrappersudo rm -rf ~/.cache/pipcd ~echo -e "# virtualenv and virtualenvwrapper\nexport WORKON_HOME=$HOME/.virtualenvs\nsource /usr/local/bin/virtualenvwrapper.sh" &gt;&gt; ~/.bashrcwget https://bootstrap.pypa.io/ez_setup.py -O - | pythonsource ~/.bashrcmkvirtualenv cv -p python3workon cvpip install numpycd ~/opencv-3.3.0/mkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D INSTALL_C_EXAMPLES=OFF \ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.3.0/modules \ -D PYTHON_EXECUTABLE=~/.virtualenvs/cv/bin/python \ -D BUILD_EXAMPLES=ON ..make 报错： 12345678[ 85%] Building CXX object modules/python3/CMakeFiles/opencv_python3.dir/__/src2/cv2.cpp.oIn file included from /root/.virtualenvs/cv/lib/python3.5/site-packages/numpy/core/include/numpy/ndarraytypes.h:4:0, from /root/.virtualenvs/cv/lib/python3.5/site-packages/numpy/core/include/numpy/ndarrayobject.h:18, from /root/opencv-3.3.0/modules/python/src2/cv2.cpp:10:/root/.virtualenvs/cv/lib/python3.5/site-packages/numpy/core/include/numpy/npy_common.h:17:5: warning: "NPY_INTERNAL_BUILD" is not defined [-Wundef] #if NPY_INTERNAL_BUILD ^c++: internal compiler error: Killed (program cc1plus) ​ 解决，参考 https://stackoverflow.com/questions/44844646/npy-common-h-npy-internal-build-is-not-defined 12345cd /root/.virtualenvs/cv/lib/python3.5/site-packages/numpy/core/include/numpy/vi npy_common.h# 将 #if NPY_INTERNAL_BUILD 替换成#ifndef NPY_INTERNAL_BUILD#define NPY_INTERNAL_BUILD 报错 12345c++: internal compiler error: Killed (program cc1plus)Please submit a full bug report,with preprocessed source if appropriate.See &lt;file:///usr/share/doc/gcc-5/README.Bugs&gt; for instructions.modules/python3/CMakeFiles/opencv_python3.dir/build.make:321: recipe for target 'modules/python3/CMakeFiles/opencv_python3.dir/__/src2/cv2.cpp.o' failed 可能是 swap 分区不够了，参考链接。 1234(cv) root@vultr:~/opencv-3.3.0/build# free -h total used free shared buff/cache availableMem: 488M 50M 309M 5.5M 128M 405MSwap: 0B 0B 0B 增加 swap 分区，https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04 继续 make 后成功。 后面的步骤 123456sudo make installsudo ldconfigcd /usr/local/lib/python3.5/site-packages/sudo mv cv2.cpython-35m-x86_64-linux-gnu.so cv2.socd ~/.virtualenvs/cv/lib/python3.5/site-packages/ln -s /usr/local/lib/python3.5/site-packages/cv2.so cv2.so 测试是否安装成功 123workon cvpython -c 'import cv2;print('cv2.__version__')'&gt;&gt; 3.3.0 安装 stasm 1234567891011(cv) root@vultr:~/.virtualenvs/cv/lib/python3.5/site-packages# pip install stasmCollecting stasm Downloading https://files.pythonhosted.org/packages/a6/87/6a0fd3cb3c9d9ce65893e69f7e35a8822a2c819be11a802666c8469bb1bd/stasm-1.2.0.tar.gz (688kB) 100% |████████████████████████████████| 696kB 10.9MB/sRequirement already satisfied: numpy&gt;=1.7 in /root/.virtualenvs/cv/lib/python3.5/site-packages (from stasm) (1.15.4)Building wheels for collected packages: stasm Running setup.py bdist_wheel for stasm ... done Stored in directory: /root/.cache/pip/wheels/09/95/23/c4819b432d510aad1e869f492ba17260a02732cbe23964e49bSuccessfully built stasmInstalling collected packages: stasmSuccessfully installed stasm-1.2.0]]></content>
      <categories>
        <category>使用教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture7 笔记]]></title>
    <url>%2Farchives%2F68049dcc.html</url>
    <content type="text"><![CDATA[权重初始化对权重分布的影响 权重太小时，激活值消失，权重初始值过大时，会出现爆炸性增长。 卷积网络中，中心化和归一化很常见。以二元线性分类为例，如果不在归一化，损失函数对于权重矩阵的线性分类器的小扰动十分敏感，如果采用归一化，那么对参数值的扰动将会不那么敏感。如果在卷积网络中不做归一化，权重矩阵的微小变化将使输出发生巨大扰动。 Babysitting 训练过程时，观察loss和 epoch的曲线，训练集和验证集上的准确率，是否过拟合等。 超参数搜索有 Grid Search 和 Random Search 两种方法，当一个超参数比其他超参数更敏感时，随机搜索对超参数的空间覆盖更好，还可以采取由粗到细的搜索。理想的初始搜索范围应该包括最佳搜索值，而非位于搜索范围的一边。 学习率参数是最重要的，需要首先确定，其他像正则化、衰减速度、模型大小等需要来回迭代，确定最好的参数。 对于低学习率是否会卡在局部最优解上，这在直觉上是可能的，但实际上并不会出现，下面的课程将解释这个问题。 Fancier optimization regularization（解决训练集和验证集之间的鸿沟） Transfer Learning（拥有的数据比预期少时，将一个问题转换成另一个问题） SGDSGD 利用损失函数下降最快的方法更新参数 123while True: weight_grad = evaluate_gradient(loss_fun, data, weights) weights -= step_size * weights_grad 对于损失值在不同方向上的下降速度不同的情况，在一个方向上敏感，在其他方向上不敏感时，SGD 会成“之”字形运动，多次跨过等高线，在高维空间中，这种情况将导致更缓慢的下降速度； 同样，SGD 会停留在局部最小值点，极小值点的梯度为 0，参数不再更新，如果跨过极小值点，会朝梯度相反的方向运动，将再次靠近极小值点。一维问题中的鞍点一侧是凹的，另一侧是凸的，该点处的梯度为0，在深层次的神经网络中，问题往往出现在鞍点处，鞍点处意味着某些方向损失增大，某些方向损失减小，而局部极小值点处，往任何一个方向前进，损失都将变大，所以，在大型神经网络里，局部极小值点并不常见。有时，鞍点附近的梯度会变得很小，更新时前进的十分缓慢。 SGD 的另一个问题是随机性。通过随机选择的小样本对损失和梯度进行估计，意味着并不会每一步计算真实的梯度，而是在当前点对梯度进行估计，会引入一些随机噪声。如果存在噪声，SGD 将花费更多的时间才能得到极小值。 即使使用 BGD，鞍点和噪声的问题仍然存在，因为网络本身具有随机性，会带一些噪声。在优化目标函数中，鞍点仍然存在。对于合适的初始化和 step size，鞍点的影响并没有那么大。 动量法梯度只影响速度，速度再影响位置。通过动量更新，参加向量会在任何有持续梯度的方向上增加速度。 动量法，将梯度估计添加到速度中，在速度方向上进行更新。动量的系数常取 0.9 或 0.99.类似于小球滚下山，在经过局部极小值点时，仍然具有速度，就有可能越过极小值点，经过鞍点时，虽然梯度较小，但是速度向量 $v_t$ 仍然可能帮助越过这个点。如下图，在梯度敏感的方向上会减少前进的步数（振荡更小），在不那么敏感的方向上加快下降的速度。 动量法的曲线更加平稳地到达最小值点。 动量法更新的示意图，红色点是当前所在位置，红色线是梯度方向，绿色线是速度方向，进行动量更新时，取速度和梯度的混合，有助于克服梯度更新中的一些噪声。 速度一般初始化为 0。速度时间上是梯度的加权和，是梯度的指数滑动平均。 动量参数 rho 的物理意义与摩擦系数更一致，在梯度改变方向时，速度 $v_t$ 减小更新，能够有效的抑制速度，降低系统动能，使质点在山底停下。 动量参数可以像学习率退火一样，在学习过程后期逐渐上升，动量可以从0.5慢慢提升到 0.99。 Nesterov 动量 红色点开始，在速度方向上步进，评估这个位置的梯度，回到初始位置将二者混合起来。 在凸优化上有一些较好的性质，对于凸函数能得到更好的收敛。（$v_t$ 影响了当前的梯度） 换元法替换之后，可以同时计算损失函数和梯度了 包含了当前速度向量 v 和先前速度向量 old_v 的修正。 这两种方法都能够依靠构建的速度跨过局部极小值点，到达真正的极小值点。如果局部极值是一个非常极端的极值点，形式上是非常狭窄的盆地，算法上一般会跨过这些点的。出现这些点的位置往往意味着出现过拟合，扩大训练数据后，这些点会消失，而常见的极值点是平缓的，这些点对测试数据往往有更好的泛化能力。 Nesterov 动量法因为有校正因子的存在，不会那么剧烈的越过局部极小值点。 Adagrad前面讨论的方法对所有的学习率都一样，如果学习率能够随训练过程自适应变化，有更好的表现。 Adagrad，在训练过程中，存在梯度平方和的累加，更新参数时，会除以梯度平方和的开方。 对于梯度较小的维度，会加快学习速度，对于梯度较大的维度，学习速度会减慢。 随着时间进行，步长会越来越小，因为一直累加梯度的平方和，对于凸函数情况，学习率逐渐减小是好事，但是对于非凸问题，会困在局部极小值点，训练无法进行下去。 缺点是在单调的学习率通常过于激进且过早停止。 learning_rate 设置过大时，会使 regularizer 过于敏感，对梯度的调节太大。 RMSProp不是简单的累加梯度平方，而是采用滑动平均的方法时梯度平方的加和按照一定的比率下降。 类似于给梯度的平方加上动量。随着训练进行，Adagrad 步长在小梯度上会加快，在较大梯度上训练减慢，现在 RMSProp 有了在梯度平方上有衰减，可能发生训练过程一直变慢。decay_rate 是一个超参数，常用[0.9, 0.99, 0.999]。 在神经网络训练时，倾向于不使用 Adagrad，一般它需要较大的学习率初始值防止在后面更新时学习率太小而卡住。也就是说，每种更新算法都有适应自己的不同的学习率。 Adam结合了动量法和 RMSProp 两种参数更新方式，像是 RMSProp 的动量版。 由于第一动量和第二动量初始化为0，因为 $\beta$ 常取 0.9 或 0.99，和指数移动平均一样，在开始的几步时，值会非常小，步长会非常之大。所以需要做偏移校正。$\hat{V_t} = V_t/( 1 - \beta ^t)$，当 t 比较大时，$\hat{V_t}$近似于 $V_t$。 可能初始化时的值并不合理，然后又因为步长太大，到了另一个区域，导致最终无法收敛。 模型开始时，Adam 是首选，一般都是 beta1 = 0.9，beta2=0.999，lr=1e-3 或 5e-4。 像二维中非椭圆形状的损失函数等高线是 Adam 解决不了的。 上图可以看出SGD很难突破对称性，一直卡在顶部。RMSProp等方法能够看到马鞍方向有很低的梯度，这是因为更新公式中分母项是所有梯度的滑动平均，能够提高在马鞍方向的学习率，使 RMSProp 继续前进。 学习率退火 学习率太高时，系统动能过大，参数向量会无规律跳动，不能稳定到损失函数更低的地方或者very high 时，loss 会发散。 知道在什么时候开始衰减学习率是有技巧的，慢慢衰减，可能在很长时间内浪费计算资源，衰减得过快，系统可能过早的失去能量。 学习率衰减有几种方法，随步数衰减、指数衰减$\alpha= \alpha_0e^{-kt}$ 和 1/t 衰减 $\alpha=\alpha_0/(1+kt)$。$\alpha_0, k$ 是超参数，t 是迭代次数。 降低学习率的解释是损失函数已经到了一个不错的区域，此时梯度很小了，保持原有的学习率只能在最优点附近来回徘徊。如果降低学习率目标函数可以进一步降低。 学习率衰减在 SGD 中很常见，Adam 的优化算法很少用。学习率衰减是一种二阶的超参数，不应该一开始就使用，开始时应该选择一个不带衰减的不错的学习率，然后观察loss曲线，看看希望在哪个地方开始衰减。在loss停止下降时，可以乘以一个常数来降低学习率。 在实践中，随步数衰减的随机失活（dropout）更受欢迎。 一阶逼近、二阶逼近和多阶逼近SGD 等这些常用的下降方法用的是在当前点的梯度信息计算这个函数的线性逼近，相当于做函数在这个点处的一阶泰勒逼近，在逼近函数上前进一小步，找到逼近的最小值。这个逼近在较大的区间上并不成立，所以不能走太多。 当然，可以考虑二阶泰勒逼近的情况，因为是二阶函数，所以可以直接跳到最小值点，这二个红点处，这就是二阶优化的思想。 二阶优化的更新公式（牛顿法）变成 \theta^{*} = \theta_{0} - H ^{-1}\Delta_{\theta}J(\theta_{0})$H^{-1}$ 是二阶的海森矩阵的逆，海森矩阵描述了损失函数的局部曲率（弯曲程度），可以是最优化在曲率小的时候大步前进，在曲率大的时候小步前进。用二次逼近到最小值的地方。这里没有学习率的参数！ 因为用了二次逼近，直接走到最小值位置，实际上，是需要学习率朝最小值方向前进的。 但是这个方法对深度网络不适用。海森矩阵的维度 $N\times N$。N 是网络中的参数数量。占用内存太大，求逆也不可行。因为有了拟牛顿法的出现，逼近这个矩阵的逆，比如说低阶逼近。L-BFGS 是一个二阶逼近。但是这些二阶逼近方法对随机情况和非凸问题处理得不是很好，而且 L-BFGS 需要对整个训练集进行优化，和 mini batch SGD 不同。 实践中，Adam 已经是很好的选择了，因为简单而且容易扩展。L-BFGS 的方法并不常见。如果能接受整个批次更新，可以尝试一下 L-BFGS。L-BFGS 在风格迁移这种参数很少的情况会使用。 如何减少训练误差和测试误差的 Gap？模型集成，在机器学习上用的很多，选择不同的随机初始值训练 10 个不同的模型，测试时运行10个模型，平均10个模型的预测结果，可以缓解一点过拟合，但提升很固定，提升几个百分点。Bagging 的思路。在比赛中常见。 另外，可以在训练过程中保留多个模型的快照，用这些模型做集成学习，测试数据时用这些模型的快照的预测结果做平均。疯狂的学习率快慢变换。 最终只会关注在验证集上的预测结果。 其他的 Tips训练模型时，对不同时刻的每个模型参数求指数移动平均值，得到网络训练中比较平滑的集成模型。使用滑动平均后的模型参数在做预测，称为 Polyak averaging。这个也并不是很常见。 正则化提高单一模型的预测效果。 防止训练集上的过拟合。 L2 正则化在神经网络中的意义可能并不是很明确。 L1, L2 与 最大范式约束L2 正则化 $\frac{1}{2}\lambda w^2$，对于大数值的权重向量惩罚严重，权重向量大多是分散的小数字； L1正则化 $\lambda|w|$，让权重向量在更新时变得稀疏，如果不是特别关注某些明确的特征选择，一般来说，L2 正则化比 L1 正则化效果好。 最大范式约束，给每个神经元权重向量设定上限，$|||\vec{w}||_2 &lt; c$。一般 c 为 3 或 4，学习率设置过高时，网络中的数值也不会出现爆炸，因为参数更新始终被限制着。 有时可能还会采用偏置正则化，但并不常见。 Dropout在正向传递时，随机将一些神经元置零，在激活函数之后。 将激活函数的结果按一定概率置零，一般在全连接层使用 DropOut，卷积层中也会使用，随机将整个特征映射置零。 一个解释是 dropout 避免了特征间的相互适应。比如在判断是不是猫时，网络不再依赖所有特征组合在一起，而是依靠一些零散的特征来判断。某种程度上抑制了过拟合。 另一种解释是 dropout 像是对一群共享参数的网络进行集成学习。每一种dropout 的方式都产生一个不同的子网络。dropout 的可能性呈指数增长。 测试的时候，输入不仅有 x，还有 z，表示 dropout 中被置零的项，z 的值是随机性的。但引入随机性在测试时又不稳定，在实践中，通过局部逼近的方式来求解这个积分。 y = f(x) = E_z[f(x, z)] = \int p(x)f(x, z)dz对于神经元 x 和 y，参数分别是 w1 和 w2，输出是 a，训练时的期望是 E[a] = \frac{1}{4}(w_1x + w_2y)+\frac{1}{4}(w_1x + 0y)+\frac{1}{4}(0x + w_2y)+\frac{1}{4}(0x + 0y) = \frac{1}{2}(w_1x + w_2y)测试时我们也就常常用 Dropout 的概率乘以输出，即 E[a] = \frac{1}{2}(w_1x+w_2y) 总结来说：训练过程中，正向传播中只需要将部分节点置零，相当于对神经网络抽取一些子集，每次基于输入数据只更新子网络的参数；测试时不进行随机失活，仅仅增加了一个乘积，做了一个平均预测。如果想要测试过程尽可能地高效，可以将这个乘积放在训练中做除法。 反向随机失活，在训练时除以 p，是为了保证 x 的数学期望在 Dropout 前后相等。 更通用的说，在训练过程中给网络增加一些随机性，防止它过拟合训练数据，一定程度扰乱它，防止完美的拟合训练数据，在测试时，则希望能够抵消掉这些随机性。Dropout 是常见的。 Batch Normalization 也是如此，对于单个数据点，在进行 BN 时，取决于随机选择的小批量样本，具有一定的随机性，在测试时则用全局估计的正则化来抵消这个随机性，而不是用小批量样本。 有时候仅仅用 BN，不用 dropout 就可以给网络增加足够的正则化效果。dropout 可以通过调节概率 p 调整正则化的力度，而 bn 则没有这种机制。 数据增强另一种符合上述通用思想的是数据增强的策略。训练时，以某种方式随机的旋转图像，以旋转后的图像训练，但是标签不变，比如水平旋转，裁剪图像等。 测试时，通过评估一些固定的裁剪图像来抵消这种随机性。通常是4个角落，中间和翻转，这是 5 种标准裁剪。 有时会用色彩抖动，更改对比度和亮度来得到一些更复杂的结果。 数据增强很常见，在不改变标签时，将随机转换应用于训练数据，增加某种随机性，测试时淡化这种随机性。 Dropconnect并不是将激活函数后的值置零，而是随机将权重矩阵中的一些值置零。 部分最大化池化池化时，随机池化正在池化的区域，下图表示3种随机池化的结果。 在测试时，要么使用固定的池化区域或选取很多样本取平均。 随机深度一个很深的网络，训练时随机的从网络中丢弃部分层。 测试时用全部的网络。实际操作中并不常用。 大多数情况下，单独使用 BN 就够了，有时当发现网络过拟合时，再考虑增加 dropout 这些方法，一般不要盲目的相信交叉验证这些方法，在网络出现过拟合时，再把他们加进去。 实践中一般用 L2 正则化，同时在所有层后面使用 Dropout，p值一般默认设为 0.5，也可以在验证集上调参。 超参数调优一般的超参数设置有：初始学习率，学习率衰减方式（例如一个衰减常量），正则化强度。还有一些其他技巧： 使用子程序记录训练过程，比如准确率，损失值变化等。 最好只用一个验证集，不需要几个数据集来做交叉验证。 学习率和正则化强度常常采用对数标尺搜索方式，比如 learning_rate = 10** uniform(-6, 1)，而Dropout等一些参数则在原始尺度上搜索，比如 dropout = uniform(0,1)。 1234max_count = 100for i in range(max_count): reg = 10 ** np.random.uniform(-5, 5) lr = 10 ** np.random.uniform(-3, -6) Random Search &amp; Grid search 常常采用 Random Search，这样可以会对于一些重要的参数进行更多次的采样，而网格采样则会减少重要参数的采样。 注意边界上的最优值确认我们得到的最优值（比如学习率）不是在搜索范围的边界处，以防错过其他更好的搜索范围。 由粗到细的搜索范围比如学习率范围，粗搜索时，设为 $10^{[-6, -1]}$，训练一个 epoch 就可以了，第二个阶段用一个更小的范围，比如$10^{[-4, -3]}$，可以运行 5 个周期，最后一个阶段在更细的范围内运行多个周期。 模型集成提高网络几个百分点预测准确率的方法。多训练几个模型，预测的时候平均他们的预测结果。集成的模型数量增加，算法也单调提升，但是提升的效果越来越少，模型间的差异越大，提升效果越好。 同一模型的不同初始化。先用交叉验证得到最好的参数，用最好的参数训练不同的初始化模型。缺点是多样性只来自于不同的初始化条件。 在交叉验证中得到最好的模型。取交叉验证过程中最好的几个模型来集成，缺点是可能包含不够理想的模型 一个模型设置多个记录点。在不同的训练时间下（比如每个周期结束）对网络留下记录点，用这些记录点进行模型集成。多样性不足，但是实践中的效果不错，而且代价比较小。 取训练过程中参数的平均值。在训练过程中，如果损失值较前一次出现指数下降时，对网络的权重进行备份，对备份的网络状态进行平均，这样“平滑”过的权重总能得到更少的误差。 迁移学习防止过拟合可以减小训练准确率和验证集准确率之间的误差，有时，是因为在一个大的模型中使用小数据很容易过拟合。实现了不需要大的样本集也能训练卷积网络。 对于和原始模型训练集中相似的数据集的处理方法。 无论是目标检测还是图像加标的论文，所有的都有一个卷积网络处理图像的模块，但大多数人都不会从头训练这些，大多数卷积神经网络都在 ImageNet 上训练，然后根据任务精调。对于图像加标，可以在文本词典上预先训练一些词向量，结合ImageNet上训练的卷积网络进行精调。 如果没有大的数据集时，首先应该做的是下载一些相关的预训练模型，然后要么重新初始化部分模型或精调模型。 总结训练一个神经网络需要： 利用小批量数据对实现进行梯度检查，还要注意各种错误。 进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。 在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。 推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。 随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。 使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。 进行模型集成来获得额外的性能提高]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture6 笔记]]></title>
    <url>%2Farchives%2Fa4ae9d52.html</url>
    <content type="text"><![CDATA[已经讲过的三大块重点，用计算图表示任何一个函数、两层神经网络和卷积神经网络。 卷积神经网络能够保持空间的输入结构。每个卷积核作为滤波器在空间上滑动，多个滤波器构成特定深度的输出。 需要的是学习这些所有的权重和系数。用 Mini-batch SGD 的方法。正向传播，loss，反向传播，梯度更新。 这节课讲的细节： 如何建立起神经网络 选择激活函数， 数据预处理，权重初始化，正则化，梯度检查 训练中的动态变化，如何监控（Babysitting），如何选择参数的特定更新规则，怎样做超参数优化 模型评估和模型集成 激活函数激活函数至关重要，如果去掉，那么两个线性层会合二为一，重新变成一个线性函数。 sigmoid每个元素都在 [0, 1] 范围内，中间区域可以看作线性函数。可以看作神经元的饱和放电率。 若干问题： 饱和神经元将使梯度消失。当输入值过大或过小（接近0或1）时会饱和，梯度接近 0，局部梯度非常小，乘以返回的上游梯度将得到一个很小的梯度，零梯度将阻止梯度传递。在权重初始化时需要特别留意，如果权重初始化过大，大多数神经元将饱和。 输出是非零中心的函数，假设对于 $wx + b$，若输入 x 一直为正，上游传回的梯度为正数或负数，对 w 求导后局部梯度为 x，乘以局部梯度后，w 的梯度将一直得到全是正数或负数，相当于沿上游梯度的正负号进行传递，总是沿同一个方向移动。 对于梯度更新来说非常低效，以下图为例，第一象限和第三象限是全为正或全为负，蓝色箭头代表 w 的梯度方向，我们不能沿 w 的方向更新参数，而是只能沿红色箭头中表示的两个方向去更新梯度。出现z字型下降 如果使用均值为 0 的数据，将得到正和负的值，就不会陷入上述梯度更新的问题。 指数函数的计算代价有些高，但不需要太注意这点。 tanh tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\\ =\frac{e^{2x}-1}{e^{2x}+1}\\ = \frac{2}{e^{2x}+1} - \frac{e^{2x}+1}{e^{2x}+1}\\ =2sigmoid(2x) -1和 sigmoid 很类似，但是是以 0 为中心的，不会有上述第二个问题，但仍会有梯度消失的问题。 可以看成是一个简单放大的 sigmoid 神经元， Relu$f(x) = max(0, x)$ 在元素上进行操作。 不会在正数区域产生饱和现象，输入空间的一半都不会有饱和 计算速度快 收敛速度快，比 sigmoid 和 tanh 快约 6 倍 不是以 0 为中心，负半轴上梯度会消失。在 x=0 处导数不存在。 负半轴上称为 dead ReLU，不会被激活和更新，可能的原因是权重初始化非常差；学习率太高，权值不断波动，被数据的多样性淘汰，通常在训练开始时较好，某个时间点后挂掉；大多数使用 ReLU 的网络会有部分的节点挂掉，通常还会再去检查。 通过合理的设置学习率，可以是这种情况的概率降低。 其他 ReLULeaky ReLU：$f(x) = max(0.01x, x)$负轴上有一个微小的斜率，仍然收敛快，而且不会出现梯度消失。 PReLU：$f(x) = max(\alpha x, x)$，倾斜区间斜率是一个超参数$\alpha$，可以进行学习。输出是以 0 为中心的。 ELU，介于 ReLU 和 Leaky ReLU 之间，输出接近零值中心，但是在负区间没有倾斜，具有负饱和机制，对噪声有更强的鲁棒性。 \begin{equation} f(x)= \begin{cases} x& \text{if x > 0}\\ \alpha(exp(x) - 1)&\text{if x]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture5 笔记]]></title>
    <url>%2Farchives%2F2a219ab1.html</url>
    <content type="text"><![CDATA[卷积神经网络简介需要训练卷积层，因为卷积层更能保留输入的空间结构。 诞生 感知机算法，判断 $Wx + b &gt; 0?$ 1986 年，首次提出反向传播，链式法则，更新规则等。 2000 年左右，提出深度神经网络可以很好的被训练 2012 年，神经网络第一次取得惊人的成果，语音识别和图像识别逐个被攻破，ImageNet 图像分类大赛卷积网络的应用。 卷积网络1959 年，猫的神经元在视觉刺激下的应激响应。视觉相关的区域经过空间映射可以映射到其他区域。简单细胞会对有向边缘和光的刺激作出响应。 1980 年，神经认知机，简单细胞和复杂细胞的交互结构 1998 年，基于反向传播和梯度方法训练神经网络，对文本识别十分有效，用于邮政编码识别上。 2012 年，AlexNet 提出的网络可以利用更大量的数据。充分发挥 GPU 的并行计算的能力。 ConveNets，图像分割和识别。人脸识别，视频分类，姿势识别，图像描述等 完成艺术作品。 如何工作全连接层进行的是一些向量化的操作，比如对于 $32\times 32\times 3$ 图像，将其展开到 3072 维的向量。然后将权重与向量相乘，得到神经元的输出。 卷积层可以保存图像的空间结构，比如三维输入结构。 卷积核虽然很小，但通常会遍历所有的通道，然后将卷积核与对应的图像位置做点积运算，比如，$5\times5\times 3$ 的卷积核需要做 $5\times 5\times3 = 75$ 次加法运算，每个卷积核与图像相应位置做运算后将得到一个数值，也可以理解成拉伸成一个向量。所以，$32\times32\times3$ 经过一个 $5\times 5\times 3$ 卷积核卷积运算后将得到 $28\times 28\times 1$ 的输出。 换句话说，卷积核的深度应时刻与输入的深度相同。 Sliding最简单的是每个像素逐个滑动，在激活映射中得到对应的值。也可以一次滑动两个像素点。当使用更大的步长时，相当于对图像进行了降采样处理，有点类似于池化，但又比池化在某些时候要好一些。 多种卷积核通常会有一组卷积核，每种卷积核都会得到一种特殊的模式或概念。这里有 6 个卷积核，每个卷积核最终形成 $28\times28\times1$ 的结果，最终 6 个卷积核形成 $28\times 28\times 6$ 的激活映射。卷积核个数 K 通常使用 2 的次幂。 像神经元的实验一样，卷积核从最简单的特征逐渐集成到更复杂的特征上。网格中的每个元素都展示了输入的样子，基本上最大化神经元的激活函数，从某种意义上是神经元在寻找什么。 padding 填充不用 0 填充一个 $7\times 7$ 的图像经过 $3\times 3$ 的卷积核，一个像素的窗口滑动卷积计算后，得到 $5\times 5$ 的图像。 当步幅为 2 时，得到 $3\times3$ 的图像，步幅为 3 时，会得到不对称的输出，不能采用。 Output Size： (N - F) / stride + 1 N 是输入维度，F 为卷积核的大小，得到最终的输出维度。 用 0 填充当用 0 填充边缘时，stride = 1，填充一圈的边缘时，$7 \times 7$ 的输入，$3 \times 3$ 的卷积核最终将得到和输入维度相同 $7 \times 7$ 的结果。(9 - 3) / 1 + 1 = 7。 Output Size： (N - F + 2P) / stride + 1 N 是输入维度，F 为卷积核的大小，P 是零填充的数量。 零填充的目的得到和之前图像大小相同的卷积结果，保持全尺寸输出。zero pad 的大小等于 $(F-1)/2​$ 卷积核 zero pad $3\times 3$ 1 $5\times 5$ 2 $7\times 7$ 3 如果不做零填充，每次图像都会变小，关于边角的信息就会丢失很多。 卷积核的权值我们并不关注所输入图片的全部，而且关注图像空间的一个区域。关注神经元在某个区域的激活程度。每个神经元只观察输入数据中的一小部分。 如果有一个 $5\times 5$ 的卷积核，我们可以称为这个神经元的一个 $5\times 5$ 的感受野。表示这个神经元所能感受到的范围。它的尺寸也就是滤波器的空间尺寸是一个超参数。 不同的卷积核作用于图像的相同区域，却有不同的作用。不同于全连接层，将一个神经元展开，与全体的输入都发生关联。 参数共享在某一层中，卷积核都是维度（权重）是相同的，做相同的运算。感受野在深度上是和输入数据是一致的。 每个神经元只关注一个特性，像是图像处理的滤波器，每个滤波器都自己关注一个图像特征，比如垂直边缘、水平边缘、纹理、颜色等，所有的神经元加起来像是整张图像的特征提取器集合。 参数数量$32\times 32\times 3$ 的图像经过 10 次 $5\times5\times3$ 的卷积，stride = 1，pad = 2，最终得到的结果是 $32\times 32\times 10$。共有 $10\times 5\times 5\times 3 + 1\times 10= 750 + 10 = 760 $个参数，每个卷积核的偏差参数 b 怎么起作用的？每个卷积核都带有一个偏差，卷积核与对应元素做完点乘后，再加上这个偏差，类似于线性回归中，$WX$ 乘完后再加上 $b$。 $1\times 1$ 的卷积如果是二维数据，比如信号处理，$1\times 1$ 的卷积没有意义，而在图像中，卷积核和输入数据的深度是一样的，$1\times 1$ 的卷积就是高效地进行 3 维点积。 池化层池化使生成的表示更小更容易控制。我们一般只做平面上的池化处理，输入和输出的深度是一样的。 图中表示 $2\times 2$ 的卷积核，步长为 2 的 Pooling 结果。max pooling 只提取最大值，用最大值代表整个区域，对于池化层来说，设定步长是他们避免重叠。 最大值池化用的更多，某个位置代表了某组卷积核的激发程度，最大值池化可以表示这组卷积核在图像任意位置的最大受激程度。最大值则表示得更加直观，将最重要的特征抽取出来，对没有用的、重复的冗余信息去除掉。 Pooling 公式 输入是 W_1 \times H_1 \times D_1\\ 卷积核尺寸为 F，步长为 S。 池化后的大小\\ W_2 = (W_1 - F)/S + 1\\ H_2 = (H_1 - F)/S + 1\\ D_2 = D_1一般不在池化层填 0，直接降采样。$2\times 2$ 的池化常用。 每个池化层输出的值实际上经过整个网络处理后的累积结果，最终表现为一组复合模版的激活情况。 反向传播参考 https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199 全连接层全连接层中，在得到长宽高的矩阵后，直接拉平，输入到朴素神经网络中。这是不再需要保全空间结构了。 实际应用中，针对不同的问题选择池化的大小，卷积核的大小，层的数量，stirde 长度，选择最优的超参数。 第一层的激活映射是直接在图片上操作的，模版的含义比较好理解。后面的层就不太好解释了。某些具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至网络更高层上的蜂巢状或车轮状图案。 总结小尺寸的卷积核和更深的网络结构是一个趋势。同时还有完全弃用池化和全连接层也是趋势，而保留卷积层，形成较深的卷积网络。ResNet 和 GoogLeNet 则用了不同的结构。]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[学习率与 loss function]]></title>
    <url>%2Farchives%2F965e28a.html</url>
    <content type="text"><![CDATA[在模型进行训练之前，通常需要合理性的检查，可以从以下三个方面入手： 检查某种特定情况下的 loss，例如在初始化参数后，已经知道某种期望条件下的 loss，比如，在 CIFAR-10 数据集中，用 Softmax 分类时，随机初始化参数后，每个分类的概率是 0.1，所以 loss 的期望是 $-log(0.1) = 2.302$，此时，应该将正则项系数设为 0。 提高正则项强度后，loss 应该增加。 对于较小的数据集（通常在 20 个 examples）会出现过拟合，也就是会得到 zero cost，在训练集上的正确率接近 100%。 学习过程中 Learning Rate 与 Loss 的关系 横坐标轴是 epoch，一个 epoch 指所有的 example 都被用过一次，横坐标应该用 epoch 而非 iterations，因为 iterations大小是由 batchsize 决定的。 当用较小的学习率时，loss 与 学习率呈线性下降的关系；当较高的学习率时，loss 呈指数性下降，但是并不能得到较好的 loss，会在 loss 底部左右来回弹跳，得不到最低处的值。当学习率很大时，loss 则会发散。 如何估算学习率可以参考这篇文章。https://www.jiqizhixin.com/articles/2017-11-17-2 CIFAR-10 数据集的小型神经网络的 loss 变化 loss 的振荡和 batch_size 有关，当 batch_size = 1时，振荡的会比较厉害；当 batch_size 是整个数据集时，振荡会变小，因为每次梯度更新后，loss 都会下降。 在画 loss 时，常常对 loss 取 log，loss 通常呈指数下降，在图上的表现为线性下降。 参考： http://cs231n.github.io/neural-networks-3/#sgd]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自适应学习率算法]]></title>
    <url>%2Farchives%2F44c99399.html</url>
    <content type="text"><![CDATA[神经网络中，学习率通常是难以设置的超参数，我们认为损失通常高度敏感于参数空间中的某些方向，动量法可能会稍稍缓解这种情况，但会引入新的超参数。假设这种敏感度在某种程度是轴对齐的，那么，对于每个参数设置不同的学习率，在整个学习过程中自适应这种学习率是可行的。 比如，在上图中，b 方向上的梯度要大于 w 方向的梯度，是否可以在学习过程中可以使 b 方向幅度较小的进行下降，使 w 方向幅度较大的进行下降呢？最普通的 mini-batch 像蓝色线一样上下波动很大，类似折线，是否可以使下降方向如绿色线一样，在陡峭的地方平缓一些，从而加快训练速度。 AdagradAdagrad 的算法过程 设置一个初始值 $r=0$，使每个参数梯度的平方累加到 $r$ 上，使更新这个参数的学习率为 $\frac{\epsilon}{\delta + \sqrt{r}}$（代码中参数的学习率为稍有不同），$\delta$ 为防止分母等于 0 出现无穷大的情况，这样使得每次参数都有自己的学习率。 每次更新时，如果参数的梯度 $g$ 比较大，$r$ 也会很大，$r$ 在分母上，相应的学习率会很小，相反，如果参数的梯度较小，相应的学习率会较大，实现了我们开始提到的想法。 但是这样也会有些问题，因为不断累积梯度的平方，平方导致数值越来越大，会导致有效学习率过早或过量的减小，导致后期收敛乏力，而无法得到较好的效果。 Adagrad 代码1234567891011# 初始化梯度平方项vs = []for param in net.parameters(): vs.append(torch.zeros_like(param.data)) def adagrad(parameters, vs, lr): eps = 1e-10 for p, v in zip(parameters, vs): v[:] = v + p.grad.data * p.grad.data div = lr / torch.sqrt(eps + v) * p.grad.data p.data = p.data - div RMSPropRMSProp 梯度下降由于 Adagrad 是梯度平方的积累，可能使得学习率在到达凸结构之前就变得很小。RMSProp 梯度积累采用指数加权的移动平均，对于离当前远的梯度给予很小的权重。 初始化 $r_i=0$ 。 累积平方梯度：r_i = \alpha r_{i-1} + (1-\alpha) g ^2\\ 计算参数更新：\theta := \theta - \frac{\epsilon}{\sqrt{r_i + \delta}}gRMSProp 代码1234567891011# 初始化梯度平方项sqrs = []for param in net.parameters(): sqrs.append(torch.zeros_like(param.data)) def rmsprop(parameters, sqrs, lr, alpha): eps = 1e-10 for p, sqr in zip(parameters, sqrs): sqr[:] = alpha * sqr + (1 - alpha) * p.grad.data * p.grad.data div = lr / torch.sqrt(eps + sqr) * p.grad.data p.data = p.data - div AdadeltaAdadelta 梯度下降Adadelta 并不需要学习率 $\epsilon$ 这个参数。初始化 $r_i=0$ 。 累积平方梯度：r_i = \alpha r_{i-1} + (1-\alpha) g ^2\\ 更新参数梯度 g：g' := \frac{\sqrt{\Delta\theta + \delta}}{\sqrt{r_i + \delta}}g\\ 更新 \Delta\theta：\Delta\theta = \alpha\Delta\theta + (1-\alpha) g' ^2\\ 更新 \theta：\theta = \theta - g'\\Adadelta 代码12345678910111213# 初始化梯度平方项和 thetasqrs, thetas = [], []for param in net.parameters(): sqrs.append(torch.zeros_like(param.data)) thetas.append(torch.zeros_like(param.data)) def adadelta(parameters, sqrs, thetas, alpha): eps = 1e-10 for p, sqr, t in zip(parameters, sqrs, thetas): sqr[:] = alpha * sqr + (1-alpha) * p.grad.data ** 2 g = torch.sqrt(t + eps) / torch.sqrt(sqr + eps) * p.grad.data t[:] = alpha * t + (1 - alpha) * g ** 2 p.data -= g AdamAdam 梯度下降Adam 方法结合了动量法（对梯度的指数加权）和 RMSProp 方法（对梯度平方的指数加权）。同时也包括了偏移校正，减轻了开始时参数初始化为 0 产生的计算指数加权移动平均的影响。 初始化一阶梯度项 $s_i = 0$ 和二阶梯度项 $r_i=0$。一般来说，$\alpha_1 = 0.9$，$\alpha_2=0.999$，$\delta$ 保证数值稳定性添加的常数。 更新一阶梯度项：s_i = \alpha_1 s_{i-1} + (1-\alpha_1) g\\ 更新二阶梯度项：r_i = \alpha_2 r_{i-1} + (1-\alpha_2) g ^2\\ 偏移校正：\hat{s_i} = \frac{s_i}{1-\alpha_1^t}；\hat{r_i} = \frac{r_i}{1-\alpha_2^t}\\ 参数更新：\theta := \theta - \epsilon\frac{\hat{r_i}}{\sqrt{\hat{s_i} + \delta}}Adam 代码12345678910111213vs, sqrs = [], []for param in net.parameters(): vs.append(torch.zeros_like(param.data)) sqrs.append(torch.zeros_like(param.data)) def adam(parameters, vs, sqrs, lr, t, alpha_1=0.9, alpha_2=0.999): eps = 1e-10 for p, v, sqr in zip(parameters, vs, sqrs): v[:] = alpha_1 * v + (1 - alpha_1) * p.grad.data sqr[:] = alpha_2 * sqr + (1 - alpha_2) * p.grad.data ** 2 v_hat = v / (1 - alpha_1 ** t) sqr_hat = sqr / (1 - alpha_2 ** t) p.data = p.data - lr * v_hat / torch.sqrt(sqr_hat + eps)]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>梯度下降</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用 Counter 进行计数统计]]></title>
    <url>%2Farchives%2Fb1fd797f.html</url>
    <content type="text"><![CDATA[计数统计简单说就是统计某一项的出现次数。比如，检测样本中某个值的出现次数，日志分析时某一消息的出现频率，分析文件中相同字符串的出现概率等。下面是用不同的数据结构来实现的。 计数统计的实现dict123456789101112131415In [1]: import randomIn [3]: data = [random.choice('abcdefghi') for _ in range(20)]In [32]: counter_freq = dict()In [33]: for i in data: ...: if i in counter_freq: ...: counter_freq[i] += 1 ...: else: ...: counter_freq[i] = 1 ...:In [34]: counter_freqOut[34]: &#123;'h': 6, 'a': 2, 'c': 3, 'g': 3, 'f': 1, 'b': 1, 'i': 1, 'e': 1, 'd': 2&#125; defaultdict1234567891011121314151617181920In [35]: from collections import defaultdictIn [36]: counter_freq = defaultdict(int)In [37]: for i in data: ...: counter_freq[i] += 1 ...:In [38]: counter_freqOut[38]:defaultdict(int, &#123;'h': 6, 'a': 2, 'c': 3, 'g': 3, 'f': 1, 'b': 1, 'i': 1, 'e': 1, 'd': 2&#125;) set &amp; list12345678910111213141516171819In [39]: count_set = set(data)In [40]: count_list = []In [41]: for i in count_set: ...: count_list.append((i, data.count(i))) ...:In [42]: count_listOut[42]:[('a', 2), ('i', 1), ('e', 1), ('c', 3), ('f', 1), ('h', 6), ('g', 3), ('d', 2), ('b', 1)] 更优雅的 Counter12345678910111213In [44]: c = Counter(data)In [45]: cOut[45]:Counter(&#123;'h': 6, 'a': 2, 'c': 3, 'g': 3, 'f': 1, 'b': 1, 'i': 1, 'e': 1, 'd': 2&#125;) Counter 的操作Counter 属于字典类的子类，是一个容器对象，主要用来统计散列对象。 初始化Counter 支持三种初始化操作 12345678In [46]: Counter('success')Out[46]: Counter(&#123;'s': 3, 'u': 1, 'c': 2, 'e': 1&#125;)In [47]: Counter(s=3, c=1, u=1, e=1)Out[47]: Counter(&#123;'s': 3, 'c': 1, 'u': 1, 'e': 1&#125;)In [48]: Counter(&#123;'s':3, 'c':2, 'u':1, 'e':1&#125;)Out[48]: Counter(&#123;'s': 3, 'c': 2, 'u': 1, 'e': 1&#125;) .elements()Counter.elements() 返回 key 值，返回一个迭代器，key 的出现次数等于 value 值 12345678910111213141516171819202122In [50]: list(Counter(data).elements())Out[50]:['h', 'h', 'h', 'h', 'h', 'h', 'a', 'a', 'c', 'c', 'c', 'g', 'g', 'g', 'f', 'b', 'i', 'e', 'd', 'd'] 访问不存在元素时当访问不存在元素时，返回 0，而非引起 KeyError 异常 12In [59]: Counter(data)['z']Out[59]: 0 most_common()返回前 N 个出现频率最高的元素及出现次数 12In [51]: Counter(data).most_common(3)Out[51]: [('h', 6), ('c', 3), ('g', 3)] +/-/&amp;/|/update/subtract&amp; 返回各元素对应的最小值，类似去交集操作。 1234567891011121314151617In [55]: Counter(data2)Out[55]: Counter(&#123;'l': 6, 'f': 4, 'm': 1, 'c': 3, 'a': 1, 'b': 2, 'd': 1, 'e': 2&#125;)In [56]: Counter(data)Out[56]:Counter(&#123;'h': 6, 'a': 2, 'c': 3, 'g': 3, 'f': 1, 'b': 1, 'i': 1, 'e': 1, 'd': 2&#125;)In [57]: Counter(data2) &amp; Counter(data)Out[57]: Counter(&#123;'f': 1, 'c': 3, 'a': 1, 'b': 1, 'd': 1, 'e': 1&#125;) | 返回各元素对应的最大值，类似取并集操作。 12345678910111213In [58]: Counter(data2) | Counter(data)Out[58]:Counter(&#123;'l': 6, 'f': 4, 'm': 1, 'c': 3, 'a': 2, 'b': 2, 'd': 2, 'e': 2, 'h': 6, 'g': 3, 'i': 1&#125;) + 将对应元素相加 12345678910111213In [60]: Counter(data) + Counter(data2)Out[60]:Counter(&#123;&apos;h&apos;: 6, &apos;a&apos;: 3, &apos;c&apos;: 6, &apos;g&apos;: 3, &apos;f&apos;: 5, &apos;b&apos;: 3, &apos;i&apos;: 1, &apos;e&apos;: 3, &apos;d&apos;: 3, &apos;l&apos;: 6, &apos;m&apos;: 1&#125;) - 将对应元素相减，计数小于等于 0 的元素将不存在 12In [61]: Counter(data) - Counter(data2)Out[61]: Counter(&#123;'h': 6, 'a': 1, 'g': 3, 'i': 1, 'd': 1&#125;) update() 同样对被统计对象做相加操作，但是是 in-place。 1234567891011121314151617181920212223242526272829In [64]: c = Counter(data)In [65]: cOut[65]:Counter(&#123;'h': 6, 'a': 2, 'c': 3, 'g': 3, 'f': 1, 'b': 1, 'i': 1, 'e': 1, 'd': 2&#125;)In [66]: c.update(Counter(data2))In [67]: cOut[67]:Counter(&#123;'h': 6, 'a': 3, 'c': 6, 'g': 3, 'f': 5, 'b': 3, 'i': 1, 'e': 3, 'd': 3, 'l': 6, 'm': 1&#125;) subtract() 对计数器对象中的元素做相减操作，如果相减后 value 值小于等于 0，仍会保留这个 key，是 in-place 操作。 123456789101112131415In [69]: c.subtract(Counter('lmaagggg'))In [70]: cOut[70]:Counter(&#123;'h': 6, 'a': 1, 'c': 6, 'g': -1, 'f': 5, 'b': 3, 'i': 1, 'e': 3, 'd': 3, 'l': 5, 'm': 0&#125;) LeetCode 中的题目LeetCode 中的 350 题，389 题，409 题，438 题，532题，697 题都可以用 Counter 很优雅的解决。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动量法梯度下降]]></title>
    <url>%2Farchives%2Fd9872287.html</url>
    <content type="text"><![CDATA[动量梯度下降法是一种比随机梯度下降算法更快的优化算法。 指数移动平均首先理解一下指数移动平均，图表示气温变化趋势，$\theta_t$ 代表 $t$ 时刻的实际温度，$v_t$ 代表指数移动平均曲线中预测的 $t$ 时刻的温度，$t$ 时刻的温度等于 v_t = \beta v_{t-1} + (1-\beta) \theta_t 假设 $\beta=0.9$ ，$V_0=0$ v_{100} = 0.1 \theta_{100} + 0.9 v_{99}\\ v_{99} = 0.1 \theta_{99} + 0.9 v_{98}\\ v_{98} = 0.1 \theta_{98} + 0.9 v_{97}\\ ... v_{100} = 0.1 \theta_{100} + 0.9 v_{99}\\ =0.1 \theta_{100} + 0.9(0.1 \theta_{99} + 0.9 v_{98})\\ =0.1 \theta_{100} + 0.1 \times 0.9\theta_{99} + 0.1\times \\0.9^2\theta_{98} + ... + 0.1\times0.9^{10}\theta_{90}\\ ... 0.1\times 0.9^{10} = (1-\beta)\times\beta^{\frac{1}{1-\beta}}\lt\beta^{\frac{1}{1-\beta}}\\ 令 {\frac{1}{1-\beta}} = N\\ \beta^{\frac{1}{1-\beta}} = (1 - \frac{1}{N})^{N} \approx \frac{1}{e}指数系数项是不断衰减的，当衰减到小于 $\frac{1}{e}$ 时，就可以忽略不计了。加权天数的计算恰好在 $N$ 处停止了，所以指数加权平均的天数近似表示为 $\frac{1}{1-\beta}$。 当 $\beta=0.98$ 时，$\frac{1}{1-\beta} = 50$，表示将前 50 天进行加权平均。$\beta$ 取值越大，指数加权平均天数越多，平均后的趋势线越平缓，趋势线会右移，有一定的延迟，给予前一天的值 $v_t$ 权重越高，当天值 $\theta_t$ 权重越低。下图绿色曲线和黄色曲线分别表示了 $\beta=0.98$ 和 $\beta=0.5$ 时，指数加权平均的结果。 由此，可以得到指数加权平均的递推公式 V_0=0\\ Repeat:\{\\ \qquad Get\ next\ \theta_t\\ V_\theta:=\beta V_\theta + (1-\beta)\theta_t\\ \}偏移校正 在开始时的几天，没有经过校正的紫色线是低于经过校正的绿色线的，所以，对 $V_t$ 做如下的处理 V_t = \frac{V_t}{1-\beta^t}这样，会将开始时的 $V_t$ 校正的大一些，而当 $t$ 逐渐增大时，$V_t$ 基本不受影响。但是在机器学习中，往往不需要偏移校正，因为迭代的初始过程一般都被忽略，等迭代到一定次数后再取值。 动量梯度下降 上图是针对两个参数优化的损失函数等高线图。像图示这种病态的二次函数（长而窄或具有陡峭边的山谷），动量梯度下降比传统的梯度下降快很多。也就是利用指数加权平均更新权重 $W$ 和 $b​$。 图中蓝色线表示原始的梯度下降算法，在下降过程中振荡比较大，时间多浪费在峡谷窄轴上的来回移动，产生类似折线的效果，而动量梯度下降，则在短轴方向上的振荡更小，在长轴上的前进速度更快，更快的到达最小值处。 权值更新的公式如下： V_{dw} = \beta v_{dw} + (1-\beta)dw\\ V_{db} = \beta v_{db} + (1-\beta)db\\ W = W - \alpha dw\\ b = b - \alpha db这里，$V{dw}$ 可以看成速度，$dw$ 则可以看成加速度，当前的速度由之前的速度和当前的加速度共同影响，$\beta &lt; 1$，限制了 $V{dw}$ 过大，起到了阻力的作用。 $\beta$ 表示之前梯度的贡献衰减的有多快，一般设置 $\beta = 0.9$，实际应用效果较好。关于偏移校正可以不使用。 其他文献资料中可能有这种写法 V_{dw} = \beta v_{dw} + dw\\ V_{db} = \beta v_{db} + db\\这样简化了表达式，学习因子 $\alpha = \frac{1}{1-\beta}$，$\alpha$ 也受 $\beta$ 的影响，但这种写法不够直观，且涉及调参 $\alpha$，更推荐第一种写法。 动量梯度下降的代码123456789# 初始化速度vs = []for i in net.parameters(): vs.append(torch.zeros_like(i.data))def momentum(parameters, vs, lr, gamma): for para, v in zip(parameters, vs): v[:] = gamma * v + lr * para.grad.data para.data = para.data - v 参考链接： http://redstonewill.com/1077/ https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/momentum.ipynb]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>梯度下降</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pillow,Jupyter,nbextensions 的几个报错和配置]]></title>
    <url>%2Farchives%2F2d3dc282.html</url>
    <content type="text"><![CDATA[运行 from scipy.misc import imread 时，出现报错 12345678910111213141516171819202122232425262728293031----&gt; 1 from scipy.misc import imread~\Miniconda3\lib\site-packages\scipy\misc\__init__.py in &lt;module&gt;() 66 from numpy import who as _who, source as _source, info as _info 67 import numpy as np---&gt; 68 from scipy.interpolate._pade import pade as _pade 69 from scipy.special import (comb as _comb, logsumexp as _lsm, 70 factorial as _fact, factorial2 as _fact2, factorialk as _factk)~\Miniconda3\lib\site-packages\scipy\interpolate\__init__.py in &lt;module&gt;() 173 from __future__ import division, print_function, absolute_import 174--&gt; 175 from .interpolate import * 176 from .fitpack import * 177~\Miniconda3\lib\site-packages\scipy\interpolate\interpolate.py in &lt;module&gt;() 19 20 import scipy.linalg---&gt; 21 import scipy.special as spec 22 from scipy.special import comb 23~\Miniconda3\lib\site-packages\scipy\special\__init__.py in &lt;module&gt;() 638 from .sf_error import SpecialFunctionWarning, SpecialFunctionError 639--&gt; 640 from ._ufuncs import * 641 642 from .basic import *ImportError: DLL load failed: 找不到指定的模块。 这与 conda 安装的 scipy 和 pillow 模块有关 解决 123conda uninstall scipy pillowpip uninstall scipy pillowpip install scipy pillow Nbextension 不生效，浏览器中没有 tab 不要安装新版 Jupyter（&gt;=5.6.0），否则在菜单栏出现汉字，而且没有 Nbextensions，找不到 Nbextension Tab 的。 直接用 pip 指定 Jupyter 版本 123pip install notebook==5.5.0pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user Numpy 求 std 时，报错 kernel died,restarting 没有查出原因，可能是 Python kernal 有问题，把 Python 和 numpy 重装后没问题。 Jupyter Theme 配置 conda install -c conda-forge jupyterthemes jt -t chesterish -cellw 90% -lineh 170 -T -N -ofs 10 -fs 12 -dfs 10 -tfs 12 latex 插件安装 jupyter nbextension install --py latex_envs]]></content>
      <categories>
        <category>Bug</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n softmax.ipynb 作业总结]]></title>
    <url>%2Farchives%2Fc8ceebf3.html</url>
    <content type="text"><![CDATA[np.hstack()与 np.concatenate(axis=1) 结果相同 12345678910111213141516171819202122232425In [24]: s = np.random.randn(3, 4)In [23]: bOut[23]:array([[1.], [1.], [1.]])In [20]: np.hstack([s, b])Out[20]:array([[ 3.58719594e-01, -3.96078700e-04, -1.20676992e+00, -9.79090334e-01, 1.00000000e+00], [-4.75659533e-01, -9.49646129e-01, -2.21206243e+00, -1.59346684e+00, 1.00000000e+00], [-3.22145168e-01, 4.25854204e-02, 5.46945002e-01, 1.32429428e+00, 1.00000000e+00]])In [21]: np.concatenate((s, b), axis=1)Out[21]:array([[ 3.58719594e-01, -3.96078700e-04, -1.20676992e+00, -9.79090334e-01, 1.00000000e+00], [-4.75659533e-01, -9.49646129e-01, -2.21206243e+00, -1.59346684e+00, 1.00000000e+00], [-3.22145168e-01, 4.25854204e-02, 5.46945002e-01, 1.32429428e+00, 1.00000000e+00]]) 利用导数定义求数值梯度123456789101112131415161718192021def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5): """ f 是个返回 loss 的函数 sample a few random elements and only return numerical in this dimensions. """ for i in range(num_checks): ix = tuple([randrange(m) for m in x.shape]) oldval = x[ix] x[ix] = oldval + h # increment by h fxph = f(x) # evaluate f(x + h) x[ix] = oldval - h # increment by h fxmh = f(x) # evaluate f(x - h) x[ix] = oldval # reset grad_numerical = (fxph - fxmh) / (2 * h) grad_analytic = analytic_grad[ix] rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic)) print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture4 笔记]]></title>
    <url>%2Farchives%2Fe68b9a2f.html</url>
    <content type="text"><![CDATA[主要讲解如何计算任何复杂函数的解析梯度。 下图是计算图的框架。 包括得分矩阵，损失函数，正则项 $R(W)$。 一旦可以用计算图模型表示一个函数，可以递归调用链式法则，计算图中每个变量的梯度。 反向传播如何工作？直观理解 反向传播最终得到的是最终变量在输入方向上的梯度。对于函数 $f(x, y, z) = (x + y) z$，用计算图模型来表示，两个节点分别表示 $+$ 和 $*$ 运算，想要得到 $f$ 对于变量 x, y, z 变化的敏感程度，也就是 $\frac{\partial f}{\partial x}$，$\frac{\partial f}{\partial y}$，$\frac{\partial f}{\partial z}$。令 $q = x + y$， $f = qz$， 得到 \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1\\ \frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q从图末端开始计算，依次可以计算出 \frac{\partial f}{\partial f} = 1; \\ \frac{\partial f}{\partial z} = q = 3; \\ \frac{\partial f}{\partial q} = z = -4; \\ \frac{\partial f}{\partial y} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial y} = -4 \times 1 = -4\\ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x} = -4 \times 1 = -4\\ 对于一些非常复杂的表达式，可以分解成一些计算节点，用简单的计算和链式法则计算出梯度。 绿色的数字代表前向传播，从输入的值计算得到输出，通过前向传入的值可以得出这些反向计算得到的梯度的值。比如 $\frac{\partial f}{\partial z} = q$，$q$ 的值可以直接通过前向传播得到。 反向传播： 我们希望得到损失 L 对输入单元 x 和 y 的梯度。在前向传播过程，输入 x 和 y，得到的输出 z，通过 z 得到最终的损失 L，这是前向传播的过程。回溯的过程中，先得到 $\frac{\partial L}{\partial z}$ 梯度，损失 L 对输入 x 的梯度等于本地梯度 $\frac{\partial z}{\partial x}$ 乘以上游梯度值 $\frac{\partial L}{\partial z}$。 基本的思路就是接受上游传回来的梯度再乘以本地梯度，通过前向传播的值计算本地梯度。然后将值传给其下一个相连的节点。 Sigmoid 门 f(w, x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}前向传播 反向传播 \frac{\partial f}{\partial f} = 1\\ 1/x 节点：f(x) = \frac{1}{x};本地梯度：\frac{\partial f}{\partial x}= -\frac{1}{x^2} = -\frac{1}{1.37^2} = -0.53；上游梯度：1；结果 = -0.53\times 1 = -0.53\\ +1 节点：f(x) = x+1；结果 = 1\times (-0.53) = -0.53\\ exp 节点：f(x) = e^x; 结果 = e^{-1} \times (-0.53) = -0.2\\ \times(-1) 节点：f(x) = -x；结果 = -1 \times (-0.2) = 0.2 \\ + 节点分支 1：f(w_2) = w_2 + C；\frac{\partial f}{\partial w_2} = 1 \times 0.2 = 0.2\\ + 节点分支 2：f(x) = x + w_2；结果 = 1 \times 0.2 = 0.2\\ + 节点分支 1：f(x) = x + C；结果 = 1 \times 0.2 = 0.2\\ + 节点分支 2：f(x) = x + C；结果 = 1 \times 0.2 = 0.2\\ \times 节点：f(w_0) = w_0 x_0；\frac{\partial f}{\partial w_0} = x_0 \times 0.2 = -0.2\\ \times 节点：f(x_0) = w_0 x_0；\frac{\partial f}{\partial x_0} = w_0 \times 0.2 = 0.4\\ \times 节点：f(w_1) = w_1 x_1；\frac{\partial f}{\partial w_1} = x_1 \times 0.2 = -0.4\\ \times 节点：f(x_1) = w_1 x_1；\frac{\partial f}{\partial x_1} = w_1 \times 0.2 = -0.6\\相比于直接对复杂解析式求梯度，为什么这样计算会简单一些，因为有了本地表达式，根据前向传播的值填充求出梯度，利用链式法则和上游传回的梯度，就可以求出当前节点的梯度值，传给下一节点，最终得到对所有变量的梯度。 利用解析梯度$\sigma(x) = \frac{1}{1+e^{-x}}$ $\frac{d}{dx}\sigma(x) = (1-\sigma(x))\sigma(x)$ 上图蓝色部分可以看作一个 sigmoid 门，将数值输入表达式后 \sigma(x) = \frac{1}{1 + e^{-1}} = 0.73\\ 梯度 = (1 - 0.73) \times 0.73 \times 1 = 0.2在计算图的简化和表达式的简单之间做出权衡。 加法门上图中，可以看出加法门只是负责给相连接的两个分支分发相同的梯度。梯度分发器。 max 门max 门将获取上游梯度，将其路由到其中一个分支。只有这个最大值影响了这个分支的值。梯度路由器 乘法门本地梯度即另一个变量的值，梯度转换器。梯度根据另一个分支的值进行缩放。 多个上游梯度如果有多个上游梯度，上游梯度等于所有节点的上游梯度加和。前向传播时，该节点影响所有的下游节点，反向传播时，所有分节点梯度回流到这个节点。 为什么是相加？不是相乘？ 如果变量 $x$ 有多条影响函数 $f$ 的路径，则在计算 $\frac{\partial f}{\partial x}$ 时需要对每条路径求导并加和。 举个栗子，$f(x) = (2x + 1)x + x^2$，可以先把 $x$ 看成三个不同的变量，把 $f$ 看成 $(2x_1 + 1)x_2+x_3^2$，分别对 $x_1$，$x_2$，$x_3$ 求偏导，$\frac{\partial f}{\partial x_1} = 2x_2$，$\frac{\partial f}{\partial x_2} = 2x_1+1$，$\frac{\partial f}{\partial x_3} = 2x_3$。把三项加起来，$2x_2 + (2x_1 + 1) + 2x_3$，去掉下标，最终结果为 $6x+1$。 在讨论神经网络的复杂函数时，我们得到每个节点的梯度，在梯度方向上前进一步，就可以更新权重（参数）。 向量的情况雅克比矩阵 假设 x, y, z 是向量，那么 $\frac{\partial z}{\partial x}$ 变成雅克比矩阵。 \frac{\partial z}{\partial x} = \begin{pmatrix} \frac{\partial z_1}{\partial x_1}&\frac{\partial z_1}{\partial x_2}&\frac{\partial z_1}{\partial x_3}\\ \frac{\partial z_2}{\partial x_1}&\frac{\partial z_2}{\partial x_2}&\frac{\partial z_2}{\partial x_3}\\ \frac{\partial z_3}{\partial x_1}&\frac{\partial z_3}{\partial x_2}&\frac{\partial z_3}{\partial x_3}\\ \end{pmatrix} 比如，$f(x)$ 是将图像向量中的每个像素与 0 比较取最大值，最终得到 4096 维的向量。雅克比矩阵是 $4096 \times 4096$ 维的，实际运算时，雅克比矩阵可能更大而无法实现。实际上，max 们形成的这个雅克比矩阵是对角矩阵，因为 $z_1$ 只与 $x_1$ 有关，$z_2$ 只与 $x_2$ 有关。 线性函数 f(x, W) = ||W\cdot x||^2 = \Sigma_{i=1}^n (W\cdot x)_i^2 x 是 $n\times 1$ 的向量，W 是 $n \times n$ 的向量。 q = W \cdot x = \begin{pmatrix} W_{11}x_1 + W_{12}x_2 + ...+ W_{1n}x_n\\ ...\\ W_{n1}x_1 + W_{n2}x_2 + ...+ W_{nn}x_n\\ \end{pmatrix}\\ f(q) = ||q||^2 = q_1^2 + q_2^2 + ..+ q_n^2正向传播 反向传播 $\frac{\partial f}{\partial W}$ \frac{\partial f}{\partial f} = 1\\ L2 节点： \frac{\partial f}{\partial q} \times 1= 2q = \begin{pmatrix} 0.44\\ 0.52 \end{pmatrix}\\ \times 节点： W 分支：\frac{\partial q_k}{\partial W_{ij}} = 1(i=k)x_j\\ \frac{\partial f}{W_{ij}} = \Sigma_k \frac{\partial f}{\partial q_k}\frac{\partial q_k}{\partial W_{ij}} = \Sigma_k (2q_k)1(i=k)x_j = 2q_ix_j\\ 写成向量的形式：\nabla_Wf = 2q\cdot x^T也可以向量求导公式 $\frac{dxA}{dx} = A^T$，$\frac{dWx}{dW} = x^T\$，利用链式法则，$\nabla_Wf = \frac{\partial f}{\partial q} \times \frac{\partial q}{\partial W} = 2q \cdot x^T$。 利用上面的公式可以计算出 \frac{\partial f}{\partial W} = \begin{pmatrix} 0.88 & 0.176\\ 0.104 & 0.208 \end{pmatrix} $\frac{\partial f}{\partial x}$ \times 节点： x 分支：\frac{\partial q_k}{\partial x_i} = W_{k,i}\\ \frac{\partial f}{x_{i}} = \Sigma_k \frac{\partial f}{\partial q_k}\frac{\partial q_k}{\partial x_{i}} = \Sigma_k (2q_k)W_{k, i}\\ 写成向量的形式：\nabla_xf = W^T\cdot 2q 或者利用维度相容原理（对向量 $x$ 求偏导的结果与变量 $x$ 的维度相同，每个梯度元素量化了每个元素对最终输出的影响）和向量求导公式也可以得出，$\nabla_xf = \frac{\partial f}{\partial q} \times \frac{\partial q}{\partial x} = W^T \cdot 2q$ \frac{\partial f}{\partial x} = \begin{pmatrix} -0.112\\ 0.636 \end{pmatrix}反向传播代码如果损失函数为 $L$，如果 $x, y, z$ 都是向量，$dx$ 表示 $\frac{\partial L}{\partial x}$，$dy$ 表示 $\frac{\partial L}{\partial y}$，$dz$ 表示 $\frac{\partial L}{\partial z}$。 乘法门 1234567891011class MultiGate(object): def forward(x, y): z = x*y self.x = x self.y = y def backward(dz): dx = self.y * dz dy = seld.x * dz return [dx, dy] 总结在神经网络中，将所有参数的梯度写下来是不现实，使用反向传播和链式法则计算梯度是神经网络的一个核心技术。 总的来说，前向传播计算节点输出，并存储中间值，反向传播中利用上游梯度和本地梯度计算输出节点方向上的梯度。 神经网络 一个两层的神经网络。线性层（$W_1$）+非线性计算层（max）+线性层（$W_2$）得到最后的积分向量。神经网络便是用层次化的方式将简单函数堆叠起来，与其他非线性函数结合在一起，形成复杂的函数。 以图像分类为例。 假设，测试集样本共有 500 个，图像维度是 3072，在之前分类中 $W$ 是 $3072 \times 10$ ，$x$ 是 $500 \times 3072$，通过 $x \cdot W$ 得到 $500 \times 10$ 的矩阵，每一行代表每个样本在每个分类中的得分； 中间隐藏层的尺寸是网络的超参数，这里设为 100。在两层神经网络中，$W_1$ 是 $3072 \times 100$，$W_2$ 是 $100 \times 10$，非线性函数是 Sigmoid 函数 $\frac{1}{1+e^{-xw}}$，通过 $x\cdot W_1$ 得到 $500 \times 100$ 的矩阵，传入 Sigmoid 函数中，进行权重分配，得到 $500 \times 100$ 的矩阵 $h$，随后矩阵 $h$ 与 $W_2$ 相乘，得到 $500 \times 10$ 的矩阵，对应每个样本在每个分类中的得分。 $W_1$ 共有 100 列，可以理解把原来 10 个分类变成 100 个分类，更加细化了，比如原来只有车一个分类，细化成红色车，黄色车等等。 $W_1$ 是样本 $x$ 的权重矩阵，$x\cdot W_1$ 后得到很多范本的的得分，$W_2$ 是对所有这些范本的权重矩阵，来确定最后属于哪个分类的得分。$h$ 是非线性后的结果。 非线性函数（激活函数）的作用是什么？ ReLU 非线性函数可以认为是与生物神经元的行为最为类似的激活函数，信号高于某个阈值，神经元将激活。一个两层的神经网络，$y = w_2A(w_1x)$， A 代表激活函数，如果不加非线性层（激活函数），相当于 $w_2w_1x = wx$，两层神经网络变成一层网络， 所以，如果没有激活函数，所有的神经网络都将变成一层。下图是激活函数对神经网络的影响。 使用激活函数后，通过改变权重，可以实现任意形状，越复杂的网络拟合的形状越复杂，这就是著名的神经网络万有逼近定律。 两层神经网络代码12345678910111213141516N, D_in, H, D_out = 500, 3072, 100, 10x, y = np.random.randn(N, D_in), np.random.randn(N, D_out)w1, w2 = np.random.randn(D_in, H), np.random.randn(H, D_out)for i in range(2000): h = 1/(1 + np.exp(-(np.dot(x, w1)))) y_pred = np.dot(h, w2) loss = np.square(y_pred - y).sum() d_y_pred = y - y_pred dh = d_y_pred.dot(w2.T) dw2 = h.T.dot(d_y_pred) dv = h * (1-h) * dh dw1 = x.T.dot(dv) w1 -= 1e-4 * dw1 w2 -= 1e-4 * dw2 激活函数 全连接层前面描述的两层神经网络就是全连接层，共有两个线性层，针对每个线性层都做了一次矩阵乘法，称为两个全连接层，也称单隐藏层神经网络。 正向传播中，利用一次矩阵乘法得到了某层所有神经元的输出结果。 作为分类器的神经元如果神经元的输出端有一个损失函数，那么这个神经元就可以作为一个线性分类器。比如前面讲到的 Softmax 分类器和 SVM 分类器。]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture3 笔记]]></title>
    <url>%2Farchives%2Fec4e9336.html</url>
    <content type="text"><![CDATA[线性分类中权重矩阵的每一行都对应一个分类模版，每一行的值都图像像素值的权重，如果将某一分类图像的像素平均值乘以这些对应的权重，就可以得到可视化每个类的模版。 作业中尝试画一下每个分类的可视化表示。 线性分类的另一种解释，高维空间中的线性决策边界。 损失函数损失函数：输入 W，计算得分，定量估计 W 的好坏。 通用公式 L = \frac{1}{N}\sum_i L_i(f(x_i, W), y_i)$f(x_i, W)$ 代表模型给出的预测分类，$y_i$ 实际的样本分类，$L_i$ 是某个损失函数，最后计算所有每个样本损失值的平均值。在所有的 W 中，寻找训练集上损失函数 L 最小的 W。 多分类 SVM 损失 多分类 SVM 是处理二分类问题的一种推广。 $s$ 代表通过 $f(xi, W)$ 得到的不同分类的分值，$y_i$ 代表真实分类的样本，$s{y_i}$ 是训练样本的真实分类分数，$s_j$ 是第 $j$ 个样本的分类分值 $f(x_i, W)_j$，对图片中的所有非真实分类样本（$j \neq y_i$）的损失相加，最后对整个训练集取平均得到最终的损失值。 对每个图片来说，损失计算分成两种情况，如果真实分类样本分数 $s{y_i}$ 减去非该分类（错误分类）的分类分数 $s_j$ 大于某个阈值$\Delta$（这里取 1），那么损失记为 0；如果小于 0，那么等于 $-(s{yi} - (s_j + 1)) = s_j + 1 - s{y_i}$。 图中 $s{y_i}$ 代表 x 轴，x 轴上的点是 $s_j$，当 $s{yi} &gt; s_j + 1$ 时，损失变成 0，否则损失的大小与 $s{y_i}$ 值呈线性负相关。 从 Hinge 损失的图像来看，SVM 只关心真实分类的分数减去错误分类分数的差值是否大于阈值 $\Delta$ 。如果大于，那么不论差值多大，损失都为 0；如果小于 $\Delta$，那么就要计算损失，而且这个差值越小，损失越大。 比如对于一个 3 分类，输入的某张图像后，输出分类分值是 $[13, -7, 11]$，这张图像实际分类是第一个类别，分数为 13，假设 $\Delta = 10$，那么将所有不正确分类加起来 L_i = max(0, -7-13+10) + max(0, 11-13+10) \\ = 0 + 8 = 8实际上，我们并不关心 $\Delta$ 的具体取值是多少，我们只关心正确分类与错误分类的分数的相对差值，如果通过缩放改变权重矩阵 $W$，最终得到的分类分数都会相应的放大或缩小，$\Delta$ 的倍数变化会被消掉，W 与 $\Delta$ 互相约束。 SVM 损失的最小值是 0（所有样本都分对），最大值趋近于无穷。 如果初始化的权重矩阵 $W \approx 0$ 且呈现均匀分布，最终得到的损失函数将 $\approx C -1$ ，遍历 $C - 1$ 项，$s_{y_i}$ 和 $s_j$ 近似相等，值接近 $\Delta = 1$，因为已知损失函数，在调试时常用。 如果将正确分类的样本也加进去，损失函数将加 1，因为 $max(0, s{y_i} - s{y_i} + 1) = 1$ 如果使用平均值，只会缩放损失函数，并不影响最终结果 可以自己构造损失函数，比如 $\Sigma{j\neq y_i}max(0, s_j - s{y_i} + 1)^2$，使用平方折叶损失 SVM 将更强烈的惩罚严重过界的边界值，平方折页损失用一种非线性的方法去评价好和坏的平衡。损失函数应该根据实践应用去选择。 使 $L = 0$ 的 W 是唯一的吗？否，可以将正确分类的 W 乘以两倍也是可以实现零损失，此时的边界值 $\Delta$ 也应该翻倍。 实际上，我们并不关心模型在训练集上的好坏，测试数据的准确性才是最关心的。下图蓝线便是过拟合的一个例子，绿色线则是泛化能力更强的模型。机器学习中，我们通常用正则化的方法防止过拟合。正则项的出现体现了奥卡姆剃刀的原则，如果有多个假设可以解释你的观察结果，选择更简约的模型。比如说，线性回归中，加入正则项可以使幂次降低。 L(W) = \frac{1}{N}\Sigma_{i=1}^nL_i(f(x_i, W), y_i) + \lambda R(W)前半部分称为 Data loss，$R(W)​$ 称为正则化惩罚项，超参数 $\lambda​$ 可以通过交叉验证的方式来获得。 L1/L2 正则化在这篇博客中集中做了总结，L1 L2 正则化理解。 最大边界SVM 中的目标函数是 $max_{w}\frac{1}{||W||}$，约束条件是 $y_i(W^Tx_i + b) \geq 1$，对所有样本都有约束。可以写成 min_{w, b}(\frac{1}{2}||W||^2)\\ s.t. y_i(W^Tx_i + b) \geq 1, i=1, 2, ..., n从另一个角度看，就是约束条件下，$L2$ 正则项作为损失函数的最小化。SVM 在算法上已经考虑到过拟合了。 多项 Logistic 回归损失（softmax loss）多项 svm 损失中，我们得到的每个分类的得分，除了知道正确分类的得分比不正确分类的得分要高之外，并没有解释这些得分的真正内涵。 Softmax 损失中，输出的归一化的分类概率。我们希望输出第 i 个样本的正确分类的概率是 1，我们定义第 i 个样本的输出概率是 P(Y=k|X = x_i) = \frac{e^{s_{y_i}}}{\Sigma_je^{s_j}}当 $s_{y_i}$ 的分数远大于其他值时，比如预测分类的分数是 $(5, -1, -1, -1)$ 时，第一个分类是图像的正确分类，最终得到的概率等于 P(Y=1|X=x_i) = \frac{e^5}{e^5+e^{-1}+e^{-1}+e^{-1}} = 0.993概率是接近于 1 的，而 $log(P(Y=1|X=xi))$ 几乎接近于 0，所以 可以将 $log(\frac{e^{s{y_i}}}{\Sigma_je^{s_j}})$ 作为损失函数，然而，如果某个分类出现的概率很小时，损失函数应该增大，所以，取负号，损失函数变成 L_i = - log(\frac{e^{s_{y_i}}}{\Sigma_je^{s_j}})KL 散度及 MLE 的理解后面再补充。 实际运算的归一化（数值稳定）由于存在指数次幂的形式，在大数值的指数计算时可能导致数值爆炸，通常在指数上做一些手脚。 \frac{e^{s_{y_i}}}{\Sigma_je^{s_j}} = \frac{Ce^{s_{y_i}}}{C\Sigma_je^{s_j}}= \frac{e^{s_{y_i}+logC}}{\Sigma_je^{s_j+logC}}通常情况，$logC = -max_j(f_j)$，在每个数值上都减去最大值后，使最大值归一化为 0，再进行指数运算。 $(5, -1, -1, -1)$ 归一化后变成 (0, -6, -6, -6) Softmax 的最小值是 0，最大值是无穷大。最小值为 0 仅仅是理想情况，当正确分类分值正无穷大，其他分类分值负无穷大时取得；最大值同时也是理想情况下得到。 如果所有权重分类十分接近于 0，某一分类损失函数的输出是 $logC$，因为 $P(Y=k|X = x_i) = \frac{e^{s{y_i}}}{\Sigma_je^{s_j}} = \frac{1}{C}$，在纠错机制中可以用。 SVM 损失与 softmax 损失的比较 SVM 损失给出的是正确分类的分值比其他分类的分值高出一个阈值，如果正确分类高出这个阈值很多后，即使在正确分类的分值上做一些微小的变化，并不会改变最终的损失值。换句话说，并没有给出衡量正确的分类有多正确的标准。 Softmax 损失则给出了每个分类的概率分布，给出了属于每个分类的可能性，Softmax 对于分数的要求不会满足，只要正确分类分数越高，错误分类分数越低，损失值总能变小。但是这种概率分布也与正则化参数 $\lambda$ 有关，随着正则化参数越来越大，权重 $W$ 将被惩罚的更多，权重 $W$ 的数值也越来越小，最终输出的概率趋近于均匀分布。Softmax 损失在从 Top n 中选择一个分类的场景中十分适合。 SVM/Softmax/正则项 损失函数的优化 上图左代表在一个维度上，一个样本沿某个随机方向损失函数的变化 $L(W+\alpha W_1)$ ，中代表一个数据集在两个维度上，损失函数的变化 $L(W+\alpha_1 W_1+\alpha_2W_2)$，右代表 100 个样本沿两个维度损失值的切片图。蓝色是低损失区域，红色是高损失区域 左图表示 SVM 在每个维度方向上对数据损失的展示，为什么会呈现出两个不同的方向？ 这里的 $x$ 和 $w$ 都是一维的，假设共有 3 个类别。横坐标代表权重 $w$，随着 $w$ 的变化，loss 是存在各个方向的。比如这里的横坐标是代表第一个类别的权重，随着 $w$ 变大，属于这个类别的 loss 逐渐降低至 0，不属于这个类别的， $w$ 越大，loss 越大。对每个损失（凸函数）取最大值后，SVM 的损失函数仍然是凸函数。 如何优化 $W$ 选择较小的损失函数？随机采样并不可取。实际上，沿梯度下降的方向进行选择是在训练大型神经网络，线性分类器等常采用的方法，梯度就是多元情况下的偏导数组成的向量，指的是函数增加速度最快的方向，负梯度方向则是函数下降最快的方向。深度学习很多情况都是利用梯度更新参数。 有限差分法（数值梯度）有限差分法去计算每个维度上的梯度，在每个维度上进行一个变化很小的量 $h$，计算前后的损失值变化，利用导数的定义 $\frac{df(x)}{dx} = \frac{f(x+h) - f(x)}{h}$ 求出在每个维度上的梯度，最后将梯度存储在变量 grad 中，变量 grad 的维度与图像向量 x 的维度相同。 这种方法的缺点效率太慢，权重矩阵的维度是 $10\times 3073$（10 个分类），每走一步需要计算 30730 次损失函数的梯度，不适合大规模数据。 在实际操作，我们会以公式的形式计算梯度，称为解析梯度，用数值梯度的方法验证解析梯度的准确性。是非常有用的 debug 方法。 作业中用的是一阶算法，速度上比较快。 梯度下降算法 初始化权重向量 $W$ 求损失函数梯度 沿负梯度方向（函数下降最快的方向）更新权重 SVM 梯度下降以 SVM 损失函数为例， L_i = \Sigma_{j\neq y_i}[max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta)]对 $w_{y_i}$ 求导后 \Delta_{w_{y_i}}L_i = -(\Sigma_{j\neq y_i}1(w_j^Tx_i - w_{y_i}^Tx_i + \Delta \gt 0)x_i)对 $w_j$ 求导后 \Delta w_{j\neq y_i}L_i = 1(w_j^Tx_i - w_{y_i}^Tx_i + \Delta \gt 0)x_i这里两点注意， 矩阵对向量的求导 $\frac{\partial (x^T A)}{\partial x} = A$ 对于 $w_{y_i}$，需要计算所有 $j\neq y_i$ 的向量的偏导，最后加和； 对于 $w_j$，只需要计算一次样本 $j$ 的偏导就可以了。 Softmax 梯度下降 L_i = - log(\frac{e^{w_{y_i}^Tx_i}}{\Sigma_je^{w_j^Tx_i}}) = -(w_{y_i}^Tx_i) + log \Sigma_je^{w_j^Tx_i}对 $w_{y_i}$ 求导后 \Delta_{w_{y_i}}L_i = -x_i + \frac{\Delta_{w_{y_i}} \Sigma_j e^{w_j^Tx_i}}{\Sigma_je^{w_j^Tx_i}} \\ = -x_i + \frac{e^{w_{y_i}^Tx_i}}{\Sigma_je^{w_j^Tx_i}}x_i\\ =x_i(s_{y_i}- 1)对 $w_{j}$ 求导后 \Delta_{w_{j}}L_i = \frac{\Delta_{w_{j}} \Sigma_j e^{w_j^Tx_i}}{\Sigma_je^{w_j^Tx_i}} \\ = \frac{e^{w_{j}^Tx_i}}{\Sigma_je^{w_j^Tx_i}}x_i\\ =x_is_{j}首先找到合适的学习率和步长，去考虑正则化等其他方面的事情。 除了普通的梯度下降之外，还有带动量的梯度下降或 Adam 优化器，基本的思想就是利用每一步的梯度决定下一步的方向，如何利用梯度的信息不同而已。 Mini-batch 梯度下降（Mini-batch Gradient descent）利用小批量数据进行梯度下降及参数更新。CNN 中典型的小批量数据是利用 256 个样本。小批量数据是对整体数据集梯度的近似，可以更快速的收敛，更频繁的参数更新。可以看作是对真实数值期望的一种蒙特卡洛估计。 Mini-batch SD 的数据量大小是个超参数，一般不需要交叉验证来调参，一般由存储器的限制来决定，或者直接设成 2 的倍数个，比如 32，64，128 等，运算更快。 随机梯度下降每次只用一个数据样本进行梯度下降，也叫在线梯度下降。但是这种情况比较少见，向量化操作中，进行 100 次 SGD 比一次计算 100 个样本的 Mini-batch GD 要慢得多。 图像的特征直接将图像的像素值传入线性分类器，由于多模态的原因，表现似乎不太好。常用的是采取两步的策略，先计算与图像特征有关的数值，比如计算与图片形象有关的数值，然后将不同的特征向量合到一起，传给线性分类器。 比如，在左图中，可以利用极坐标变换将数据由线性不可分转换成线性可分。但对图像元素来说，通常不采用极坐标变换，而是采用其他有作用的图像特征表示传入分类器，比如方向梯度直方图。 方向梯度直方图强调图像中有向边缘的重要性 获取图像，按照八个像素区分成八份 在八个像素区的每个部分计算每个像素值的主要边缘方向 将这些边缘方向量化到几个组 在每个区域内计算不同的边缘方向，从而得到一个直方图 全特征向量就是这些不同组的边缘方向直方图 可以看出图像中有哪些不同类型的边缘。比如在下图中可以看到大量的对角边缘。 词袋模型 从所有图像中进行小的随机块的采样，作为“词”。用 K-mean 等聚合成簇，获得聚类中心。这些聚类中心可能代表“词”的不同类型。 视觉单词获取的不同颜色就像不同方向有向边缘的不同类型 视觉单词也可以作为码本对图像进行编码，在图像中出现的次数等特征。 之前的图像分类通过计算词袋或方向梯度直方图等不同特征表示的图像，来喂给线性分类器。神经网络并非提前记录特征，而是直接从数据中学习特征。将像素值输入 CNN 中，经过计算，得到数据驱动的特征表示的类型，在整个网络中训练所有权重。]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture2 笔记]]></title>
    <url>%2Farchives%2F20e493a8.html</url>
    <content type="text"><![CDATA[图像分类时，系统已经清楚了一些确定的分类或标签的集合，计算机的任务是看图片，分配一些图像标签。 图像的边缘对于计算机视觉十分重要。但这种算法的识别效果并不是可拓展的，对于其他对象需要推倒重写。 所以，现在是数据驱动，不需要具体的分类规则来识别，抓取大量的不同类别数据集，训练机器，生成模型，总结出不同分类对象的核心知识要素。 1234567def train(): ... return modelsdef predict(): ... return test_labels K 近邻几乎是最简单的分类器，训练过程只需要单纯的存储数据，预测时，只需要找到与新图片最相似的图片，给出一个标签。 机器学习中常用的图片数据集 CIFAR10，32 x 32 像素的图片。 KNN如何比较两张图片的相似度呢？常用的欧式距离计算方式是先平方再开方的形式，称为二范式（L2）。 d = \sqrt{(x_1^{(1)}-x_1^{(2)})^2 + (x_2^{(1)}-x_2^{(2)})^2+ ... + (x_n^{(1)}-x_n^{(2)})^2} =||x^{(1)} - x^{(2)}||_2如果是一范式，那就称为曼哈顿距离（L1 距离），在每个维度上差的绝对值的加和，像曼哈顿街区。 d = |x_1^{(1)}-x_1^{(2)}| + |x_2^{(1)}-x_2^{(2)}|+ ... + |x_n^{(1)}-x_n^{(2)}| 训练速度和预测速度问题假设有 n 个样本，KNN (K Nearest Neighbor) 在训练时的复杂度是 $O(1)$，因为只需要存储数据，而预测时的复杂度是 $O(n)$。这并不是我们想要的，理想情况是训练时可以与样本数量有关，可以花更多的时间，而预测速度需要更快，分类器需要更快的执行。代码表示 K=1 时的情况（NN）。 123456789101112131415161718class NN(): def __init__(self, X, y): """ X 是 N x D 的样本，每行代表一个样本 """ self.xtr = X self.ytr = y def predict(self, X): sample_n = self.xtr.shape[0] # 存储每个样本的预测结果 y_prd = np.zeros(sample_n) # 遍历每个样本 for i in range(sample_n): distance = np.sum(np.abs(self.xtr - X[i, :]), axis=1) min_dst = np.argmin(distance) y_prd[i] = self.ytr[min_dst] return y_prd K 的取值根据空间位置分类效果并不是很好，有分类错误的点，绿色中的黄色点周围都会被划分成黄色，这就造成较高的错误率，同时失真信号或噪声比较明显。 噪声的出现因为在这个区域附近离绿点是最近的，所以才会被预测成绿色。 更多情况，会选择前 k 个距离最近的点，采用多数投票的方法决定属于哪个分类，这样，黄色周围的点不会被划分成黄色了，决策边界也更加平滑，对噪声也有更好的鲁棒性。 白色区域代表在这些区域的附近没有距离最近的点，也就是在投票时两个以上分类的票数相同。 低 k 值更容易过拟合，更高的 k 值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。 L1 和 L2 距离 L1 中，方形上的每个点与原点在 L1 上等距的，因为 L1 的计算是每个维度上差的绝对值的加和，$|x_1^{(1)} - 0| + |x_2^{(2)} - 0|$。 L2 则表现的像个圆。 L1 距离取决于坐标系统L1 距离依赖于你的坐标系统，如果转动坐标轴，将改变点与点的 L1 距离，对 L2 距离毫无影响，L2 距离是确定的。 如果输入的特征向量有些重要的意义，比如对员工进行分类时的薪资，工作年限等特征，L1 可能更适合；如果只是空间中的通用向量，实际代表的含义未知，可能 L2 更适合些。最好的方法是两种都尝试一下，看哪个效果好。 其他距离通过指定不同的距离计算方式，就可以更好的将这个算法应用到不同的领域。 K 和 距离度量的选择K 的个数和距离度量这些没办法直接从数据中学习得到的参数称为超参数。 选择不同的超参数，看哪个效果好： 并不是选择在训练数据上的预测效果最好的超参； 同样也不是测试集上预测效果最好的超参。这组参数只是反映了在这组测试集上的表现，在从未见过的全新数据上的表现仍未知，很可能出现测试集过拟合，实际部署效果会差很多。 这两种超参选择的方式都不推荐。 更好的方式是用训练集的一部分数据训练好参数，用另一部分数据做验证集，选择效果最好的参数，再拿到测试集上跑。 测试数据集只使用一次，即在训练完成后评价最终的模型时使用。 交叉验证在小数据集上常用，在深度学习中并不常用，将训练集分成若干份，轮流让每一份做验证集，最后取所有 k 次验证结果的平均值作为验证结果。深度学习中，训练过程消耗很大的计算能力，这种方法并不常用。 假设所有的数据都是独立的，服从同一概率分布，但现实并非如此，测试集也不能代表真实的时间。实践中常常采用对数据集进行随机划分。 图像分类中的做法图像分类中常常不采用交叉验证 计算量太大 L1 和 L2 距离不适合用在比较图像上，这种向量化的距离函数不适合表示图像间视觉的相似度。 KNN 的劣势 KNN 实际上是由背景主导的，这也是非常大的劣势。 KNN 没有参数，每接受一个测试数据，都需要与全部的训练数据做计算，时间非常久。 维度灾难K 近邻需要将空间划分成不同的部分，要求训练数据在空间中能够密集分布，否则待测数据可能离训练数据的实际距离特别远，相似性没那么高。因此在高维空间里，常常需要指数倍的增加数据，比如在二维空间中，需要 $4^2$ 个覆盖整个空间，在三维空间中，则需要 $4^3$ 个覆盖空间…，但实际的图片集数量可能没那么大。 决策边界KNN 的决策边界是非线性的，取决于 K 的样本中出现次数最多的类别。 线性分类网络中最基本的构建块，基础模块 模型参数输入图像，经 $f(x, W)$，输出属于某个分类的得分情况。 KNN 中是没有参数的，在测试时需要所有的训练数据，而线性分类在测试时只需要参数即可。 F(x, W) = Wx + b\\ (10, 1) = (10, 3072) \times (3072, 1) + (10, 1)输出 10 个分类中每个分类的得分情况。$b$ 仅代表针对某一类的偏好值，不与训练数据交互。不平衡数据中，可以对某一分类权重增加。 模版匹配权重矩阵 $W$ 的每一行对应图像的每个元素，然后做点积，最终再加上偏移量 $b$，得到最终的分类。$W$ 也可以认为是从输入图像 $x$ 到最终分类的映射函数。 如果已经有了线性分类器，$f(x, W) = Wx + b$，就可以得到权重矩阵的行向量对应每个类别的可视化结果。 行向量时如何对应到每个类别的？ 在训练过程中，我们指定矩阵的某个行向量对应到某个已知的分类上。 线性分类器问题每个类别只能学习一个模版（行向量），每次使用一个单独的模版去识别其中的每个类别，如果出现不同的变体，那么它将去这些变体的平均值。而神经网络模型则没有限制。 下方是经过训练得到的每个类别的模糊图像，怎么画出来的？ 将权重矩阵的一行看作是一个不同的分类，在一行中，每个值代表图像像素值的不同权重，将这一行 normalize 到 [0, 1]，再乘以 255，就可以得到针对某个类别的可视化图像（上图最后一行图像）。 从另一个角度讲，线性分类也可以看作是 KNN，只不过现在每个类别只有一张图片（训练好的行向量），使用内积来计算每个分类与图片向量的距离，而非使用 L1 或 L2 距离。 高维空间每个分类的划分都是将当前分类与除该分类之外的其他分类作对比，这正是 Softmax 的思想。以红线为例，所有红线右侧的是正例，所有红线左侧的是负例。 对于 $W$ 的几何解释：如果改变 $W$ 的某一行，那么行对应的分类器的直线将进行旋转，如果改变 $b$，分类器对应的直线将平移。 无法划分 奇数和偶数的划分（相反的象限），多分类问题（最右侧），蓝色出现在不同的象限。 多模态数据是线性分类使用受限的地方。 偏差处理的技巧 将 $W$ 和 $b$ 合并成一个矩阵，这样只需要学习一个权重矩阵就行。 图像数据预处理零均值的中心化 计算出所有训练集图像的平均值 每个图像减去这个平均值，数据分布由 $[0, 255]$ 变成 $[-127, 127]$ 使数值分布区间变为 $[-1, 1]$。 交叉验证交叉验证的目的是为了得到更好的超参数，比如学习率，迭代次数和 batch size 等。 如果数据集大，一般不做 k-fold cross validation，只取一份固定的验证集，直接保存测试效果最好的模型和超参数； 如果数据集不大，通过 k fold 得到最好的超参后，重新在整个训练集上训练模型，保存测试效果最好的模型作为预测。 如果预测的超参数很多，那么需要更大的验证集来有效的估计，如果验证集太小，会出现很大的随机性，这也是为什么 k fold cross validation 需要取平均来表示该超参数下模型的性能。]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture1 笔记]]></title>
    <url>%2Farchives%2Fae6b944b.html</url>
    <content type="text"><![CDATA[早期的 Object Segmentation 的任务时把一张图片中的像素点归类到有意义的区域。 2000 年，统计机器学习方法出现了一些面部检测的方法，比如 SVM，boosting，图模型等。特别是 Adaboost。 SIFT 特征检测，已知一个特征去检测另一张图像中的相同特征。某些特征在变化中具有表现性和不变性，去检测这些特征要简单得多。 图片质量的提高确保了获得更高质量的数据，提出一个重要的问题——目标识别。标注数据集的出现十分重要，最有名的是 PASCAL visual object challenge，有 20 个分类。 传统的机器学习模型在复杂的可视化数据上可能会过拟合问题，维数太高，要调的参数过多。 IamgeNet 则拥有巨大的图像数据集，14 M images，22 K categories。ImageNet 图像识别大赛。2012 年卷积神经网络模型使得识别错误率大幅下降，展示了强大的模型容量和能力，深度学习开始在计算机视觉，NLP及语音识别领域取得进步 图像分类ImageNet了解 ImageNet 挑战赛背景；图像分类的比赛 2012 年，AlexNet 七到八层，CNN 2014 年，GoogleNet 19 层 基本分类器创建一个基本分类器，接受输入，进行图像分类； 目标检测与图像摘要目标检测（画出边界框），图像摘要（生成描述）生成； CNN 90 年代出现，应用于手写字符的识别 计算能力的提高，GPU 图像处理单元，超高的并行计算能力非常适合卷积神经网络，以及大量高质量标签数据集的出现，使得图像视觉飞速发展。 更有挑战性的话题，3D 图形，动作识别，增强现实和虚拟现实等。 图像基因组，一系列属性，位置等描述。 圣杯：以非常深刻的方式去理解一张图片的故事，即使图像一闪而过。 深刻的图像理解仍有很长的路。 花书 DeepLearning 选读 课程目标 理解所有 CV 算法背后的深层架构，对算法的深层机理有深刻的理解，比如说搭建起神经网络后，内部发生的事情，架构的设置产生什么影响，网络是如何训练和测试的 从零实现完整的 CNN 网络，实现正向和反向传播算法 最新的论文内容]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵求导]]></title>
    <url>%2Farchives%2F5ceaab19.html</url>
    <content type="text"><![CDATA[实值函数对向量求导 \frac{dx^T}{dx} = I； \frac{dx}{x^T} = I\\ \frac{da^Tx}{dx} = a； \frac{dx^Ta}{dx} = a；\\ \frac{dAx}{dx} = A^T； \frac{dAx}{dx^T} = A\\ \frac{dx^TA}{dx} = A； \frac{dxA}{dx} = A^T\\ \frac{dx^TAx}{dx} = (A+A^T)x；\\如果 $A$ 对称矩阵，可以简化为 $2Ax$。 向量函数 $u(x)$ \frac{\partial u}{\partial x} = (\frac{\partial u^T}{\partial x})^T 向量函数内积的求导法则 若存在 $u(x)$ 和 $v(x)$，则 \frac{\partial u^Tv}{\partial x} = (\frac{\partial u}{\partial x})^Tv+ (\frac{\partial v}{\partial x})^T u $X$ 未做特殊说明的求导变量 \frac{\partial u^TXv}{\partial X} = uv^T\\ \frac{\partial u^TX^TXu}{\partial X} = 2Xuu^T\\ \frac{\partial [(Xu - v)^T(Xu -v)]}{\partial X} = 2(Xu - v)u^T 当 $f: R^n \rightarrow R^m$ 时，向量对向量的求导将得到雅克比矩阵，第 (i, j) 个元素 $\frac{\partial y_i}{\partial x_j}$ y = (y_1, y_2, ...y_m)^T\\ x = (x_1, x_2, ...x_n)^T\\ \frac{\partial y}{\partial x} = \begin{pmatrix} \frac{\partial y_1}{\partial x_1}&\frac{\partial y_1}{\partial x_2}&...\frac{\partial q_1}{\partial x_n}\\ ...\\ \frac{\partial y_n}{\partial x_1}&\frac{\partial y_n}{\partial x_2}&...\frac{\partial y_n}{\partial x_n}\\ \end{pmatrix}{} 标量对向量求导 X = \begin{pmatrix}x_{11} & x_{12}\\ x_{21}& x_{22}\end{pmatrix}\\ \frac{\partial L} {\partial X} = \begin{pmatrix} \frac{\partial L}{\partial x_{11}} & \frac{\partial L}{\partial x_{12}}\\ \frac{\partial L}{\partial x_{21}}& \frac{\partial L}{\partial x_{2}} \end{pmatrix}比如对于向量 (1, 2, 3, 4) 乘以 2 再加和后的导数是 (2, 2, 2, 2)。因为对于每个 $x_i$ 先乘二再加和的导数都是 2。 平均值求导： x = (x_1, x_2, ..., x_n)\\ u = \frac{1}{n}\Sigma_{i=1}^{N}x_i\\ \frac{\partial u}{\partial x} = \frac{1}{n} 线性变换中的反向传播 Y = XW，损失函数为 L，求\frac{\partial L}{\partial X}\\ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y}\frac{\partial Y}{\partial X} = \frac{\partial L}{\partial Y}W^T \\ \frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y}\frac{\partial Y}{\partial W} = X^T\frac{\partial L}{\partial Y}例如 Z = XY\\ X = \begin{pmatrix} 1&2\\ 3&4 \end{pmatrix}\\ Y = \begin{pmatrix} 4\\5 \end{pmatrix}\\ Z = \begin{pmatrix}14\\32\end{pmatrix}\\ L = mean(Z) = 23\\ \frac{\partial L}{\partial Z} = \begin{pmatrix} \frac{1}{2}\\\frac{1}{2} \end{pmatrix}\\ \frac{\partial L}{\partial X} = \frac{\partial L}{\partial Z}Y^T =\begin{pmatrix} \frac{1}{2}\\\frac{1}{2} \end{pmatrix}\begin{pmatrix} 4&5 \end{pmatrix} =\begin{pmatrix} 2&2.5\\2&2.5 \end{pmatrix}\\ \frac{\partial L}{\partial W} = X^T\frac{\partial L}{\partial Z} =\begin{pmatrix} 1&3\\2&4 \end{pmatrix} \begin{pmatrix} \frac{1}{2}\\\frac{1}{2} \end{pmatrix} =\begin{pmatrix} 2\\3 \end{pmatrix} Pytorch 中复杂情况下的求导 m = (m_0, m_1) = (2, 3)\\ n = (n_0, n_1) = (m_0^2, m_1^3) = (4, 27)\\ \frac{\partial n}{\partial m} = \frac{\partial (n_0, n_1)}{\partial (m_0, m_1)}\\ 自动求导需要传入和 n 一样大的参数 w，这里是 w_0，w_1。\\ 求导结果是\\ \frac{\partial n}{\partial m_0} = w_0\frac{\partial n_0}{\partial m_0} + w_1\frac{\partial n_1}{\partial m_0}\\ \frac{\partial n}{\partial m_1} = w_0\frac{\partial n_0}{\partial m_1} + w_1\frac{\partial n_1}{\partial m_1}\\ 具体参考 https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions]]></content>
      <categories>
        <category>机器学习</category>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[L1 L2 正则化理解]]></title>
    <url>%2Farchives%2Fdd5856e4.html</url>
    <content type="text"><![CDATA[在深度学习的线性分类中的损失函数通常是下面的样子 L(W) = \frac{1}{N}\Sigma_{i=1}^nL_i(f(x_i, W), y_i) + \lambda R(W)前半部分称为 Data loss，$R(W)$ 称为正则化惩罚项，超参数 $\lambda$ 可以通过交叉验证的方式来获得。 对正则化的理解 机器学习中，越复杂的模型中越会尝试对样本中的所有点进行拟合，包括一些异常值和噪声，带来的结果往往是模型的参数过大，由于学到一些噪声和异常值，过拟合常常使模型不稳定，variance 大。 正则化的方法是给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。避免模型的参数过大，带来过拟合的风险。 正则化方法有L0，L1和 L2 正则化，L0 正则是指非零参数的个数，在选择特征时，直接选择参数非零的特征即可，L1 正则是指各个参数绝对值的加和，L2 正则是各个参数的平方和的加和。 其实，加正则项和带约束条件的优化函数是等价的。 L1 正则化更容易得到稀疏解，大部分的 w 为 0，可用于特征选择，L2 正则倾向于赋予所有特征权重，很难得到完全稀疏的结果。 参考模型的拟合问题 - 方差偏差权衡、损失函数、L2，L1， Elastic Net弹性网络、K折交叉验证法 L1 与 L2 正则化$L1$ 正则化： R(W) = \Sigma_k \Sigma_l \|W_{k, l}\|$L2$ 正则化： R(W) = \Sigma_k \Sigma_l W_{k, l}^2$L_1 + L_2$ 正则化： R(W) = \Sigma_k \Sigma_l \beta W_{k,l}^2 + \|W_{k, l}\|$L_1$ 正则化鼓励最终得到的矩阵稀疏。 为什么 L1 更容易得到系数矩阵？下图解释了为什么 $L1$ 正则更容易得到稀疏矩阵。这张图展示的是 $L1$ 和 $L2$ 距离解的分布性。 当参数过多时，模型过于复杂，容易发生过拟合。通常寻找一些限定条件，比如在 $L2$ 正则中限定 $\Sigma_j{w_j^2} \leq C$，或者 $L1$ 正则中限定 $\Sigma_j{|w_j|} \leq C$。 $L2$ 和 $L1$ 的限定条件在坐标轴上的分别如上图阴影所示（注：$L1​$ 距离又叫曼哈顿距离，是各个维度绝对值的加和）。图中的圆圈代表损失函数的等值线，越靠近中心，损失函数越小。所以我们所求的问题转换成，在参数限定的上限内，寻找使损失函数最小的位置。 当损失函数的负梯度方向 $-\nabla E{in}$（蓝色箭头）与 $w$ 的运动方向（切线方向）垂直时，也就是损失函数的负梯度方向 $-\nabla E{in}$ 与 $w​$ 的方向平行时，有 -\nabla E_{in} + \lambda w = 0这时候，在保证 $w_j$ 满足限定条件时的损失函数最小。以 $L2$ 为例，我们构造一个新的损失函数 E_{aug} = E_{in} + \frac{\lambda}{2}w^2这个损失函数的梯度为 0 时，恰好能得到上述的平行关系式。换句话说，在损失函数与 $w$ 限定区域的最外层相切时，能够取得的损失函数最小。损失函数再往外，不满足 $w$ 的限定条件，再往里，损失会变大。$L1​$ 正则化也可以这样理解。 $L1$ 的限定区域是正方形，方形顶点与蓝色圆圈相交的概率很大，凸点更接近损失函数最优解的位置，而凸点意味着 $w_1$ 或 $w_2$ 有一个其中是 $0$，所以 $L_1$ 正则化的解更具有稀疏性，可以忍受其中某个参数很大，但希望有更多的 $0$ 或接近 $0$ 。$L2$ 距离不能忍受出现较大的参数。$L1$ 距离可以将得到的接近 0 或 0 的参数丢掉，更有利于做模型压缩和特征选择。 但 $L1​$ 距离非常难以优化。在沿损失下降的方向优化时，$|W|​$ 对 $W​$ 求导的结果要么是 1，要么是 -1，$L_2​$ 求导得到 $2 W​$。$L2​$ 距离在优化时，由于 $W​$ 的存在，梯度会随着优化而变小，能够自适应优化的过程，但 $L1​$ 的梯度是 $\pm 1​$，当 loss 比较小时，梯度可能会很大，比较难优化，如果不加正则项系数，非常容易过拟合。 避免模型过度复杂的方法 模型使用较低的阶数；加入正则项作为惩罚，减少特征 深度学习中的其他正则化方法 如 Dropout，batch normalization，stochastic depth（随机深度），这些惩罚的目的为了减轻模型的复杂度，而不是试图拟合数据。 举个栗子 在线性回归的语境下，$L1​$ 正则化得到的 $w_1 = (1, 0, 0, 0)​$，$L2​$ 正则得到的 $w_2 = (0.2, 0.3, 0.25, 0.25)​$，通常第二种的回归性会更好一些，原因是范数比较小的情况下，$L2​$ 正则结果能够更多的传递输入 $x​$ 在多大程度上对输出有影响，能够传递更多的元素信息，而并不是只传递某几个特定元素。 如何选择 $L1$ 或 $L2$ 正则 需要思考模型的复杂度如何度量。$L1​$ 考虑的是模型中非零元素的个数，$L2​$ 则考虑 $W​$ 的整体分布。 $L2$ 正则与贝叶斯最大后验概率（MAP）估计 如果我们假设 $w$ 服从高斯的先验分布，通过应用 MAP 可以得到 $L2$ 正则项。 参考文章 https://zhuanlan.zhihu.com/p/38309692 https://blog.csdn.net/vividonly/article/details/50723852]]></content>
      <categories>
        <category>机器学习</category>
        <category>正则化</category>
      </categories>
      <tags>
        <tag>L1/L2 正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n SVM.ipynb 作业总结]]></title>
    <url>%2Farchives%2Fcf1b5064.html</url>
    <content type="text"><![CDATA[SVM.ipynb 作业中主要实现了 SVM 向量化的损失函数 向量化求解损失函数的梯度 利用数值梯度方法验证梯度的准确性 利用验证集调节超参数学习率和正则项系数 利用 SGD 优化损失函数 对学习得到的权重矩阵对应的分类可视化 数据集划分数据集划分成训练集，验证集，测试集和 development data，development data 是从训练集中随机选择的一小部分。 数据处理 各类数据均减去训练数据平均值 数据的维度上 +1（各类数据均增加一列 1），使 $wx + b$ 转换成 $wx$。 利用循环的方法求 loss 和梯度求 loss 和梯度时考虑加上正则项。 利用数值梯度验证梯度的准确性random.randrange([start], stop, [, step]) 12345In [27]: random.randrange(5, 10)Out[27]: 7 In [29]: tuple([random.randrange(i) for i in (10, 5)])Out[29]: (0, 3) 利用下面的公式求数值梯度 \frac{df(x)}{dx} = \frac{f(x+h) - f(x - h)}{2h}利用向量化求 loss 和梯度求 loss 时的关键 np.maximum(0, scores - y_score + 1) 求梯度时的关键求 $\Delta Li w{y_i}$ ds[np.arange(num_train), y] = -1 * (np.sum(ds, axis=1))，最后再用矩阵乘法。 和 KNN.ipynb 相同，用 difference = np.linalg.norm(grad_naive - grad_vectorized, ord=&#39;fro&#39;) 验证两个向量的相似性。 np.max()np.max(a, axis=None, out=None, keepdims=False) 返回数组中的最大值 至少接受一个参数 1234567891011In [10]: aOut[10]:array([[7, 1, 5, 1], [4, 2, 4, 6], [4, 5, 4, 3]])In [11]: np.max(a)Out[11]: 7In [12]: np.max(a,axis=1)Out[12]: array([7, 6, 5]) np.maximum(X, Y, out=None)X，Y 逐位比较取最大者，至少需要两个参数 12345678910111213In [16]: b = np.arange(4, 7).reshape(-1, 1) In [18]: bOut[18]:array([[4], [5], [6]])In [17]: np.maximum(a, b)Out[17]:array([[7, 4, 5, 4], [5, 5, 5, 6], [6, 6, 6, 6]]) 可视化math.log10() plt.scatter 可以传入 s 和 c 参数表示点的大小和颜色（列表形式） 将一组数归一化到 [0，1] 的方法先减去最小值，然后除以最大值减最小值 123456789In [36]: s = np.random.randint(-10, 10, size=10)In [37]: sOut[37]: array([ -9, 2, 8, -2, -9, 9, 1, -10, 9, -7])In [38]: (s - s.min()) / (s.max() - s.min())Out[38]:array([0.05263158, 0.63157895, 0.94736842, 0.42105263, 0.05263158, 1. , 0.57894737, 0. , 1. , 0.15789474]) np.squeeze() 删除数组中值为 1 的维度，默认全部删除，axis 参数可以指定删除的轴。若指定轴的维度不等于 1，将报错。 1234567891011121314151617181920212223242526272829303132In [63]: x = np.array([[[0], [1], [2]]])In [64]: x.shapeOut[64]: (1, 3, 1)In [65]: x.squeeze()Out[65]: array([0, 1, 2])In [66]: x.squeeze(axis=0)Out[66]:array([[0], [1], [2]])In [67]: x = np.array([[[0, 1], [2, 3], [3, 4]]])In [68]: x.shapeOut[68]: (1, 3, 2)In [69]: x.squeeze(axis=0)Out[69]:array([[0, 1], [2, 3], [3, 4]])In [70]: x.squeeze(axis=1)---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-70-50e9153f96b9&gt; in &lt;module&gt;()----&gt; 1 x.squeeze(axis=1)ValueError: cannot select an axis to squeeze out which has size not equal to one]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anychart API]]></title>
    <url>%2Farchives%2Fbda569ce.html</url>
    <content type="text"><![CDATA[Common Settings Labelhttps://docs.anychart.com/Common_Settings/Labels 字体大小1chart.labels().fontSize(18); 字体颜色1chart.labels().fontColor('black') Legendhttps://docs.anychart.com/Common_Settings/Legend/Overview 字体大小1chart.legend().fontSize(20); 字体颜色1chart.legend().fontColor("black"); Sunbursthttps://docs.anychart.com/Basic_Charts/Sunburst_Chart 改变环形颜色fill() 12345678910111213&#123;name: "Microbial Species", children: [ &#123;name: "Bacteria", fill:"#00cec9", children: [ &#123;name: "Pseudomonas"&#125;, &#123;name: "Vibrio"&#125;, &#123;name: "Photobacterium"&#125;, &#123;name: "Aeromonas"&#125;, &#123;name: "Plesiomonas"&#125;, &#123;name: "Alteromonas"&#125;, &#123;name: "Acinetobacter"&#125;, &#123;name: "Bacillus"&#125;, &#123;name: "Corynebacterium"&#125;, &#123;name: "Moraxella"&#125; ]&#125;, 调整线宽和颜色1234567chart.container("container");chart.hovered().fill("#96a6a6", 0.4);chart.selected().fill("#96a6a6", 0.6);chart.selected().hatchFill("forward-diagonal", "#96a6a6", 0.5, 12);chart.normal().stroke("#96a6a6", 0.6);chart.hovered().stroke("#96a6a6", 0.4);chart.selected().stroke("#96a6a6", 0.7); 参考 https://docs.anychart.com/Appearance_Settings/Lines_Settings 将内层的 label 横置1234// set labels settingschart.labels()// set labels position .position('radial') 参考 https://www.anychart.com/products/anychart/gallery/Sunburst_Charts/Gold_Medal_Winners_at_the_Sochi_Olympics.php 隐藏某一层12// hide first levelchart.level(1).enabled(false); Mapbubble用 color 属性 legend 颜色才会跟随 bubble 颜色变化而变化，用 .fill() 时 legend 颜色不会发生改变。 123456789101112131415// Creates bubble seriesvar series1 = map.bubble() .data(dataSet) // Sets series settings .geoIdField(&apos;iso_a2&apos;) .color(&apos;#ff8f00 0.6&apos;) .stroke(&apos;1 #ff6f00 0.9&apos;);// Sets legend map.legend() .enabled(true) .position(&apos;center-top&apos;) .align(&apos;center&apos;) .itemsLayout(&apos;horizontal&apos;) .padding(0, 0, 30, 0) .paginator(false);]]></content>
      <categories>
        <category>可视化</category>
        <category>Anychart</category>
      </categories>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小象学院机器学习——SVM 笔记]]></title>
    <url>%2Farchives%2F75378e04.html</url>
    <content type="text"><![CDATA[在图像识别中，比如 Facenet，在得到最终分类结果的前一步中，可以将 m 个样本及 k 个神经元最终得到的 $m\times k​$ 个矩阵作为 X，分类结果作为 Y，应用 SVM，得到最终的分类结果。所以 SVM 这种经典的分类器并没有完全过时，但是，直接用 SVM 做图像分类的方式现在几乎不用了。 主要内容 SVM 的原理和目标（重要） SVM 的计算过程 软间隔最大化 线性不可分的数据的分割面 线性可分需要使用“软间隔”目标函数吗？ 了解核函数 了解 SMO 算法过程 调参的过程学习过程中，很重要的一点是学会调参，知道参数长什么样子。下面 4 张图，前 4 张是采用线性核的分类结果，红色线代表分界线；下面 8 张是采用高斯核的分类结果。 $\gamma$ 和 $C$ 大小高斯核中，$\gamma$ 值越大，越能够沿着不同类别的训练数据的边界给出，边界形状越扭曲，越不光滑。也就是说，理论上，对于二分类问题，总有一个足够大的 $\gamma$，能使分类器在训练集上的准确度达到 100%，但这种情况下更容易过拟合。 C 值越大，两侧虚线表示的过渡带越来越窄，落在过渡带上的样本更少，使用更少的样本进行了分类，有可能会过拟合，但是在训练集上的训练精度会更高。 训练精度高往往对应着泛化能力下降。 落在过渡带中的样本是否就是支持向量？是的 两条较深和较浅的虚线分别代表什么意思？等值线分别是 0.5 到 1 的值。 与线性核及 KNN 的关系$\gamma$ 值足够小时，分界线会逐渐退化成一条直线，高斯核逐渐变成线性核。 $\gamma$ 值足够大时，分类结果越靠近离它最近的点的分类结果，高斯核则逐渐退化成 KNN，尤其的 NN（Nearest Neighbor）。 线性可分 SVM 的推导过程线性可分 SVM 的假设是认为样本是线性可分的，所有样本可以由一个线性分割面全部正确分割。 分界面 $W^T\vec{x} + b = 0$对于一条直线 $Ax + By + C = 0​$，令 W = \begin {pmatrix} A\\B \end{pmatrix}\\ \vec{x} = \begin {pmatrix} x\\y \end{pmatrix}$C = b$，构造出的方程就是 $W^T\vec{x} + b = 0$，$W​$ 则可以看作直线的法线方向（分割超平面的法向量）。 以 $2x +y - 4 = 0​$ 为例，$(2， 1)​$ 可以看作直线的法线方向，将 $(2, 2)​$ 点代入方程后，$2\times 2 + 2 - 4 = 2 &gt; 0​$，点 $(2, 2)​$ 位于直线上方（法线方向正向），相反 $(0， 0)​$ 代入后，$2x + y - 4&lt;0​$，点位于直线下方（法线方向负向）。因此，$Ax + By +C = 0​$，也就是 $W^T\vec{x} +b = 0​$ 可以看作是分类的分界面。 任何一条直线都可以写成一个向量 $W​$ 和一个标量 $b​$ 的形式，任何一个样本点代入方程后，如果为正，那么位于法线方向的正向，如果为负，那么位于法线方向的负向。$W^T\vec{x} + b​$ 称为决策函数，$W^T\vec{x} + b = 0​$ 则称为线性分割超平面。 答疑 SVM 与 Adaboost 都具有可以得到 100% 训练集的正确分类能力，svm 的优势在哪？ 原理上，推导过程优雅；理论上，天生带防止过拟合的能力，分类效果不错。Adaboost 从理论上来说是容易过拟合的。 SVM 对样本的要求高吗？会像 Logistic Regression 一样，要求全部样本都参与吗？ 对样本的要求高，数据量不能太大，样本数据量大时，训练速度太慢，但是效果还是不错的。SVM 对样本的敏感性同样高，如果样本有噪声，要用线性 SVM，而不能用线性可分 SVM。 SVM 多分类模型？ 如果是 k 分类，直接在目标函数上修改，将多个分类面参数求解合并到一个最优化问题中，时间复杂度是 $k^2 \times$ 单个 SVM 二分类，常常采用二分类组成多分类模型，有两种方式： 1 vs rest 比如有 A、B、C、D 四种分类方式，A 与 BCD，B 与 ACD，C 与 ABD，D 与 ABC 做 4 个二分类器 或 1 vs 1 AB，AC，AD，BC，BD，CD 做 6 个二分类器。$k(k - 1) / 2​$ 个 SVM 防止过拟合的问题 目标函数上已经考虑到防止过拟合的问题 可以通过调参来防止过拟合，如开头所讲 样本不均衡问题 样本不均衡是通过外部数据变化来实现的，本身并不能解决这个问题。 目标函数样本 $i$ 到某条直线的距离样本 $i$ 表示为 $x_i = (x_1, x_2, …, x_n)^T$，某条直线为 $W_1x + b_1$，样本 $i$ 到直线的距离为 $\frac{|W_1x_i + b_1|}{||W_1||}$，分母 $||W_1||$ 是向量 $W_1$ 的二范式（先取平方和再开方）。样本 $i$ 的标记 $y_i \in {-1, 1}$，当属于样本正例时，$y_i = 1$，$W_1x_i + b_1 &gt; 0$；负例时，$y_i = -1$，$W_1x_i + b_1 &lt; 0$。绝对值 $|W_1x_i + b_1|$ 便可以写成 $y_i(W_1x_i + b_1)$，样本 $i$ 到直线的距离便是 $d(W_1, b_1, x_i) = \frac{y_i(W_1x_i + b_1)}{||W_1||}​$。 所有样本到直线最小距离的最大值根据上式，我们能拿到所有样本到该条直线的距离最小值 $mini(d)$，在所有可能的直线中，我们希望这个最小值越大越好，最小值越大，样本离直线越远，越可以抵抗噪声，于是有 $max{w, b}(min_i(d))$。目标函数就有 max_{w, b}(\frac{1}{||W||}min_i(y_i(Wx_i + b))，$||W||​$ 与样本没关系，可以作为系数提出来。 答疑 如何保证分割的直线在两个数据集之间？ 在根据目标函数选择直线（分割超平面）的前提是在这些直线（分割超平面）下，所有样本都已经分类正确，选择的思路不要过拟合，选出的这条直线是最不过拟合的（这里的过拟合实际是要求 $W$ 不要太大）。而如果其他直线的分割精度都达不到 100%，则不予考虑。这里的思路是先考虑正确率，再去想过拟合的问题。 什么是支持向量？ 从某种角度上讲，对于最终分割直线的 $W$ 与样本 $x_i$ 的维度是相同的，样本 $i$ 可以表示为 $(x_i, y_i)$，如果给定每个样本一个系数 $\alpha_i$，调整 $\alpha_i$，有可能使得 $\Sigma_i^n{\alpha_i y_i x_i} = W$，那么，如果仅有几个样本的 $\alpha_i$ 不等于 0，其他样本的 $\alpha_i$ 都等于 0，我们说这几个样本把这条直线支撑住了，这几个样本又叫支撑向量，这种方法叫支撑向量方法（Support Vector Machine, SVM）。在上图 Gap 区域内（虚线内）的样本都可以称为支撑向量。 所有样本都有权值，只不过大部分样本的权值 $\alpha_i$ 为 0，所以这又可以称为是稀疏模型。CNN 也是一种稀疏模型。 并不是越靠近红线权值 $\alpha_i$ 越大，只是说在虚线之间的 $\alpha$ 是大于 0 的，在直线上的点 $\alpha$ 可能是某个定值。在虚线外的点权值为 0。 真正的目标函数对于目标函数 $max_{w, b}(\frac{1}{||W||}min_i(y_i(Wx_i + b))$，假设是 K 维数据，分母 $||W|| = \sqrt{w_1^2 + w_2^2 +… + w _k^2}$，分子上也有 $W$，太难优化。所以转换一下思路。 样本 $(x_i, y_i)$ 到直线的距离公式为 $d = \frac{y_i(Wx_i + b)}{||W||}$，在计算所有样本点到直线的距离后，总是可以得到样本点到直线的最小距离以及最近距离样本点，最后，调整 $W$ 使得 $y_i(Wx_i + b)$ 值等于 1。 比如所有样本点中正例点 $(1, 1)$ 到直线 $3x + 4y + 3 = 0$ 的距离 $d = \frac{|3 + 4 + 3|}{\sqrt{4^2 + 3^ 2}} = 2$ 是最小的，如果，我们将直线表示成 $\frac{3}{10}x + \frac{4}{10}y + \frac{3}{10} = 0$，这和之前是同一条直线，那么，$y_i(Wx_i + b) = (1)\times(\frac{3}{10} \times 1 + \frac{4}{10} \times 1 + \frac{3}{10}) = 1$。 通过这种方式，总能找到 $mini(y_i(Wx_i + b)) = 1$ 的情况，所以，目标函数就变成 $max{w, b}\frac{1}{||W||}$，约束条件是 $y_i(W^Tx_i + b) \geq 1$，样本到直线的最小距离 1，对于所有样本都有约束，可以写成 min_{w, b}(\frac{1}{2}||W||^2)\\ s.t. y_i(W^Tx_i + b) \geq 1, i=1, 2, ..., n 答疑 这里的约束条件不是假设，是真正推导出来的。约束条件里是有样本信息 $x_i$ 的。 最小距离是通过计算所有样本到该直线的距离得到的，样本到直线的距离 $d = \frac{y_i(Wx_i + b)}{||W||}$ 是确定的，不变的，但 $y_i(Wx_i + b)$ 是可以变化的，总是可以调节 $W$ 使 $y_i(Wx_i + b)$ 等于 1 的。 最小距离等于 0 的情况要尽量避免，这时，这条分割直线恰好经过这个样本，对线性可分来说，这条直线是不合适的。 调整的过程直线本身是不变的，只是系数发生了变化，$3x + 4y - 10 = 0$ 和 $\frac{3}{2}x + 2y -5 = 0$ 是同一条直线。 当属于正例时，$y_i = 1$，$W_1x_i + b_1 &gt; 0$；负例时，$y_i = -1$，$W_1x_i + b_1 &lt; 0$。这也保证了这条直线把所有的样本都正确分开。 神经网络也可以处理非线性问题，为什么全连接层换成 SVM 更好呢？这个问题上 svm 有优势吗？ softmax 回归实际上就是一个全连接网络（这句话不理解。逻辑回归的实验课中讲过）。用 svm 的损失函数有时候要好一些，天然加入了防止过拟合的东西。一般来讲，2~3 次全连接也有可能解决问题，或者用全卷积网络，不使用全连接，可以省 GPU。 带约束的求极值问题用拉格朗日乘子法求带约束的极值问题如下图示，要求如下： 不等式约束 $f_i(x) \leq 0$ ，这在 SVM 目标函数的约束条件中需要正负号的转化。 不等式约束的乘子 $\lambda_i \geq 0$ ，等式约束的乘子 $\upsilon_j$ 属于实数域。 对 Lagrange 函数对 $x$ 求偏导，令偏导等于 0，解出 $x$ 关于 $\lambda$ 和 $\upsilon$ 的式子，回代到 Lagrange 函数中，得到关于 $\lambda$ 和 $\upsilon$ 的对偶函数。 对 $\lambda$ 和 $\upsilon$ 的对偶函数求最大值，对偶函数的最大值往往接近原函数 $f_0(x)$ 的最小值。 Lagrange 函数 min_{w, b}(\frac{1}{2}||W||^2)\\ s.t. y_i(W^Tx_i + b) \geq 1, i=1, 2, ..., n写成 Lagrange 函数 L(W, b, \alpha) = \frac{1}{2}||W||^2 - \Sigma_{i=1}^n\alpha_i (y_i(W^Tx_i + b) - 1)\\ s.t. \alpha_i \geq 0原问题是极小极大问题 min_{w, b}max_{\alpha}L(W, b, \alpha)转换成原问题的对偶问题，是极大极小问题 max_\alpha min_{W, b} L(W, b, \alpha)对偶问题： 对于一个函数 $f$，一定有 minmaxf>=maxminf也就是说最大的里面挑一个最小的也要比最小的里挑一个最大的要大。 强队偶问题： minmaxf = maxminfLagrange 函数分别对 W 和 b 求偏导， \frac{\partial{L}}{\partial W} = W - \Sigma_{i=1}^n\alpha_i y_i x_i = 0 \\ \rightarrow W = \Sigma_{i=1}^n\alpha_i y_i x_i\\ \frac{\partial L}{\partial b} = 0 \\ \rightarrow 0 = \Sigma_{i=1}^n\alpha_i y_i将 $W$ 和 $\Sigma_{i=1}^n\alpha_i y_i = 0$ 代入 $L(W, b, \alpha)$，得到一个关于 $\alpha$ 的函数，求这个关于 $\alpha$ 函数的最大值。推导过程注意，对向量来说 $||W||^2 = W^TW$，$\phi(x_i)$ 这里等于 $x_i​$。 最终得到的 $\alpha$ 个数是和样本个数相同的，求出 $\alpha$，便可以根据 $W = \Sigma_{i=1}^n\alpha_i y_i x_i$ 求出 $W$，可以根据 $\alpha$ 求出支撑向量，根据支撑向量可以求出 $b$ 值（为了更加鲁棒，求的是所有支持向量 b 的均值）。比如有两个支撑向量分别是 $x_1$ 和 $x_2$，$W$ 已知，过这两个支撑向量的直线分别是 $Wx_1 + b_1 = 0$，$Wx_2 + b_2 = 0$，那么 $b = (b_1 + b_2 )/2$。因为对于直线 $Wx + b = 0$，$W$ 代表斜率，$b$ 则代表截距。$W$ 和 $b$ 求出来后，决策面也就求出来了。 对 $\alpha​$ 求最大值是一个二次优化问题，可以采用坐标上升或者 SMO（序列化的最小优化问题） 的方法。 答疑 KKT 条件 保证了对偶问题的最大值等于原始问题的最小值，和前面的不同主要引入 $\lambda_i f_i(x) = 0​$，一共有 5 个条件，其他条件都是构建对偶函数以及求极值需要满足的。现在不那么重视这个了。 为什么要转换成对偶函数？ 对于有约束条件的求极值问题，往往构造 Lagrange 函数，通过求 Lagrange 函数的最大值，来近似原函数的最小值。 支撑向量的 $\alpha$ 是大于 0 的，非支撑向量的 $\alpha$ 值是等于 0 的。 $W = \Sigma_{i=1}^n \alpha_i y_i x_i$ 的结果证明了在答疑中的猜测是正确的。直观理解就是 $W$ 是样本的线性加权得到的。 线性支持向量机线性可分的支持向量机只用了极少数的样本将样本分类，会不会过拟合？会的。实线是线性可分的支持向量机结果，并不是最好的分类结果，过渡面也非常窄。 如果放弃掉左上角的样本，虚线则是线性可分的分类结果，这种分类结果使得过渡带更宽。如果过渡带变宽，待分类的样本含有噪声，会使得它被分错的概率更小。 对于任何一个样本，都给与一个不同的松弛因子 $\xi_i \geq 0​$，保证约束条件 $y_i(Wx_i +b ) \geq 1- \xi_i​$ 即可，举个栗子，如果 $y_i(Wx_i +b ) = -0.3​$，这个样本是分错的，给松弛因子 $\xi_i = 1.3​$。那么我们把 $\xi_i​$ 也加入到目标函数中 min_{W, b} = \frac{1}{2}||W||^2 + C \Sigma_{i=1}^n\xi_i\\ C > 0，是一个超参数推导过程目标函数现在变成 min_{W, b} = \frac{1}{2}||W||^2 + C \Sigma_{i=1}^n\xi _i\\ s.t. y_i(Wx_i + b) \geq 1 - \xi_i\\ \xi_i \geq 0, i=1, 2, ...,n通过构造 Lagrange 函数， L(W, b, \xi, \alpha, \upsilon) = \frac{1}{2}||W||^2 + C \Sigma_{i=1}^n\xi _i - \Sigma_{i=1}^n\alpha_i(y_i(Wx_i + b) - 1 + \xi_i) - \Sigma_{i=1}^n\upsilon_i \xi_i通过对 $W, b, \xi​$ 求偏导，令偏导等于 0，得到 $W​$，将 $W​$ 回代回去，整理不等式条件，得到下面关于 $\alpha_i​$ 的对偶问题。 答疑 松弛因子相当于降低样本权值的作用吗？ 答案是肯定的。通过松弛因子，在目标函数中引入超参数 C，C 的大小决定了 $\alphai​$ 的上限（$0\leq \alpha_i \leq C​$），通过对 $W​$ 求偏导，令偏导等于 0，得到 $W = \Sigma{i=1}^n\alpha_i y_i x_i​$，$\alpha_i​$ 的上限由此决定了 $W​$ 的上限，也就降低了样本权值，$W​$ 不会过大，则在一定程度上防止了过拟合。这就是 SVM 天然自带防止过拟合的原因。 损失函数$\xi$ 和 $\upsilon$ 大小的分析 我们将 $yi(Wx_i + b)​$ 暂且称为距离，实际上 $\frac{y_i(Wx_i + b)}{||W||}​$ 才是点到直线的距离。KKT 条件中，有一项需要不等式约束等于 0，$\Sigma{i=1}^n\upsilon_i \xi_i = 0​$，对 $\xi_i​$求导后，$C=\alpha_i+\upsilon_i​$。 在点 1 处，绿色点属于正例，分类正确，点离分隔线的距离大于 1，所以 $\xi_i​$ 等于 0 即可满足 $y_i(Wx_i + b) \geq 1 - \xi_i​$，$\upsilon_i=C​$，$\alpha_i=0​$，因为是非支撑向量。 在点 2 处，点恰好离分隔线的距离是 1，所以，这个点是 $\xi_i = 0​$ 的分界点，满足 $y_i(Wx_i + b) \geq 1 - \xi_i​$，因为是支撑向量，$0&lt;\alpha_i&lt;C​$。 在点 3 处，点离分隔线的距离大于 0 小于 1。假设距离是 0.4，所以需要 $\xi_i = 0.6​$ 才能满足 $y_i(Wx_i + b) \geq 1 - \xi_i​$，在这个区域内 $0 \lt \xi_i \lt 1​$，$\upsilon_i=0​$，$\alpha_i=C​$。 在点 4 处，点恰好在分隔线上，需要 $\xi_i = 1​$，才能满足 $y_i(Wx_i + b) \geq 1 - \xi_i​$，0 恰好等于 0。在分隔线上的点的 $\xi_i = 1​$，$\upsilon_i=0​$，$\alpha_i=C​$。 在点 5 处，属于错分类点，$y_i(Wx_i + b)​$ 是一个负值，需要 $\xi_i \gt 1 ​$ 才能满足 $y_i(Wx_i + b) \geq 1 - \xi_i​$。这个区域内的 $1 \lt \xi \lt 2​$，$\upsilon_i=0​$，$\alpha_i=C​$。 在点 6 处，错分类点离分隔线的距离恰好是 -1，需要 $\xi_i = 2$ 才能满足 $y_i(Wx_i + b) \geq 1 - \xi_i$，$\upsilon_i=0$，$\alpha_i=C$。 总结如下，对正例这侧来说，如果正确分类且在蓝色线外，$\xi_i​$ 都是等于 0 的，蓝色线是 $\xi_i = 0​$ 的分隔线，在分隔线和蓝色线之间，$0 \lt \xi_i\lt 1​$，在这个区间，虽然样本是分对的，但是样本分隔线太近了，一样会受到 $\xi_i​$ 的惩罚；在分隔线（红色线）上，$\xi_i = 1​$，在错误分类时，$\xi_i \gt 1​$，且错误分类点离分隔线越远，$\xi_i​$ 越大。 于是就可以画出点到分隔线的距离与 $\xi_i​$ 大小的图像，以点到分隔线的距离为横坐标，$\xi_i​$ 为纵坐标，可以得到蓝线表示的 Hinge 损失函数。实际上，每个样本的 $\xi_i​$ 的加和可以看作真正的 SVM 损失函数。 SVM 真正的损失函数（$\xi_i$ + L2 正则） min_{W, b} = \frac{1}{2}||W||^2 + C \Sigma_{i=1}^n\xi_i\\ C > 0，是一个超参数现在来看这个损失函数，将每个样本的松弛因子 $\xi_i$ 的加和看作是损失函数，样本的权值 $W$ 则是和线性回归中相同的 $L2$ 正则项，$L2$ 正则项作用是防止过拟合，因为过拟合时正则项 $W$ 通常会很大。在线性回归中的正则项 $\frac{1}{2}\lambda ||W||^2$，$\lambda$ 是超参数。在 SVM 损失中，超参数 $C$ 则可以看成 $\frac{1}{\lambda}$。当 $C$ 取无穷大时，$\xi_i$ 只要是一个大于 0 的数，损失函数将变得很大，这时候只有 $C = 0$，线性 SVM 退化成线性可分 SVM。 在线性可分 SVM 中，每个样本都分对了，且样本里直线的距离都大于等于 1，所以，所有的 $\xii​$ 项都等于 0，只保留了正则项 ，损失函数变成了 $min{w, b} (\frac{1}{2}||W||^2)​$ 。 几何原理上，我们可以说 SVM 是样本离直线的最小距离取最大； 机器学习原理上，我们可以说，损失函数是松弛因子 $\xi_i​$ 的加和加上 $L2​$ 正则项，所以，SVM 在算法上已经考虑到防止过拟合了。 $\xi_i​$ 是连接 SVM 几何原理和机器学习的桥梁。 答疑 松弛因子的意义可以说是选择性的允许某些点离分隔线的距离小于 1，不必要求所有点都大于 1。 当样本在不同位置时，$\xi$ 与超参数 $\upsilon$ 的关系是什么？ 在核函数中讲解 松弛因子就是造成损失的原因。 松弛因子的存在，实际上也是允许了噪声和特别个例的存在，也就是在过渡带内的某几个点，不用过分地考虑。如果想减轻这些噪声对分类的影响，$C$ 减小，允许出现更多的 $\xi_i &gt; 0$ 的情况，过渡带内的点变多，过渡带变宽，模型的泛化能力更强；$C$ 增大，会尽量少的出现 $\xi_i &gt; 0$ ，如果出现太多，损失函数会变得很大，过渡带变窄，模型在训练集上的准确性更强，但容易过拟合；极端情况下 $C$ 取无穷大时，过渡带退化成一条直线，过渡带内不存在样本，模型变成线性可分 SVM。 $C$ 是给出的是 $\alpha$ 的上限，$W$ 由 $\alpha$ 决定，因此，$C$ 的给出也防止了过拟合。 支撑向量个数越少，有可能分类精度越高，但也容易过拟合。 核函数核函数的出现为了解决样本点无法线性可分的情况。 核函数的思想先把数据映射到高维，在高维空间中分开，再回到低维空间的过程。 在线性可分 SVM 中得到的目标函数中 如果把 $x​$ 替换成 $\phi(x)​$，上面的目标函数变成 min_{\alpha}\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j-\sum_{i=1}^n\alpha_i\\ s.t. \Sigma_{i=1}^n\alpha_i y_i = 0\\ \alpha_i \geq 0, i=1, 2, ..., n最终我们并不关心 $\phi(x)$ 是什么，真正关心的 $\phi(x_i)$ 和 $\phi(x_j)$ 的点乘。因此，定义 $\kappa(x_i, x_j) = \phi(x_i)\cdot\phi(x_j)$ ，这里的点乘也表示 $\phi(x_i)$ 与 $\phi(x_j)$ 的相似度。 线性核： $\kappa(x_1, x_2) = x_1\cdot x_2$ 多项式核： $\kappa(x_1, x_2)= (x_1\cdot x_2 + c)^d​$ 比如 \kappa(x, y) = (x\cdot y)^2\\ =(\sum_{i=1}x_iy_i)^2\\ =\sum_{i=1}^n\sum_{j=1}^n(x_ix_j)(y_iy_j)$\phi(x)​$ 变成了什么？ \phi_(x) = vec(x_ix_j)|^n_{i, j=1}\\ n=3 时，\\ \phi(x) = \begin{pmatrix} x_1x_1\\ x_1x_2\\ x_1x_3\\ x_2x_1\\ x_2x_2\\ x_2x_3\\ x_3x_1\\ x_3x_2\\ x_3x_3\\ \end{pmatrix} \phi(y) = \begin{pmatrix} y_1y_1\\ y_1y_2\\ y_1y_3\\ y_2y_1\\ y_2y_2\\ y_2y_3\\ y_3y_1\\ y_3y_2\\ y_3y_3\\ \end{pmatrix}这样，将原来的 3 维（$x_1, x_2, x_3​$）映射成 9 维。由此看来，在计算 $\phi(x)​$ 时，是平方时间复杂度，计算量很大，而计算 $\kappa(x, y)​$ 时，只需要做按维度做对应的点乘就行。 所以说，核函数更像是一种技巧，绕过了 $\phi(x)​$ 的定义，直接使用 $\kappa(x_1, x_2)​$ 函数去计算。如此做的维度提升。而 linearRegression + polyfeature 的做法是直接使用 $\phi(x)​$ 计算。 高斯核: \kappa(x_1, x_2) = \ exp(-\gamma||x_1-x_2||^2)高斯核是无穷维的，理论上不能直接给出 $\phi(x)​$ 的值，但是我们只需要 $\kappa(x_1, x_2)​$ 就可以了。 $\gamma = \frac{1}{\sigma^2}​$，称为高斯核函数的精度，该值越大，高斯核函数越高瘦，很快就会衰减，越容易过拟合。该值越小，高斯核函数低矮。 SVM 加高斯核很容易过拟合，如果 SVM 加高斯核效果很好，那就要考虑是否过拟合了，反之，如果 SVM 加高斯核效果不好，那就是 SVM 这个方法并不适合。 sigmoid 核函数 \kappa(x_1, x_2) = tanh(x_1\cdot x_2 +c)核函数的直观理解对于线性 SVM 推导中有这样的结论 w = \sum_{i=1}^n \alpha_i y^{(i)}x^{(i)}将其代入 $y=wx+b​$ 中，得到 y = [\sum_{i=1}^n\alpha_i y^{(i)} x^{(i)}]x + b\\ =\sum_{i=1}^n[\alpha_iy^{(i)}x^{(i)}\cdot x] +b\\ =\sum_{i=1}\alpha_iy_i + b$x^{(i)}​$ 代表第 i 个样本的实际自变量（定值），$x​$ 代表要预测的样本的 x 值。把每个 $x_i​$， $y_i​$， $\alpha_i​$ 带入后最终得到的是预测值 $y​$。 所以，核函数的直观理解可以是把 $​$ 做某种映射后 $\kappa(x^{(i)}, x) = &lt;\phi (x^{(i)}), \phi(x)&gt;​$，作为 kernal 函数，最终得到 y。 SVR在得到的分界线选出上下两条线，落在中间的点称为支持向量。 SVM 和 logistic 回归的关系SVM 计算的是样本点距离分界面的距离，这个距离并不具有概率意义。但是，具有分界面越近的点越有可能被分错，所以，可以根据距离计算分到某类的置信度。 logistic 回归给出的则是属于哪个分类的后验概率。 答疑： 一般来说，SVC 的计算速度由优化算法确定的，比如选择 SMO，选择坐标上升或梯度下降法，与超参数的调节关系不大。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS Tensorflow Pytorch 环境搭建]]></title>
    <url>%2Farchives%2Febf9b807.html</url>
    <content type="text"><![CDATA[在入坑深度学习过程中，最开始面对的就是 TensorFlow、Pytorch 等框架的安装，在炸过一台 AWS 服务器后，终于把 TensorFlow-GPU 版和 Pytorch安装好了。 AWS 云计算平台的申请主要参考廖星宇老师的教程，https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/aws.md，要注意的是， 注册 AWS 帐号时，如果收不到验证电话，可以提单到 AWS，会有人工打电话过来确认帐号名称和电话。 在 Ubuntu 16.04 的系统镜像已经被移到最下方。在第一次安装时，我不慎选择了 Ubuntu 18.04 后，是没办法安装 Cuda8.0 的，选择 GPU 计算的实例类型时， 选择 p2.xlarge 只能安装 cuda 8。 创建实例时如果遇到一下错误，说明你在实例次数达到上限，请直接提单到 AWS，将错误粘贴发送 request 就可以解决。 123Summary of limit(s) requested for increase: [US East (Ohio)]: EC2 Instances / Instance Limit (p2.xlarge), New Limit = 1 GCC 安装12sudo apt-get updatesudo apt-get install build-essential make 确认安装成功 1234ubuntu@ip-172-31-18-15:~$ which cc/usr/bin/ccubuntu@ip-172-31-18-15:~$ which make/usr/bin/make Conda 安装可以选择 Anaconda 或 MiniConda，Anaconda 已经集成了科学计算包，Miniconda 仅安装了 Conda、Python 和 一些依赖项。 Miniconda 用的 Python 3.6.3 的版本，最新版本是 Python 3.7.0 的。建议用 Anaconda 安装，Miniconda Python 3.6.3 的 Numpy 计算 std 时会出现 kernel died。 Python 3.6.3 版本在创建虚拟环境后，环境里有单独的 Conda 管理器，而 Python 3.7.0 的 Conda 在虚拟环境里找不到虚拟环境中的 Conda。 Python 3.6 版的 Miniconda 链接：https://pan.baidu.com/s/1O7yQYJJAohRjoARpw69kNQ提取码：2fmz Python 3.6 版的 Anaconda 链接：https://pan.baidu.com/s/1bvp-SQpVw5u5cxrLJto1vA提取码：cr2k复制这段内容后打开百度网盘手机App，操作更方便哦 1sh Miniconda3-latest-Linuxpy36_x86_64.sh 注意最后选择 yes 加入环境变量 1source ~/.bashrc 确认安装成功 12ubuntu@ip-172-31-18-15:~$ which conda/home/ubuntu/miniconda3/bin/conda Jupyter Notebook 安装创建新的虚拟环境 1234ubuntu@ip-172-31-18-15:~/software$ conda create -n venv python=3.6ubuntu@ip-172-31-18-15:~/software$ source activate venv(venv) ubuntu@ip-172-31-18-15:~/software$ which conda/home/ubuntu/miniconda3/envs/venv/bin/conda 加入环境变量 12echo 'export PATH="/home/ubuntu/miniconda3/envs/venv/bin:$PATH"'&gt;&gt;~/.bashrcsource ~/.bashrc 1conda install -c anaconda jupyter 安装好 Jupyter 后需要配置一下服务器才能访问，配置主要参考 AWS 官网的教程。https://docs.aws.amazon.com/zh_cn/dlami/latest/devguide/setup-jupyter-config.html 设置密码123456cdmkdir sslcd sslsudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout "cert.key" -out "cert.pem" -batchipython# 运行 from IPython.lib import passwd;passwd() 以设置密码，获得密码哈希，记录密码哈希 1vi ~/.jupyter/jupyter_notebook_config.py # 如果没有请运行 jupyter notebook --generate-config 将以下文本粘贴到末尾 12c = get_config() # Get the config object.c.NotebookApp.password = 'sha1:fc216...密码哈希' 配置 Putty选择 配置 Windows 客户端 按照教程配置 Putty 客户端，https://docs.aws.amazon.com/zh_cn/dlami/latest/devguide/setup-jupyter-configure-client-windows.html，我这里分配的是 8800 端口，为避免与常用的 8888 端口冲突。 验证安装成功，运行 1jupyter notebook --port=&apos;8800&apos; 输入前面设置的密码 MacOS 可以通过 ssh 映射到本地 https://docs.aws.amazon.com/zh_cn/dlami/latest/devguide/setup-jupyter-configure-client-mac.html Jupyter Theme 配置conda install -c conda-forge jupyterthemes jt -t chesterish -cellw 90% -lineh 170 -T -N -fs 10 -dfs 10 -tfs 11 配置自动导入常用的包编辑 ~/.ipython/profile_default/ipython_kernel_config.py 和 ~/.ipython/profile_default/ipython_config.py，如果没有请运行 ipython profile create 1234567891011121314c = get_config()c.InteractiveShellApp.exec_lines = ["import pandas as pd","import numpy as np","import matplotlib.pyplot as plt","import matplotlib as mpl","import seaborn as sns","plt.rcParams['font.sans-serif']=['SimHei']","plt.style.use('fivethirtyeight')","plt.rcParams['axes.unicode_minus']=False",]c.IPKernelApp.matplotlib = 'inline'c.InteractiveShellApp.extensions = ['autoreload']c.InteractiveShellApp.exec_lines = ['%autoreload 2'] Cuda 8.0从 https://developer.nvidia.com/cuda-toolkit-archive 中找到合适的版本，如果购买的是 p2.xlarge，选择 Cuda 8.0 12wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-runsudo sh cuda_8.0.61_375.26_linux-run 12345678910111213141516accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?(y)es/(n)o/(q)uit: yDo you want to install the OpenGL libraries?(y)es/(n)o/(q)uit [ default is yes ]: yDo you want to run nvidia-xconfig?(y)es/(n)o/(q)uit [ default is no ]: nInstall the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location [ default is /usr/local/cuda-8.0 ]:Do you want to install a symbolic link at/usr/local/cuda?(y)es/(n)o/(q)uit: yInstall the CUDA 8.0 Samples?(y)es/(n)o/(q)uit: n 验证安装成功 123456789101112131415161718ubuntu@ip-172-31-18-15:~/software$ nvidia-smiSun Oct 14 08:04:30 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.26 Driver Version: 375.26 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla K80 Off | 0000:00:1E.0 Off | 0 || N/A 42C P0 68W / 149W | 0MiB / 11439MiB | 96% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 添加环境变量 12echo "export LD_LIBRARY_PATH=\$&#123;LD_LIBRARY_PATH&#125;:/usr/local/cuda-9.1/lib64" &gt;&gt;~/.bashrcsource ~/.bashrc libcupti-dev1sudo apt-get install libcupti-dev cudnn进入 https://developer.nvidia.com/cudnn，点击注册后，选择 8.0 Linux 版本下载 12345sudo wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/8.0_20171129/cudnn-8.0-linux-x64-v7 --no-check-certificatetar -xzvf cudnn-8.0-linux-x64-v7.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* TensorFlow 安装网上已经有用 conda 安装 TensorFlow 的教程，https://mp.weixin.qq.com/s/23eb0S69N-qKHK0EVFhl0w，这里的安装仍参考官网，通过创建 conda 虚拟环境的方式安装。https://tensorflow.google.cn/install/pip 123pip install --upgrade pippip install tensorflow-gpupip install --upgrade tensorflow 验证安装成功 12(venv) ubuntu@ip-172-31-18-15:~/software$ python -c "import tensorflow as tf; print(tf.__version__)"1.11.0 123456&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()2018-10-14 08:19:36.759432: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA&gt;&gt;&gt; print(sess.run(hello))b'Hello, TensorFlow!' Pytorch 安装Pytorch 采用官网的安装方式 https://pytorch.org/get-started/locally/ 12conda install pytorch torchvision cuda80 -c pytorchpip install torchvision 验证安装成功 12345678910(venv) ubuntu@ip-172-31-18-15:~$ pythonPython 3.6.6 |Anaconda, Inc.| (default, Oct 9 2018, 12:34:16)[GCC 7.3.0] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import torch&gt;&gt;&gt; x_gpu = torch.Tensor([3]).cuda()&gt;&gt;&gt; print(x_gpu)tensor([3.], device='cuda:0')&gt;&gt;&gt; x = torch.Tensor([3])## No GPU&gt;&gt;&gt; print(x) 参考： https://github.com/sharedeeply/DeepLearning-StartKit 创建镜像环境搭建好后，记得创建镜像，下次开始时直接使用镜像（AMI）创建实例，不需要从头开始。]]></content>
      <categories>
        <category>使用教程</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n KNN.ipynb 作业总结]]></title>
    <url>%2Farchives%2Fcbd20b23.html</url>
    <content type="text"><![CDATA[整理 KNN.ipynb 作业过程中学到的代码。 KNN 在训练过程中仅仅存储数据，在预测过程中，需要与所有的训练样本作比较，在 K 个中选出 Label 出现次数最多的分类。 习题内容包括读取图像内容并可视化，用 two loops，one loop 和 no loop 进行 KNN 距离计算；通过交叉验证确定最佳的 K。 cs231n.data_utils.pypickle 压缩保存文件CIFAR-10 数据集的每个文件都是 1 个 Python Pickle 对象，读取和保存需要用到 Pickle 模块，cPickle 是用 C 语言实现的，拥有更快的速度。 pickle 保存以 wb 的形式写入文件，最终会多出 pickle_example.pickle 文件，列表和字典都能被保存，dump 第三个参数 True 能够提高压缩比。 12345In [97]: from six.moves import cPickle as pickleIn [101]: with open('pickle_example.pickle', 'wb') as w: ...: a_dict = &#123;'da': 111, 2: [23,1,4], '23': &#123;1:2,'d':'sad'&#125;&#125; ...: pickle.dump(a_dict, w, True) pickle 提取文件123456789101112In [93]: with open('data_batch_1', 'rb') as f: ...: data = pickle.load(f, encoding='latin1') ...:In [94]: list(data.keys())Out[94]: ['batch_label', 'labels', 'data', 'filenames']In [95]: len(data['data'])Out[95]: 10000In [96]: len(data['data'][0])Out[96]: 3072 load 后以字典的形式保存。 查看 Python 版本1234In [102]: import platformIn [103]: platform.python_version_tuple()Out[103]: ('3', '6', '2') 123&gt;&gt;&gt; import platform&gt;&gt;&gt; platform.python_version_tuple()('2', '7', '14') array.transpose()高维数组的转置用 array.transpose()，一维或二维数组的转置一般用 array.T。 二维数组的 shape 返回的元组下标对应为 0 和 1，而 array.transpose() 接受的参数 shape 的元组索引，比如 b.transpose((1, 0)) 是交换 0 和 1 轴，数值 4 对应的索引 [1, 0] 变成 [0, 1]。 1234567891011121314In [109]: b = np.arange(12).reshape(3, 4)In [110]: bOut[110]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])In [111]: b.transpose((1, 0))Out[111]:array([[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]]) 三维数组 shape 返回的下标索引为 (0, 1, 2)，a.transpose((1, 2, 0)) 则是先交换 0和 1 轴，再交换 1 和 2 轴，元素 9 对应的下标由 [0, 2, 1] 变成 [2, 1, 0]。 12345678910111213141516171819202122232425262728293031323334In [105]: a = np.arange(24).reshape(2, 3, 4)In [106]: aOut[106]:array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) In [108]: a.transpose((1, 2, 0)) Out[108]: array([[[ 0, 12], [ 1, 13], [ 2, 14], [ 3, 15]], [[ 4, 16], [ 5, 17], [ 6, 18], [ 7, 19]], [[ 8, 20], [ 9, 21], [10, 22], [11, 23]]]) In [112]: a[0, 2, 1]Out[112]: 9In [113]: a.transpose((1, 2, 0))[2, 1, 0]Out[113]: 9 a.transpose((1, 2, 0)) 与 a.swapaxes(1, 0).swapaxes(1, 2) 等价 12345678910111213141516In [116]: a.swapaxes(1, 0).swapaxes(1, 2)Out[116]:array([[[ 0, 12], [ 1, 13], [ 2, 14], [ 3, 15]], [[ 4, 16], [ 5, 17], [ 6, 18], [ 7, 19]], [[ 8, 20], [ 9, 21], [10, 22], [11, 23]]]) np.concatenate()在合并多个 array 或 list 中常用，axis 控制了合并的轴。比 np.vstack() 和 np.hstack() 更灵活。 1234567891011In [122]: a = [[1, 2, 3, 4]]In [123]: b = [[5, 6, 7, 8]]In [124]: np.concatenate((a, b), axis=0)Out[124]:array([[1, 2, 3, 4], [5, 6, 7, 8]])In [125]: np.concatenate((a, b), axis=1)Out[125]: array([[1, 2, 3, 4, 5, 6, 7, 8]]) KNN.ipynbmatplotlibplt.rcParams设置图像中默认字体大小 1plt.rcParams['figure.figsize'] = (10.0, 8.0) plt.imshow()接受一个三维数组，显示一张图片 12345In [130]: X_train[1000].shapeOut[130]:(32, 32, 3)In [131]: plt.imshow(X_train[idx].astype('uint8')) plt.errorbar()对散点图添加误差棒，第一个参数为 x 轴坐标，第二个参数为 y 轴坐标，yerr 为计算的误差，多为标准差。 1plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std) ipython config在 notebook 里 import 模块后，如果模块发生变化，自动重新加载模块。 12%load_ext autoreload%autoreload 2 可以在 ~/.ipython/profile_default/ipython_config.py 中添加以下代码，在打开 ipython 后自动设置 12c.InteractiveShellApp.extensions = ['autoreload'] c.InteractiveShellApp.exec_lines = ['%autoreload 2'] 与 Python 解释器中的 from importlib import reload; reload(module)相同。 Numpynp.flatnonzero()返回扁平化矩阵后，非零元素的索引 123456789101112In [126]: a = np.arange(-5, 7).reshape(2, -1)In [127]: aOut[127]:array([[-5, -4, -3, -2, -1, 0], [ 1, 2, 3, 4, 5, 6]])In [128]: np.flatnonzero(a)Out[128]: array([ 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11], dtype=int64)In [129]: np.flatnonzero(a &gt; 0)Out[129]: array([ 6, 7, 8, 9, 10, 11], dtype=int64) np.linalg.norm()numpy 下求范数的方法，默认是二范数，即向量的每个元素平方的加和再开方的形式。 1234In [182]: a = np.array([3, 4])In [183]: np.linalg.norm(a)Out[183]: 5.0 orf 参数也可以指定其他范数，比如一范数 ord=1 和无穷范数 ord = np.inf 或 (np.-inf) 参数 说明 计算方法 默认 二范数：$l_2$ = $\sqrt{x_1^2 + x_2^2 + x_3^2} $ $ord=2$ 二范数：$l_2$ 同上 $ord = 1$ 一范数：$l_1$ = $ x_1 + x_2 + … + x_n $ $ord = np.inf$ 无穷范数：$l_\infty$ = $max( x_i )$ $ord = ‘fro’$ F 范数，即矩阵的二范数 = $\sqrt{x{11}^2+x{12}^2+… + x_{nn}^2} $ 12345678910111213141516171819In [187]: aOut[187]: array([3, 4])In [188]: np.linalg.norm(a, ord=1)Out[188]: 7.0In [189]: np.linalg.norm(a, ord=2)Out[189]: 5.0In [190]: np.linalg.norm(a)Out[190]: 5.0In [194]: np.linalg.norm(a, ord=np.inf)Out[194]: 4.0 In [191]: b = np.arange(4).reshape(2, 2)In [192]: np.linalg.norm(b, ord='fro')Out[192]: 3.7416573867739413 axis 控制范数计算的轴 12345678910In [201]: bOut[201]:array([[0, 1], [2, 3]])In [199]: np.linalg.norm(b, ord=2, axis=1) # 行上计算二范数Out[199]: array([1. , 3.60555128])In [200]: np.linalg.norm(b, ord=2, axis=0) # 列上计算二范数Out[200]: array([2. , 3.16227766]) keepdims 维持了范数计算后矩阵的维度特性 12345678910111213In [202]: np.linalg.norm(b, ord=2, axis=1, keepdims=True)Out[202]:array([[1. ], [3.60555128]])In [203]: np.linalg.norm(b, ord=2, axis=1, keepdims=True).shapeOut[203]: (2, 1)In [205]: np.linalg.norm(b, ord=2, axis=1).shapeOut[205]: (2,)In [204]: b.shapeOut[204]: (2, 2) np.split()切分数组，第二个可以切分数量或索引，axis 指定切分的轴，默认 axis=0 1234567891011121314151617181920212223242526272829303132333435363738394041424344In [211]: a = np.arange(24).reshape(4, -1)In [212]: aOut[212]:array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]])In [213]: np.split(a, 2, axis=1)Out[213]:[array([[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]]), array([[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]])]In [214]: np.split(a, 2, axis=0)Out[214]:[array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]), array([[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]])]In [215]: np.split(a, [1, 2])Out[215]:[array([[0, 1, 2, 3, 4, 5]]), array([[ 6, 7, 8, 9, 10, 11]]), array([[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]])]In [216]: np.split(a, [1, 2], axis=1)Out[216]:[array([[ 0], [ 6], [12], [18]]), array([[ 1], [ 7], [13], [19]]), array([[ 2, 3, 4, 5], [ 8, 9, 10, 11], [14, 15, 16, 17], [20, 21, 22, 23]])] np.array_split()仅对索引进行切分，参数与 np.split() 相同 12345In [217]: np.array_split(a, 2, axis=0)Out[217]:[array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]), array([[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]])] k_nearest_neighbor.pyNumpynp.argsort()返回数组升序排序后的索引 12345678910In [136]: a = np.random.randint(10, size=5)In [137]: aOut[137]: array([6, 1, 3, 8, 5])In [138]: np.argsort(a)Out[138]: array([1, 2, 4, 0, 3], dtype=int64)In [139]: np.argsort(-a) Out[139]: array([3, 0, 4, 2, 1], dtype=int64) 对于多维数组数组，axis 指定轴上的排序方式 12345678910111213141516In [147]: b = np.random.randint(10, size=6).reshape(2, 3)In [148]: bOut[148]:array([[9, 7, 8], [5, 6, 7]])In [149]: np.argsort(b, axis=0) # 列上排序Out[149]:array([[1, 1, 1], [0, 0, 0]], dtype=int64)In [150]: np.argsort(b, axis=1) # 行上排序Out[150]:array([[1, 2, 0], [0, 1, 2]], dtype=int64) np.unique()返回数组的非重复元素 12In [158]: np.unique([1, 1, 2, 2, 3, 3])Out[158]: array([1, 2, 3]) 二维数组默认返回展平后的序列，如果有 axis 参数，则以某一轴为参考 123456789101112131415161718192021In [165]: a = np.array([[1, 1, 0], [1, 1, 0], [2, 2, 4]])In [166]: aOut[166]:array([[1, 1, 0], [1, 1, 0], [2, 2, 4]])In [167]: np.unique(a)Out[167]: array([0, 1, 2, 4])In [168]: np.unique(a, axis=1)Out[168]:array([[0, 1], [0, 1], [4, 2]])In [169]: np.unique(a, axis=0)Out[169]:array([[1, 1, 0], [2, 2, 4]]) return_index 返回 unique 元素及 unique 元素第一次出现时的索引 1234567In [172]: u, idx = np.unique(a, return_index=True)In [173]: uOut[173]: array(['a', 'b', 'c'], dtype='&lt;U1')In [174]: idxOut[174]: array([0, 1, 3], dtype=int64) return_inverse 返回原数组在 unique 数组中的索引 12345678910In [175]: u, inv = np.unique(a, return_inverse=True)In [176]: uOut[176]: array(['a', 'b', 'c'], dtype='&lt;U1')In [177]: invOut[177]: array([0, 1, 1, 2, 0], dtype=int64)In [178]: u[inv]Out[178]: array(['a', 'b', 'b', 'c', 'a'], dtype='&lt;U1') return_counts 返回 unique 元素的出现次数 1234In [179]: u, cnt = np.unique(a, return_counts=True)In [180]: cntOut[180]: array([2, 2, 1], dtype=int64) np.argmax()返回最大数字的索引 12In [181]: np.argmax([1, 1, 2, 2, 3, 3])Out[181]: 4 np.broadcast_to()将数组广播到新的形状。比如 shape 为 (1, 2) 的数组只能广播到 (n, 2) 维，没办法广播到 列数不等于 2 维的数组。 123456789In [206]: aOut[206]: array([3, 4]) In [208]: np.broadcast_to(a, shape=(4, 2))Out[208]:array([[3, 4], [3, 4], [3, 4], [3, 4]])]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组的 view 与 copy]]></title>
    <url>%2Farchives%2Ffe0aad15.html</url>
    <content type="text"><![CDATA[拷贝（Copy）与视图（View）浅拷贝与深拷贝 浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。常见的浅拷贝有：列表切片操作、工厂函数、对象的 copy() 方法、copy 模块中的 copy() 函数。 深拷贝：是指创建一个新的对象，然后递归的拷贝原对象所包含的子对象。深拷贝出来的对象与原对象没有任何关联。深拷贝只有一种方式：copy 模块中的 deepcopy() 函数。 123456789101112131415161718192021222324252627282930In [18]: import copyIn [19]: a = [[1, 2], [5, 6], [8, 9], 'a', 1]In [20]: b = copy.copy(a)In [21]: c = copy.deepcopy(a)In [22]: [id(i) for i in [a, b, c]]Out[22]: [279589512, 304508616, 279730760] In [21]: for x, y in zip(a, b): ...: print(id(x), id(y)) ...: ...:296105992 296105992 # 可变对象 id 相同278258440 278258440273903240 27390324031933640 31933640 # 不可变对象 id 相同1381362800 1381362800In [22]: for x, y in zip(a, c): ...: print(id(x), id(y)) ...: ...:296105992 273960456 # 可变对象 id 不同278258440 273916616273903240 27390042431933640 31933640 # 不可变对象 id 相同1381362800 1381362800 实质上，浅拷贝和深拷贝的不同仅仅是对可变对象来说。而对于数字、字符串以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。 在深拷贝中，当两个元素指向同一个不可变对象时，对其中一个赋值不会影响另外一个，并不会影响 a 和 b 的相互独立性，所以不可变对象的 id 仍然是相同的。 视图（View）和拷贝不一样，视图在 numpy 数组中常见，视图与 array 共享数据，一些情况下，数据操作返回视图，一些情况下，返回拷贝。 Numpy 中的切片numpy.array() 的切片返回数组的视图对象，对视图对象修改会直接改变原对象。 12345678910In [8]: a = np.array([[1], 2, 3, 4])In [9]: b = a[:2]In [10]: b[0].append(999)In [11]: b[1] = 888In [12]: aOut[12]: array([list([1, 999]), 888, 3, 4], dtype=object) 数组 a 的第二个元素发生了改变。 List 中的切片列表切片返回浅拷贝，不是列表的视图对象（view），浅拷贝对于可变对象不会进行拷贝 12345678910In [3]: a = [[1], 2, 3, 4]In [4]: b = a[:2]In [5]: b[0].append(999)In [6]: b[1] = 888In [7]: aOut[7]: [[1, 999], 2, 3, 4] Numpy 中的花式索引花式索引也包括布尔索引，花式索引返回拷贝对象，而非视图。 123456789101112131415161718192021222324252627In [23]: x = np.arange(5)In [24]: xOut[24]: array([0, 1, 2, 3, 4])In [25]: copy = x[[1, 2]] # 花式索引In [26]: copy[1] = -1In [27]: copyOut[27]: array([ 1, -1])In [28]: xOut[28]: array([0, 1, 2, 3, 4])In [29]: copy_2 = x[x &gt; 2] # 布尔索引In [30]: copy_2Out[30]: array([3, 4])In [31]: x[3] = 10In [32]: copy_2Out[32]: array([3, 4])In [33]: xOut[33]: array([ 0, 1, 2, 10, 4])]]></content>
      <categories>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>View</tag>
        <tag>Copy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy 中的广播]]></title>
    <url>%2Farchives%2Fa3b21454.html</url>
    <content type="text"><![CDATA[广播指的是指的是不同形状的数组的运算方式，常常在用一个小的矩阵对大的矩阵做一些运算时用到，比如在将向量与矩阵的每一行或每一列做加减运算时，采用循环的方式效率会很慢，Numpy 的广播机制采用向量化的运算，而且在 C 上进行，速度快很多。在标量与向量的运算中，会用到广播，同时也可以利用广播的原理设置数组的值等。 实例理解广播要先搞清楚向量的维度表示 123456789101112In [102]: a = np.arange(5) In [104]: a.shape Out[104]: (5,) In [105]: a.reshape(5, 1) Out[105]: array([[0], [1], [2], [3], [4]]) 向量 a 的维度在 numpy 中默认为 (5，)，可以理解成是 1 x 5 的向量，也就是行向量，经过 reshape 后，维度变成 (5， 1)，可以理解成列向量。 下面是几个广播的实例 向量与标量这是最简单的广播例子 123456In [76]: a = np.array([1.0, 2.0, 3.0])In [77]: b = 2In [78]: a * bOut[78]: array([2., 4., 6.]) 非广播做法 123456In [79]: a = np.array([1.0, 2.0, 3.0])In [80]: b = np.array([2.0, 2.0, 2.0])In [81]: a * bOut[81]: array([2., 4., 6.]) 在概念上，我们可以理解成广播把较小的矩阵 b 拉伸成和较大的矩阵 a 相同的形状。 向量与向量12345678In [84]: a = np.array([0.0, 10.0, 20.0])In [85]: b = np.array([1.0, 2.0])In [88]: a + b.reshape(-1, 1)Out[88]:array([[ 1., 11., 21.], [ 2., 12., 22.]]) 向量与向量的广播最终会形成二维数组，相当于在较大矩阵 a 的每一行加上较小矩阵 b 的对应元素。 向量与二维数组向量与数组的广播最常见，常常需要在矩阵的每一行或每一列加减某个向量。 对矩阵的每一列分别加上某个向量 12345678In [181]: v = np.array([1, 2, 3])In [182]: x = np.array([[1,2,3], [4,5,6]])In [183]: x + vOut[183]:array([[2, 4, 6], [5, 7, 9]]) 对矩阵的每一行分别加上某一向量 12345678In [178]: v = np.array([4, 5]) In [179]: x = np.array([[1,2,3], [4,5,6]]) In [180]: (x.T + v).T Out[180]: array([[ 5, 6, 7], [ 9, 10, 11]]) 多维数组的广播多维数组的广播比较难以理解，但有一点可以记住，经过广播后，产生的数组尺寸和较大的数组尺寸相同。 123456789101112131415161718192021222324252627A (3d array): 2 x 3 x 4B (1d array): 1Result (3d array): 2 x 3 x 4 In [115]: a = np.random.randn(2, 3, 4)In [116]: aOut[116]:array([[[-0.96111778, 1.47972594, -1.15130462, 0.4305135 ], [-0.24022793, -1.06030145, -1.58573438, 0.05505269], [-0.43919454, -1.14417902, -2.91746117, -0.29782114]], [[ 0.56464212, -1.58476705, -0.44750839, -1.03800012], [ 1.73099857, 0.84889209, 0.08640296, 0.44313508], [-0.4374654 , -1.19161581, -0.44465803, -0.68662385]]])In [117]: b = 10In [118]: a + bOut[118]:array([[[ 9.03888222, 11.47972594, 8.84869538, 10.4305135 ], [ 9.75977207, 8.93969855, 8.41426562, 10.05505269], [ 9.56080546, 8.85582098, 7.08253883, 9.70217886]], [[10.56464212, 8.41523295, 9.55249161, 8.96199988], [11.73099857, 10.84889209, 10.08640296, 10.44313508], [ 9.5625346 , 8.80838419, 9.55534197, 9.31337615]]]) 123456789101112131415161718192021A (3d array): 2 x 3 x 4B (2d array): 3 x 4Result (3d array): 2 x 3 x 4 In [119]: b = np.arange(12).reshape(3, 4)In [120]: bOut[120]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])In [121]: a + bOut[121]:array([[[-0.96111778, 2.47972594, 0.84869538, 3.4305135 ], [ 3.75977207, 3.93969855, 4.41426562, 7.05505269], [ 7.56080546, 7.85582098, 7.08253883, 10.70217886]], [[ 0.56464212, -0.58476705, 1.55249161, 1.96199988], [ 5.73099857, 5.84889209, 6.08640296, 7.44313508], [ 7.5625346 , 7.80838419, 9.55534197, 10.31337615]]]) 123456789101112131415161718192021A (3d array): 2 x 3 x 4B (3d array): 2 x 1 x 4Result (3d array): 2 x 3 x 4 In [123]: b = np.arange(10, 90, 10).reshape(2, 1, 4)In [124]: bOut[124]:array([[[10, 20, 30, 40]], [[50, 60, 70, 80]]])In [125]: a + bOut[125]:array([[[ 9.03888222, 21.47972594, 28.84869538, 40.4305135 ], [ 9.75977207, 18.93969855, 28.41426562, 40.05505269], [ 9.56080546, 18.85582098, 27.08253883, 39.70217886]], [[50.56464212, 58.41523295, 69.55249161, 78.96199988], [51.73099857, 60.84889209, 70.08640296, 80.44313508], [49.5625346 , 58.80838419, 69.55534197, 79.31337615]]]) 原理 如果两个数组的后缘维度（从末尾开始算起的维度）的轴长度相符或其中一个后缘维度是 1，则认为它们是广播兼容的。 比如（2，3）的数组可以与（3，）广播，可以与（2，1）的数组广播，但不能与（2，）进行广播。 123456789101112131415161718192021222324252627282930In [106]: a = np.random.rand(2, 3)In [108]: b = np.array([1, 10])In [109]: aOut[109]:array([[0.83343121, 0.00571902, 0.41142338], [0.09910297, 0.07579308, 0.95831601]])In [110]: a + b---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-110-f96fb8f649b6&gt; in &lt;module&gt;()----&gt; 1 a + bValueError: operands could not be broadcast together with shapes (2,3) (2,)In [111]: b = np.array([1, 10, 100])In [112]: a + bOut[112]:array([[ 1.83343121, 10.00571902, 100.41142338], [ 1.09910297, 10.07579308, 100.95831601]])In [113]: b = np.array([1, 10]).reshape(2, 1)In [114]: a + bOut[114]:array([[ 1.83343121, 1.00571902, 1.41142338], [10.09910297, 10.07579308, 10.95831601]]) 广播后，数组的尺寸与较大数组的尺寸一样。当两个数组相同时，则优先考虑在行形式表示的向量上加减。 12345678910111213In [85]: a = np.array([0.0, 10.0]) In [90]: b = np.array([1.0, 2.0])In [91]: a + b.reshape(-1, 1)Out[91]:array([[ 1., 11.], [ 2., 12.]])In [92]: b + a.reshape(-1, 1)Out[92]:array([[ 1., 2.], [11., 12.]]) 在任何一个维度上，如果一个数组的轴长度为 1，另一个数组的轴长度大于 1，那么在该维度上相当于对第一个数组进行了复制。 应用np.newaxis在广播中，常常需要添加一个新轴的方法，reshape 是个办法，但是插入轴需要构造一个表示新形状的元组，Numpy 提供了一种通过全切片的索引机制以及 np.newaxis 属性插入新轴。 1234567891011121314151617181920212223242526In [127]: arr = np.zeros((4, 4))In [128]: arr_3d = arr[:, np.newaxis, :]In [129]: arr_3dOut[129]:array([[[0., 0., 0., 0.]], [[0., 0., 0., 0.]], [[0., 0., 0., 0.]], [[0., 0., 0., 0.]]])In [130]: arr_1d = np.random.randn(5)In [133]: arr_1d[:, np.newaxis] Out[133]: array([[ 0.73538673], [-0.17193569], [ 0.43638703], [ 0.00523918], [-2.04092717]]) In [134]: arr_1d[np.newaxis, :] Out[134]: array([[ 0.73538673, -0.17193569, 0.43638703, 0.00523918, -2.04092717]]) 距平化1234567891011121314151617181920212223In [135]: a = np.random.randn(3, 4)In [136]: aOut[136]:array([[ 2.48370622, -2.0992251 , 0.26653361, 2.91719966], [-0.29576162, -0.73998686, 0.17512665, -0.8897951 ], [-1.17694086, -0.45444691, 0.33272112, -0.6638607 ]])In [137]: m = a.mean(1)In [138]: m.shapeOut[138]: (3,) In [140]: a + m.reshape(-1, 1) Out[140]: array([[ 3.37575981, -1.2071715 , 1.1585872 , 3.80925325], [-0.73336585, -1.1775911 , -0.26247759, -1.32739934], [-1.66757269, -0.94507874, -0.15791071, -1.15449254]]) In [141]: dm = a - m.reshape(-1, 1) In [142]: dm.mean(1) ## 结果为 0 Out[142]: array([ 1.11022302e-16, 2.77555756e-17, -6.93889390e-17]) 如何记住在哪个 axis 应用运算？ Note the axis you apply the operation will have its dimension removed from the shape. 你所应用运算的 axis 将在 shape 属性中在移除。比如 a.shape = (3, 4)，m.shape = (3, )，因为在 axis = 1上计算了平均值。 利用广播的原理设置数组利用一维数组设置目标数组的各列 1234567891011121314In [143]: arr = np.zeros((4, 3))In [144]: arr[:] = 5In [145]: col = np.random.randn(4)In [146]: arr[:] = col[:, np.newaxis]In [147]: arrOut[147]:array([[ 0.86820161, 0.86820161, 0.86820161], [-1.58301283, -1.58301283, -1.58301283], [-0.09929726, -0.09929726, -0.09929726], [ 0.46483818, 0.46483818, 0.46483818]]) 利用一维数组设置目标数组的各行 12345678In [148]: arr[:2] = [[1], [2]]In [149]: arrOut[149]:array([[ 1. , 1. , 1. ], [ 2. , 2. , 2. ], [-0.09929726, -0.09929726, -0.09929726], [ 0.46483818, 0.46483818, 0.46483818]]) 这里注意一下数组的切片 选出（0，0），（1，1），（2，0）位置的元素 1234In [150]: a = np.array([[1,2], [3, 4], [5, 6]])In [151]: a[[0, 1, 2], [0, 1, 0]]Out[151]: array([1, 4, 5]) 选出 （0，0），（1，1）位置元素 12In [153]: a[[0, 0], [1, 1]]Out[153]: array([2, 2]) 选出行并交换列的顺序 12345678910111213141516171819In [154]: a = np.arange(32).reshape(8, 4)In [155]: aOut[155]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]])In [160]: a[[0, 1, 4, 6]][:, [1, 3, 2, 0]]Out[160]:array([[ 1, 3, 2, 0], [ 5, 7, 6, 4], [17, 19, 18, 16], [25, 27, 26, 24]]) 两个向量的广播与 np.meshgrid()1234567891011121314151617181920212223In [161]: x = np.arange(4)In [162]: y = np.arange(4, 7)In [166]: x + y.reshape(-1, 1)Out[166]:array([[4, 5, 6, 7], [5, 6, 7, 8], [6, 7, 8, 9]])In [167]: X, Y = np.meshgrid(x, y)In [168]: XOut[168]:array([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]])In [169]: YOut[169]:array([[4, 4, 4, 4], [5, 5, 5, 5], [6, 6, 6, 6]]) 虽然二者都返回 (3， 4) 的矩阵，但是两个向量的广播是在较大的数组的每一行加上较小数组的对应元素。 np.meshgrid() 返回的第一个数组是以第一个参数 x 为行，列数为 len(y) 的数组；返回的第二个数组是以第一个参数 x 为列，行数为 len(y) 的矩阵。 参考： https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html 《利用 Python 进行数据分析》]]></content>
      <categories>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Broadcasting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2Farchives%2Ff77f0066.html</url>
    <content type="text"><![CDATA[朴素贝叶斯（Naive Bayes）主要特点 前提假设是特征与特征之间是条件独立的，比如在文本分类中，在已知文章所属分类的条件下，假设单词与单词之间是相互独立，互不影响的。 朴素贝叶斯是一种生成模型，不同于回归模型这样的判别模型，利用输入变量和参数直接计算出目标变量 $y$，NB 则是利用输入变量的条件概率分布，通过贝叶斯定理获得最大后验概率的输出 $y​$。 朴素贝叶斯有个非常强的假设，特征与特征之间时条件独立的，所谓的条件就是在给定某个分类的情况，可能在一些非线性及相关性比较强的场景中表现不太好，但是在文本分类、情感分析和垃圾邮件识别中应用广泛。 贝叶斯定理贝叶斯定理如下： P(A_i|B) = \frac{P(A_i)P(B|A_i)}{\sum_jP(A_j, B)}\\ =\frac{P(A_i)P(B|A_i)}{\sum_jP(A_j)P(B|A_j)}\\分母是全概率公式，$P(A_i)$ 称为先验概率，$P(A_i|B)​$ 称为后验概率，也称条件概率。 朴素贝叶斯模型 训练数据集是由独立同分布产生的， 样本之间相互独立，所有样本符合同一个分布，或者是高斯分布，或者是伯努利分布产生的 $P(X, Y)$。 $T={(x_1,y_1 ),(x_2,y_2 ),…,(x_m,y_m )}$ 先验概率是指 y 属于某个类别的概率，由极大似然估计产生。 $P(Y=C_k), k=1, 2, …, K$ 以二项分布的极大似然估计为例，某个事件的发生概率等于该事件的发生次数除以总事件的次数。 条件概率，给定一个类别 $C_k​$，求出每个样本 $x​$ 的概率 $P(X=x|Y=C_k)$，假设 $x$ 由特征 $x_1, x_2, …, x_n​$ 构成，类比一篇文章由很多个单词构成。 我们想要的结果是 $P(Y=C_k|X=x)​$，给定一个样本，求样本属于某个分类的概率变得可行。 P(Y=C_k|X=x) = \frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)} 问题在于 $P(X=x|Y=C_k)​$ 是个十分复杂的分布，所以又做出了一个假设，在给出 $C_k​$ 时，$x_1,x_2,x_3…​$ 之间是相互独立的。 P(X=x|Y=C_k)\\ = P(X_1=x_1, X_2=x_2, ..., X_n=x_n|Y=C_k)\\ =P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)...P(X_n=x_n|Y=C_k)​ 到此，朴素贝叶斯做出的假设 样本 $x​$ 之间是独立同分布的，类比很多篇文章是独立同分布的 样本的特征之间是条件独立的，类似文章中的各个单词是条件独立的 朴素贝叶斯算法给定输入 $x$，得到后验概率 $P(Y=C_k|X=x)$ 最大，也就是属于哪个分类的概率最大。 P(Y=C_k|X=x) \\= \frac{P(X=x|Y=C_k)P(Y=C_k)}{\sum_{j}P(X=x|Y=C_j)P(Y=C_j)}\\ =\frac{\prod_i P(X_i=x_i|Y=C_k)P(Y=C_k)}{\sum_j\prod_i P(X_i=x_i|Y=C_j)P(Y=C_j)}对于每个样本来说，分母是全概率公式，都是相同的，目的也就变成求 $argmax_{C_k} P(Y=C_k)\prod_i^nP(X_i=x_i|Y=C_k)P(Y=C_k)$ 朴素贝叶斯的参数估计—-极大似然估计 $P(Y=Ck) = \frac{\sum{i=1}^n I(y_i=C_k)}{N}$ 设第 $i​$ 个特征 $xi​$可能取值的集合为 ${a{1},a{2},…a{j} }​$，条件概率的极大似然估计是 P(X_i=x_i|Y=C_k) = \frac{\sum_j^nI(x_i=a_j, y_i=C_k)}{\sum_{i=1}^NI(y_i=C_k)} 注意： 连续特征可能需要离散化。 连乘的可能会导致小数溢出，可能取 log 后再相加。 针对计算出的每个分类概率需要做归一化，使最终得到的概率加和为 1。 贝叶斯估计当某个特征 $x_i​$ 属于每个分类的概率为 0 时，可能会影响到后验概率的计算结果。 这时，需要采用贝叶斯估计的方法，我们假设先验概率也是满足某种分布的，这种分布同后验概率分布形成共轭分布，最终去求期望。比如二项分布的共轭分布是 Beta 分布，多项分布的共轭分布是 Dirichlet 分布。 先验概率的贝叶斯估计为 P_\lambda(Y=C_k) = \frac{\sum_{i=1}^nI(y_i=C_k)+\lambda}{N + K\lambda}$K​$ 是所有的分类数量。 条件概率的贝叶斯估计为 P(X_i=x_i|Y=C_k) = \frac{\sum_j^nI(x_i=a_j, y_i=C_k) + \lambda}{\sum_{i=1}^NI(y_i=C_k)+S_j \lambda}$S_j$ 指特征 $x_i$ 所有可能取值。 $\lambda=1$ 时称为拉普拉斯平滑；$\lambda=0$ 时，称为 Lidstone 平滑；$\lambda=0$ 时，则是极大似然估计。]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2Farchives%2Ff77f0066.html</url>
    <content type="text"><![CDATA[朴素贝叶斯（Naive Bayes）主要特点 前提假设是特征与特征之间是条件独立的，比如在文本分类中，在已知文章所属分类的条件下，假设单词与单词之间是相互独立，互不影响的。 朴素贝叶斯是一种生成模型，不同于回归模型这样的判别模型，利用输入变量和参数直接计算出目标变量 $y$，NB 则是利用输入变量的条件概率分布，通过贝叶斯定理获得最大后验概率的输出 $y$。]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[熵和决策树]]></title>
    <url>%2Farchives%2F2a0ca566.html</url>
    <content type="text"><![CDATA[本章节的重点是决策树部分，而理解熵的概念，不仅对于决策树有更好的认识，而且卷积神经网络中也经常使用交叉熵做为损失函数。 熵熵的直观理解假设我们要定义一个新的物理量，叫事物的不确定性。经验上讲，事情发生的概率越小，事物的不确定性越大，带来的信息量越大，与概率 $p(x)$ 呈反比；第二点，假设 x 和 y 相互独立，$p(x, y) = p(x)p(y)$，我们希望 x 和 y 的不确定性是可以累加的，$h(x, y) = h(x) + h(y)$，而 log 函数恰好可以满足这种累加性，于是现在有了一个新的度量 $-lnp(x)$；最后我们希望能够获得平均信息量，也就是求 $-lnp(x)$ 的数学期望：$H[x] = -\sum_{x} p(x)ln p(x)$，这就是熵的概念，代表了随机事件 X 的信息量。 总结来说，log 函数使熵变得可加，负号保证与经验相符，最终得到的是数学期望。 熵的定义熵度量了事物的不确定性，熵越大，事物越不确定。 H(X) = -\sum_{i=1}^np_ilogp_i两点分布的信息熵 $p\in [0, 1]$ 因为数学上 $ln 0​$ 是没有意义的，保证代码上的严格性，加上 eps。 1234eps = 1e-4p = np.linspace(eps, 1 - eps, 100)h = -(1 - p) * np.log2(1 - p) - p * np.log2(p)plt.plot(p, h, 'r-', lw=3) 在 $p=0.5​$ 处的熵最大，意味着这个随机事件不确定程度最大。 在 $p = 0$ 和 $p = 1$ 时，熵为 0，随机事件退化成必然事件。 均匀分布的信息熵假设离散分布取 N 个值，每个值的概率都是 $1/N​$，$p_i = 1/N​$。这个概率分布的熵就是 联合熵 H(X, Y) = -\sum_{i=1}^np(x_i, y_i)logp(x_i, y_i)指事件 X, Y 同时发生的不确定性。 条件熵条件熵 $H(Y|X)​$ 指在 $X​$ 发生的前提下，$Y​$ 的不确定性。 H(X, Y) 指 X 和 Y 总的不确定度。如果 X 和 Y 独立，H(X, Y) = H(X) + H(Y)，值变成两个圆的加和。 \begin{align} H(Y|X) =& H(X, Y) - H(X) \\ =& -\sum_{i=1}^np(x_i, y_i)logp(y_i|x_i)\\ =& \sum_{j=1}^np(x_j)H(Y|x_j) \end{align}证明如下： $p(x) = \Sigma_yp(x, y)$，边缘概率等于所有的联合概率加和。 $p(y|x) = \frac{p(x, y)}{p(x)}$，条件概率公式。 上面得到的公式好像并不对称，并没有得到 $-\Sigma_{x, y}p(y|x)logp(y|) = H(Y|X)​$ 的形式，这个式子继续推导… 也就是说，我们要的 $H(Y|X) = E_{p(x)}[H(Y|X=x)]$。给定 $X=x$，$H(Y|X=x)$ 在 $p(x)$ 下的期望。 交叉熵与相对熵交叉熵$H(p, q) = -\Sigma_x p(x)log q(x)$，我们假设 $p(x)$ 和 $q(x)$ 分别代表训练样本和模型预测的 label 分布。$H(p, q)$ 的意义是根据预测结果来衡量训练样本的不确定性（熵）。 相对熵用 $H(p, q)$ 减去训练样本的熵值 $H(p)$ 得到两个熵之间的差值，称为相对熵 $D(p||q)$，是两个随机分布间距离的度量，用来评估这两个分布的差异性。 相对熵又称 KL 散度，设 p(x)、q(x) 是两个概率分布，则 p 对 q 的相对熵是： D(p||q) = H(p, q) - H(p)\\ = -\sum_{x} p(x)logq(x) + \sum_{x} p(x)logp(x) \\ = \sum_{x} p(x)log\frac{p(x)}{q(x)} =E_{p(x)}log\frac{p(x)}{q(x)}相对熵一定是大于等于 0 的，可以用 Jensen 不等式证明。也可以这样理解，用预测样本 q(x) 衡量 p(x) 的不确定性是要大于 p(x) 本身的不确定性的，不确定性又可以用熵度量，$H(p, q) &gt;= H(p)$，当且仅当 p 和 q 两个分布相等时，等号成立。 因此，相对熵可以衡量两个随机变量的“距离”，也可以作为损失函数。 互信息互信息 $I(X, Y)​$ 也叫信息增益。两个随机变量 X，Y 的互信息，定义为 X，Y 的联合分布和独立分布乘积的相对熵。 I(X, Y) = D(p(x, y)||p(x)p(y))\\ = \sum_{x, y}log\frac{p(x, y)}{p(x)p(y)}在上图的韦恩图中，互信息可以表示为 $I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$ 。 推导如下： I(X, Y) = H(X) - H(X|Y) \\ = -\sum p(x)logp(x) + \sum_{x, y}p(x, y)logp(y|x)\\ = -\sum_{x, y}p(x, y)log p(x) + \sum_{x, y}p(x, y)log \frac{p(x, y)}{p(y)}\\ =\sum_{x, y}log\frac{p(x, y)}{p(x)p(y)}互信息的意义：衡量了在给定 $Y$ 时，X 的不确定性减少的程度，或者说在给定 $X$ 时，Y 的不确定性减少的程度。 如果某个特征的信息增益越大，说明不确定性减少的越大，该特征越适合作为分裂点。 联合熵、条件熵和互信息的关系 H(Y|X) = H(X, Y) - H(X)\\ H(Y|X) = H(Y) - I(X, Y)\\ I(X, Y) = H(X) + H(Y) - H(X, Y)决策树直观理解用于分类或回归的无参数的监督学习方法。 如上图，如果有 4 个特征，m 个样本，训练集的矩阵是 $m\times 4$ 的，如果是二叉决策树，决策树的深度是 n，最多有 $2^n$ 个叶节点。 对于离散数据，对每个叶子节点的样本个数计数，采用少数服从多数的原则，选择类别最多的类别作为这个叶子节点的类别，便可以做分类；对于连续数据，则经常选择平均值或中位数作为这个叶子节点的预测值。既能做分类又能做回归的决策树又叫 CART（Classification and Regression Tree）树。 多棵决策树形成的结果则是随机森林，比如上图中，对于 boy 的预测，上图是将两个预测值相加的结果。 随机森林由多棵决策树构成，而构建一棵决策树的重点是如何选择分裂点对当前样本进行划分？划分的指标如何选择？ 下图是利用决策树对鸢尾花数据进行划分的结果。 主要包括三个步骤特征选择、决策树的生成和决策树的剪枝。剪枝为了防止过拟合的发生。 ID3选择信息增益最大的特征作为分裂点。 信息增益 I(D, A) = H(D) - H(D|A)$I(D, A)$ 代表特征 A 对于数据集 D 的信息增益，H(D) 代表集合 D 的熵，H(D|A) 代表给定特征 A 时，数据集 D 的条件熵。 H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$|D|​$ 代表数据集 D 的样本数目，$C_k​$ 代表第 k 个分类下的样本数目 H(D|A) = -\sum_{i=1}^nP(A_i)H(D|A_i)\\ = -\sum_{i=1}^nP(A_i)\sum_{k=1}^KP(D_k|A_i)logP(D_k|A_i)\\ =-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$|D_i|$ 代表特征 A 将数据集 D 划分成 $D_1$、$D_2$ … $D_n​$ 的样本个数 $D_{ik}$ 代表第 k 个分类下，样本 $D_i$ 的样本个数。 当叶子结点数量较少或已经做到完全分类（熵为 0）时，便不再划分。 特殊情况 如果某个特征被选为当前的分裂点，但是在现存数据中只有一个值，另一个值对应的记录为空，则这个时候针对不存在的特征值，将它标记为该特征在所有训练数据中所占比例的最大类型（李烨 Gitchat 课程）。 问题 不能处理连续特征 容易偏向取值较多的特征，极端情况下，将 ID 作为一个特征时，形成多叉树，信息增益大，容易首先取到，但是没有意义。 过拟合 对特征缺失值比较敏感 C4. 5信息增益比利用信息增益比克服将信息增益作为标准时，容易偏向取值较多的特征的问题。 $I_R(D, A) = \frac{I(D, A)}{H_A(D)}​$ $H_A(D)$ 指特征 A 的熵 H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}特殊情况当 $H_A(D)​$ 趋向于 0 时，信息增益比 $I_R(D, A)​$ 趋向于 无穷，也就是说 $H_A(D)​$ 的分类本身已经足够好，所以熵值接近于 0。为避免这种情况发生，可以采用启发式的思路，先计算特征对应的信息增益，在信息增益较高的情况下，才应用信息增益率作为分裂标准。 问题 生成多叉树 只能用于分类 使用熵模型，涉及大量运算，很耗时 CART最主流的决策树算法，既能生成分类树，又能生成回归树； 处理连续值时，与 C4.5 类似，不同的是使用基尼系数作为特征选择指标； 对于离散和连续特征，CART 构建的是二叉树而不是多叉树。 基尼系数$Gini(p) = \sum_{k=1}^K p_k(1-p_k)$，$p_k$ 为第 k 个类别的概率。 基尼系数代表模型的不纯度，基尼系数越小，纯度越高，特征越好。 对于样本 D，基尼系数的计算 Gini(p) = \sum_{k=1}^K p_k(1-p_k)\\ =1 - \sum_{k=1}^Kp_k^2\\ =1 - \sum_{k=1}^K (\frac{|C_k|}{|D|})^2对于给定特征 A 和样本 D，特征 A 将样本分成 $D_1, D_2, …D_n​$ $Gini(D, A) = \sum_{i=1}^n \frac{|D_i|}{|D|}Gini(D_i)​$ 基尼系数与信息熵的近似信息熵的公式 $H(X) = -\sum{i=1}^np_ilogp_i = \sum{i=1}^np_i(-logp_i)$ 基尼系数的公式 $Gini(p) = \sum_{k=1}^K p_k(1-p_k)$ 也就是说，如果我们用 $1-p​$ 去近似 $-lnp​$，基尼系数就变成了熵的公式，在图像上的表示为用红线去近似蓝色线。 而且，$-lnx​$ 在 $x = 1​$ 处的一阶泰勒展开恰恰是 $1-x​$。所以，基尼系数也可以作为样本划分的依据。 二分类的熵和基尼系数$Gini(p) = 2p(1-p)​$ 123456789eps = 1e-4p = np.linspace(eps, 1 - eps, 100)x = np.sin(eps)h = -(1 - p) * np.log2(1 - p) - p * np.log2(p)gini = 2 * (1 - p) * p # 为了美观，gini 系数除以 2plt.plot(p, gini, 'r-', lw=3, label='Gine')plt.plot(p, h / 2, 'g-', lw=3, label='Entropy')plt.legend()plt.show() 基尼系数第二定义衡量贫富差距的指标。 横轴为人口累计百分比（按财富值的最小值到最大值排序），纵轴为该部分人的收入占全部人口总收入的百分比，红色线段表示人口收入分配处于绝对平均状态，而橘色曲线就是劳伦茨曲线，表现的是实际的收入分配情况（李烨 Gitchat 课程）。 Gini 系数实际上就是 $\frac{A}{A+B}​$ 的比值，面积越大，财富分配越不均衡。 在决策树中，应该选择使节点基尼系数最小的特征作为分裂点，也说明划分后的节点分布越不平均，越可以用来做分类。这里的基尼系数和统计意义的基尼系数表达的意思恰好相反。 二叉树比如特征 A 有 $a_1, a_2, a_3$ 三个特征值，分别计算每个特征值对数据集 D 的基尼系数，比如计算特征值 $a_1$ 基尼系数时，会将 D 划分成 $D_1$ 和 非 $D_1$ 子集进行计算。选择基尼系数最小的特征（也就是选择父节点基尼系数差值最大，类比信息增益）及对应的特征值，建立左右两个节点，形成二叉树。 回归树与分类树的主要区别： 连续值的处理方式不同 决策树建立后的预测方式不同 连续特征离散化的方法比如特征 A 有 m 个样本，排序后 $a1, a_2, a_3, … a_m​$，取相邻两个样本的均值，一共 m-1 个划分点，其中第 i 个划分点使用 $T_i=(a_i+a{(i+1)})/2​$ 表示。分别计算这 m-1 个点的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。 还可以通过给定采样间隔或随机取值的方法使连续特征离散化。 评价方式对于任意划分点 s，划分成集合 $D_1​$ 和 $D_2​$，使 $D_1​$ 和 $D_2​$ 的均方差最小，同时 $D_1​$ 和 $D_2​$ 的均方差之和最小的特征和特征值作为划分点 min_{A,s}[min_{c_1}\sum_{x_i\in D_1}(y_i - c_1)^2 + min_{c_2}\sum_{x_i\in D_2}(y_i - c_2)^2]$c_1​$ 是数据集 $D_1​$ 的输出均值，$c_2​$ 是数据集 $D_2​$ 的输出均值，$y_i​$ 代表实际值 预测结果分类树中，某一个叶节点的预测结果采用投票的方式进行决定； 回归树中，叶节点的均值或中位数作为输出结果。 决策树的过拟合问题只要不存在矛盾样本（相同的 X 对应不同的 Y ）的情况下，决策树总可以使训练集上叶子节点的样本类别是相同的。 随着树的深度增大，训练集上的错误率逐渐降低，但是在测试集上错误率增加。决策树深度等于 3 时，在测试集上的表现最好，实际操作时，采用交叉验证的方式来进行。 预剪枝在树的生成过程中，当某个节点满足剪枝条件，比如样本数量过少、深度过深、或在验证集上不能带来准确率的提升时，停止分支的划分。 根据交叉验证（gridsearchcv）的方式选择最优的超参数。 后剪枝先生成决策树，再根据在验证集上的准确性决定是否剪掉这个节点。 实际情况做的更多是预剪枝，而非后剪枝。（详细 CCP 算法参考《百面机器学习》中如何对决策树进行剪枝） 随机森林另外一种防止过拟合的方式是做随机森林。 对缺失值的处理处理缺失值时，常常是对特征中的非缺失值数据计算相对熵后，对相对熵乘以（非缺失数据/总样本）权重比例。]]></content>
      <categories>
        <category>机器学习</category>
        <category>决策树</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树类模型]]></title>
    <url>%2Farchives%2F2a0ca566.html</url>
    <content type="text"><![CDATA[决策树用于分类或回归的无参数的监督学习方法 主要包括三个步骤特征选择、决策树的生成和决策树的剪枝 剪枝为了防止过拟合 熵 熵度量了事物的不确定性，熵越大，事物越不确定 H(X) = -\sum_{i=1}^np_ilogp_i 两个变量的联合熵 H(X, Y) = -\sum_{i=1}^np(x_i, y_i)logp(x_i, y_i) 条件熵 $H(Y|X)$ 指在 $X$ 发生的前提下，$Y$ 的熵（不确定性） \begin{align} H(Y|X) =& H(X, Y) - H(X) \\ =& -\sum_{i=1}^np(x_i, y_i)logp(y_i|x_i)\\ =& \sum_{j=1}^np(x_j)H(Y|x_j) \end{align} 信息增益（互信息） $I(X, Y)$ $I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$ 衡量了在给定 $Y$ 时，X 的不确定性减少的程度，或者说在给定 $X$ 时，Y 的不确定性减少的程度。 如果某个特征的信息增益越大，说明不确定性减少的越大，该特征越适合作为分裂点。 联合熵、条件熵和互信息的关系 H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)\\ H(Y) = H(Y|X) + I(X, Y) H(X) = H(X|Y) + I(X, Y) ID3信息增益 I(D, A) = H(D) - H(D|A)$I(D, A)$ 代表特征 A 对于数据集 D 的信息增益，H(D) 代表集合 D 的经验熵，H(D|A) 代表给定特征 A 时，数据集 D 的经验条件熵。 H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$|D|$ 代表数据集 D 的样本数目，$C_k$ 代表第 k 个分类下的样本数目 H(D|A) = -\sum_{i=1}^nP(A_i)\sum_{k=1}^KP(D_k|A_i)logP(D_k|A_i)\\ =-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$|D_i|$ 代表特征 A 将数据集 D 划分成 $D_1$、$D_2$ … $D_n$ 的样本个数 $D_{ik}$ 代表第 k 个分类下，样本 $D_i$ 的样本个数 问题 不能处理连续特征 容易偏向取值较多的特征，极端情况下，将 ID 作为一个特征时，信息增益大，容易首先取到，但是没有意义。 过拟合 C4. 5信息增益比利用信息增益比克服将信息增益作为标准时，容易偏向取值较多的特征的问题 $I_R(D, A) = \frac{I(D, A)}{H_A(D)}$ $H_A(D)$ 指特征 A 的熵 H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}连续特征离散化的方法比如特征 A 有 m 个样本，排序后 $a1, a_2, a_3, … a_m$，取相邻两个样本的均值，一共 m-1 个划分点，其中第 i 个划分点使用 $T_i=(a_i+a{(i+1)})/2$ 表示。分别计算这 m-1 个点的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。 还可以通过给定采样间隔或随机取值的方法使连续特征离散化。 问题 生成多叉树 只能用于分类 使用熵模型，涉及大量运算，很耗时 CART最主流的决策树算法，既能生成分类树，又能生成回归树； 处理连续值时，与 C4.5 类似，不同的是使用基尼系数作为特征选择指标； 对于离散特征，CART 构建的是二叉树而不是多叉树。 基尼系数$Gini(p) = \sum_{k=1}^K p_k(1-p_k)$，$p_k$ 为第 k 个类别的概率。 基尼系数代表模型的不纯度，基尼系数越小，不纯度越低，特征越好。 对于样本 D，基尼系数的计算 Gini(p) = \sum_{k=1}^K p_k(1-p_k)\\ =1 - \sum_{k=1}^Kp_k^2\\ =1 - \sum_{k=1}^K (\frac{|C_k|}{|D|})^2对于给定特征 A 和样本 D，特征 A 将样本分成 $D_1, D_2, …D_n$ $Gini(D, A) = \sum_{i=1}^n \frac{|D_i|}{|D|}Gini(D_i)$ 二叉树比如特征 A 有 $a_1, a_2, a_3$ 三个特征值，分别计算每个特征值对数据集 D 的基尼系数，比如计算特征值 $a_1$ 基尼系数时，会将 D 划分成 $D_1$ 和 非 $D_1$ 子集进行计算。选择基尼系数最小的特征及对应的特征值，建立左右两个节点，形成二叉树。 回归树与分类树的主要区别： 连续值的处理方式不同 决策树建立后的预测方式不同 评价方式对于任意划分点 s，划分成集合 $D_1$ 和 $D_2$，使 $D_1$ 和 $D_2$ 的均方差最小，同时 $D_1$ 和 $D_2$ 的均方差之和最小的特征和特征值作为划分点 min_{A,s}[min_{c_1}\sum_{x_i\in D_1}(y_i - c_1)^2 + min_{c_2}\sum_{x_i\in D_2}(y_i - c_2)^2]$c_1$ 是数据集 $D_1$ 的输出均值，$c_2$ 是数据集 $D_2$ 的输出均值，$y_i$ 代表实际值 预测结果分类树中，某一个叶节点的预测结果采用投票的方式进行决定； 回归树中，叶节点的均值或中位数作为输出结果。 集成算法组合多个基础模型的预测值，提高模型的泛化能力。 主要有两个方法得到最终的预测值： Averaging：独立的建立多个模型对预测结果做平均或投票，又叫 Bagging。 Boosting：有序列的，依赖多个基模型，用后一个模型修正前一个模型的 bias。 Bagging集成规则有平均或少数服从多数的原则。 每个基分类器的数据集是随机抽取得到的，通过引入随机性来减少方差。 Boostrap 采样方法： 有放回的随机采样，每次采集一个样本，然后放回，一轮随机采样的次数与样本集数目 m 相同，当 $m \rightarrow \infty$ 时，一轮随机采样中，样本不被采集到的概率是 \lim_{m\rightarrow \infty}(1 - \frac{1}{m})^m = \frac{1}{e} \approx 0.368这 36.8% 的数据作为 Out of Bag (OOB)，可以作为测试集，验证模型的泛化能力。这部分数据是没有被模型学到的。Bagging 不需要划分训练数据和测试数据。 Bagging 对弱分类器没有限制，LR，DT，KNN，朴素贝叶斯，SVM 等基础模型都可以作为基本分类器。 随机森林属于 Bagging 的一种模型，基分类器是 CART 决策树。 假设训练集有 m 个样本，一次决策树共做 m 次有放回采样，得到 m 个样本的采样集； 每建一棵树时，特征是随机选择的，选择最优特征作为分裂点进行左右划分； 共做 T 次决策，如果是分类决策，采用投票最多的样本类别；如果是回归算法，则取 T 个弱分类器结果的平均值作为输出。 每建一棵树时，都可以计算一下 OOB Score，如果共做 T 次决策，则可以计算这 T 次 OOB Score 的平均值作为在测试集上的表现。 AdaboostBoosting：下一个分类器的表现会依赖于上一次分类的结果。 Adaboost，是一种比较经典的 boosting 思路，像 GBDT，XGboost，LGB 是基于梯度的 boosting 思路。 主要思路同时给予分类器和每个样本权重，假设开始时每个样本的权重相同，每轮预测结束后，预测错误的样本权重上升，预测正确的样本权重下降。下一轮预测基于上一轮中预测错误的样本来进行，也就是权重变高的那些样本。最终的结果基于分类器的权重系数得到的。 假设共有 m 个样本，迭代次数 T 次，第一次预测初始化每个样本的权值 $D^1(i)$ 分布为 $\frac{1}{m}$ for t=1, …, T do 拿一个基本分类器做一次预测，比如 SVM，DT，LR 等做一次二分类 计算这个分类器的误差，也就是预测错误的部分，估计值 $f_t(X_i)$ 不等于 $y_i$ 的加和。 误差率的计算将权值 $D^t(i)$ 也考虑进去，第二步公式等同于下式，$I$ 是指示函数。 \epsilon_t = \sum_{i=1}^m \omega_{ti}I\{f_t(X_i)\neq y_i\} 计算系数 $\alpha_t$，这个系数会被认为是分类器 $f_t(X)$ 的权重，同时 $\alpha_t$ 也会用来计算下一次样本 $i$ 的权重。 更新每个样本的权重 $D^{(t+1)}(i)$，如果预测错误，$y_if_t(X_i) = -1$，$e^{-\alpha_t y_if_t(X_i)}$ 将大于 1，下一次该样本的权重将变大；同理，如果预测正确，$e^{-\alpha_t y_if_t(X_i)}$ 将小于 1，下一次预测该样本权重将变小；归一化因子 $Z^{(t)}$ 保证权重在变化后的加和仍然是 1，满足一个概率分布。 最终的分类器将是 T 个基本分类器 $f_t(X)$ 加上权重 $\alpha_t$ 加和的结果。]]></content>
      <categories>
        <category>机器学习</category>
        <category>树模型</category>
      </categories>
      <tags>
        <tag>DT</tag>
        <tag>Adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归与逻辑回归]]></title>
    <url>%2Farchives%2F68e9aa20.html</url>
    <content type="text"><![CDATA[要点线性回归模型的用途，代价函数，从概率的角度解释代价函数以及梯度下降的应用。 线性回归的概率解释解释为什么线性回归中使用最小二乘法作为代价函数可以确保找到合适的解。 假设样本共有 n 个特征，那么第 i 个样本的线性回归模型可以写成 $y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$ 其中 $\theta = (\theta_0, \theta_1, … \theta_n)^T$，$x^{(i)} = (1, x^{(i)}_1, x^{(i)}_2, … x^{(i)}_n,)$，$\epsilon^{(i)}$ 称为误差项或随机噪声，代表模型中不能解释的部分。 误差项的假设独立同分布（IID）样本与样本间的误差项是相互独立的，且每个样本的误差项均符合同一个分布。 在 Regression 模型中，假设误差项服从高斯分布； 在 Classification 模型中，像 Logistic Regression 这种二分类模型中，假设误差项服从伯努利分布的 概率分布假设误差项服从 $\epsilon^{(i)}$~ $N(0, \sigma^2)$ 的正态分布 P(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(\epsilon^{(i)})^2}{2\sigma^2}}因为 $y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$，所以 $y^{(i)}$~$N(y^{(i)}-\theta^T x^{(i)}, \sigma^2) $ 的高斯分布 P(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}}$y^{(i)}$ 在给定 $x^{(i)}$ 和 $\theta$ 时是服从独立同分布的，似然函数可以写成 \begin{align} L(\theta) =& L(\theta; x, \vec{y}) = P(\vec{y}; X;\theta)\\ =& \prod_{i=1}^m P(y^{(i)}|x^{(i)};\theta)\\ =& \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}} \end{align}极大似然估计（MLE）MLE 目的是求出一个 $\theta$ 使 $L(\theta)$出现的概率最大，也就是对 $L(\theta)$求极大值。针对连乘的形式，为方便求导，常常取对数，转换成连加的形式。因为 Log 函数并不改变函数的单调性。 \begin{align} l(\theta) =& log(L(\theta)) = \sum_{i=1}^m log \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}}\\ =& mlog\frac{1}{\sqrt{2\pi}\sigma} + (-\frac{1}{\sigma^2} \frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2)\\ & max(l(\theta))\rightarrow min(\frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2) \end{align}要使 $l(\theta)$ 最大化，也就是使 $min(-l(\theta))$，去掉常数项和系数项，使损失函数 $\frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2$ 最小化，$y^{(i)}$ 是实际值，$\theta^T x^{(i)}$ 是估计值，二者差的平方和最小化也就是上面的代价函数最小化，这也是最小二乘法的来源。 参数 $\theta$ 的求解并不依赖于方差 $\sigma^2$，因此在数据集上也具有普适性。 梯度下降对代价函数 $\frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2)$求梯度 批量梯度下降表示为 \theta_j := \theta_j - \alpha \sum_{i=1}^m(\theta^T x^{(i)}- y^{(i)})x^{(i)}_j随机梯度下降表示为 \theta_j := \theta_j - \alpha (\theta^T x^{(i)} - y^{(i)})x^{(i)}_j上标 $i$ 代表第 $i$ 个样本，下标 $j$ 代表第 $j$ 个特征 重点Linear Regression 的公式形式，代价函数（最小二乘法的表示），梯度下降（对代价函数求导）的形式 Logistic 回归的梯度下降Logistic Regression 公式形式 h_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^Tx}}转化成 Sigmoid 函数的表示为 $g(z) = \frac{1}{1+e^{-z}}$，函数值的取值范围 (0, 1)，也就是一个概率值，$\theta^T x$ 是线性回归的表示形式。 \begin{align} g'(z) =& \frac{1}{(1+e^{-z})^2}(-(e^{-z}(-1)))\\ =& \frac{1}{(1+e^{-z})^2}e^{-z}\\ =& \frac{1}{1+e^{-z}}\frac{e^{-z}}{1+e^{-z}}\\ =& g(z)(1-g(z)) \end{align}极大似然估计Logistic Regression 表示的是二分类模型，返回的是属于某一类的概率，因此可以表示为 $P(y=1|x; \theta) = h_{\theta}(x)$ $P(y=0|x; \theta) = 1 - h_{\theta}(x)$ 上述两个式子可以写成一个式子 P(y|x; \theta) = h_{\theta}(x)^y + (1-h_{\theta}x)^{(1-y)}由独立同分布可以得出似然函数 L(\theta) = P(\vec{y}|X;\theta) = \prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)\\ =\prod_{i=1}^m h_{\theta}(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}取对数后 l(\theta) = \sum_{i=1}^m y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))用梯度下降的方法求 $\theta​$，建议面试时直接将这个推导结果背下来，而不是慢慢的一步步推出来，太浪费时间了。 对 $l(\theta)​$ 求和公式展开，只有当 $i = j​$ 时，导数不为 0 \begin{align} \frac{\partial}{\partial \theta_j}l(\theta) =& (y\frac{1}{h_{\theta}x} - (1-y)\frac{1}{1-h_{\theta}{x}})\frac{\partial}{\partial\theta_j}(h_\theta x) \\ =& (y\frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)})\frac{\partial}{\theta_j}g(\theta^Tx) \\ =& (y\frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)}) g(\theta^Tx)(1 - g(\theta^Tx))\frac{\partial}{\theta_j}(\theta^T x) \\ =& (y\frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)}) g(\theta^Tx)(1 - g(\theta^Tx))x_j \\ =& (y(1-g(\theta^T x)) - (1-y)(g(\theta^T x )))x_j \\ =& (y - g(\theta^T x))x_j \end{align}Logistic Regression 的似然函数梯度便是真实值 $y​$ 减去预测值 $g(\theta^T x)​$ 再乘以对应的特征 $x_j​$。 Logistic Regression 的损失函数也就是对对数似然取负号 损失函数梯度下降的表示形式是 \theta:= \theta - \alpha (g(\theta^T x) - y)x_j随机梯度下降（单个样本的梯度下降） \theta_j := \theta_j - \alpha(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j上标 $i$ 代表第 $i$ 个样本，下标 $j$ 代表第 $j$ 个特征 Logistic Regression 与线性回归的梯度的不同在于 $h_\theta(x^{(i)})$ 的形式不同 Linear Regression: $h_\theta(x^{(i)}) = \theta^T x^{(i)} $ Logistic Regression: $h_\theta(x^{(i)}) = g(\theta^T x^{(i)}) = \frac{1}{1+e^{-\theta^Tx^{(i)}}}​$ 为什么选用交叉熵损失而不是平方损失函数？交叉熵损失函数由数据服从伯努利分布推导而来，求导方便，而且是凸函数。 而如果将平方损失函数应用到 logistic 回归中，那么损失函数将是非凸的。 https://blog.csdn.net/v1_vivian/article/details/69396405 决策边界根据 $h_{\theta}(x)​$ 是否大于 0.5，很容易推导出逻辑回归的决策边界是 $\theta^T x = 0​$。 如果有两个特征，很容易画出决策边界的直线 如何利用 log-sum-exp 函数解决 logistic 回归中上溢出或下溢出问题？https://lingpipe-blog.com/2012/02/16/howprevent-overflow-underflow-logistic-regression/ 参考文章 https://www.jianshu.com/p/c5ba12a1b2c8]]></content>
      <categories>
        <category>机器学习</category>
        <category>线性回归</category>
      </categories>
      <tags>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anychart export to PNG JPG SVG]]></title>
    <url>%2Farchives%2F4985cd57.html</url>
    <content type="text"><![CDATA[最近喜欢上利用 Anycharts 进行科研作图，图表丰富，配色舒服，利用 JS 实现交互，展示效果很好。虽然需要在 ss 上开启全局模式才能访问其网站。 但是，最近在科研作图时，一直因为不会直接导出矢量图格式，在工作中浪费了很多时间去统一调整字号，字体和位置等。自己之前一直通过 Chrome 的打印功能获取 PDF，但有些时候处理这种方式导出的 PDF 并不方便。某天在 Anychart 官网中找到了其导出图片的方法。 要运行 Anychart Export Server，需要本地开启 Web 服务器，利用 PhantomJS、Chrome 或 Firefox 模拟服务器上的浏览器，运行图表，获取 SVG，PNG，JPG 等。服务器仅仅充当中间人，允许使用浏览器保存文件，具体步骤如下： 准备工作 从PhantomJS 官方网站下载 PhantomJS，确保 PhantomJS 二进制文件在 PATH 中。 或者，安装 Firefox 浏览器（版本 56.0 及更高版本）并安装 geckodriver 用于 Mac 用户的 brew install geckodriver 或者从geckodriver 官方网站下载并将其添加到 PATH。 或者，安装 Chrome 或 Chromium 浏览器（版本 60.0 及更高版本）并安装chromedriver 用于 Mac 用户的brew install chromedriver 或者从chromedriver 官方网站下载并将其添加到 PATH。 安装 Java，并将其添加至 PATH。 下载 AnyChart Export Server 二进制文件，格式为 jar 文件，export-server.jar。 服务器模式 运行服务器 java -jar export-server.jar server 主要参数参考官方网站 在作图 JS 代码中添加 1anychart.exports.server("http://localhost:2000"); 将 chart 改为全局变量，在 JS 代码中添加 1234567891011121314151617181920212223// save the chart as pngfunction png() &#123; chart.saveAsPng(&#123;"width": 360, "height": 500, "quality": 0.3, "filename": "My Chart PNG"&#125;);&#125;;// save the chart as svgfunction svg() &#123; chart.saveAsSvg(&#123;"paperSize": "A4", "landscape": false, "filename": "My Chart SVG"&#125;);&#125;;// save the chart as jpgfunction jpg() &#123; chart.saveAsJpg(&#123;"width": 360, "height": 500, "quality": 0.3, "forceTransparentWhite": false, "filename": "My Chart JPG"&#125;);&#125;; 在 body 中添加 123&lt;button onclick="png()"&gt;PNG&lt;/button&gt;&lt;button onclick="svg()"&gt;SVG&lt;/button&gt;&lt;button onclick="jpg()"&gt;JPG&lt;/button&gt; 打开 html 文件，点击相应的 button 即可将图片下载到本地。 注意 html 文件中尽量不要出现中文，否则会因为编码问题生成 svg 出错。 官方实例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;!doctype html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta content="IE=edge" http-equiv="X-UA-Compatible"&gt; &lt;meta content="width=device-width, initial-scale=1" name="viewport"&gt; &lt;title&gt;Exports 01&lt;/title&gt; &lt;link href="https://playground.anychart.com/docs/samples/Exports_01/iframe" rel="canonical"&gt; &lt;meta content="Bar Chart,Bar Graph,Column Chart,Vertical Chart" name="keywords"&gt; &lt;meta content="AnyChart - JavaScript Charts designed to be embedded and integrated" name="description"&gt; &lt;!--[if lt IE 9]&gt;&lt;script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"&gt;&lt;/script&gt;&lt;script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"&gt;&lt;/script&gt;&lt;![endif]--&gt; &lt;link href="https://cdn.anychart.com/releases/8.3.0/css/anychart-ui.min.css" rel="stylesheet" type="text/css"&gt; &lt;style&gt;html, body &#123; width: 100%; height: 100%; margin: 0; padding: 0;&#125;button &#123; margin: 10px 0 0 10px;&#125;#container &#123; position: absolute; width: 100%; top: 35px; bottom: 0;&#125;&lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;button onclick="png()"&gt;PNG&lt;/button&gt; &lt;button onclick="svg()"&gt;SVG&lt;/button&gt; &lt;button onclick="jpg()"&gt;JPG&lt;/button&gt; &lt;div id="container"&gt;&lt;/div&gt; &lt;script src="https://cdn.anychart.com/releases/8.3.0/js/anychart-base.min.js"&gt;&lt;/script&gt; &lt;script src="https://cdn.anychart.com/releases/8.3.0/js/anychart-exports.min.js"&gt;&lt;/script&gt; &lt;script src="https://cdn.anychart.com/releases/8.3.0/js/anychart-ui.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; // 添加服务器 anychart.exports.server("http://localhost:2000"); anychart.onDocumentReady(function () &#123; // data var data = [ ["Chocolate paste", 5], ["White honey", 2], ["Strawberry jam", 2], ["Condensed milk", 1] ]; // set chart type chart = anychart.bar(data); // set chart title chart.title("Save as Image Buttons"); // display chart chart.container("container").draw();&#125;);// save the chart as pngfunction png() &#123; chart.saveAsPng(&#123;"width": 360, "height": 500, "quality": 0.3, "filename": "My Chart PNG"&#125;);&#125;;// save the chart as svgfunction svg() &#123; chart.saveAsSvg(&#123;"paperSize": "A4", "landscape": false, "filename": "My Chart SVG"&#125;);&#125;;// save the chart as jpgfunction jpg() &#123; chart.saveAsJpg(&#123;"width": 360, "height": 500, "quality": 0.3, "forceTransparentWhite": false, "filename": "My Chart JPG"&#125;);&#125;;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 参考来源 https://docs.anychart.com/Common_Settings/Exports]]></content>
      <categories>
        <category>可视化</category>
        <category>Anychart</category>
      </categories>
      <tags>
        <tag>anychart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-编程导论（第-2-版）读书笔记-9-10-章（搜索与排序）]]></title>
    <url>%2Farchives%2Fa5f4c049.html</url>
    <content type="text"><![CDATA[9~14 章主要讨论了 一些简单的算法，比如搜索、排序（选择排序和归并排序）、贪心算法（背包问题）、图论（DFS 和 BFS）、动态规划等。 chapter 9本章主要内容是如何用大 O 表示法估计程序的复杂度。 思考计算复杂度时间复杂度我们常使用程序执行的基本步骤来度量算法的效率。一般情况下，考虑三种情形： 最佳情形：给定输入规模时的最短运行时间。 对于线性搜索，最佳情形的运行时间与列表长度无关，只要保证待查找元素位于列表第一个位置，只需要一步即可返回结果。 最差情形：给定输入规模时的最长运行时间。 我们通常最关注最差情形，它给出了运行时间的上界。在大多数时间内满足的条件通常不令人满意。 平均情形：给定输入规模时的平均运行时间。 如果输入值的分布有先验信息，这种先验信息也应考虑在内。比如快速排序中，如果基准值的选择随机时，快排平均情形下的时间复杂度便是 $O(log_n)$，最差情形是 $O(n^2)$。 渐进表示法在比较不同算法时，常常忽略加法中的常数和乘法中的常数。渐进输入法则是用来描述输入规模无穷大时的算法复杂度。 如果运行时间是多项式，保留增长速度最快的项 如果保留项是个乘积，去掉常数 常用大 O 表示法，暗示渐进最差情形运行时间的上界，代表运行时间的增速。 重要的复杂度常用的复杂度有：$O(1)$、$O(log_n)$、$O(n)$、$O(nlog_n)$、$O(n^k)$、$O(c^n)$。 常数复杂度 $O(1)$复杂度与输入规模无关，但并不意味着没有循环或递归调用，只是循环或递归与输入规模无关。 对数复杂度运行时间是某个数的对数，但是这个对数与底数无关，无非是通过换底公式将原来底数的对数乘以常数。 举个栗子，$执行时间（步数） = log_x^n$，$x^{执行时间（步数）} = n$，也就是底数的时间复杂度次幂等于 $n$，在二分查找中，执行 $2^{时间复杂度}$ 次达到 $high - low$。 常见的对数复杂度有二分查找，及下面将整数数字转换成字符串函数 12345678910111213In [3]: def int_to_str(i): ...: digits = '0123456789' ...: if i == 0: ...: return '0' ...: result = '' ...: while i &gt; 0: ...: result = digits[i % 10] + result ...: i = i // 10 ...: return result ...:In [4]: int_to_str(456)Out[4]: '456' 线性复杂度对序列中每个元素都进行常数次处理，并非只有循环语句具有线性复杂度，递归也有可能具有线性复杂度，比如 下面的阶乘函数 123456789In [5]: def factorial(x): ...: if x == 1: ...: return 1 ...: else: ...: return x * factorial(x - 1) ...:In [6]: factorial(5)Out[6]: 120 上述函数每次递归调用都会将内存空间分配一个新的栈帧，这个栈帧会一直占用内存，直至调用返回。所以这里的空间复杂度也是 $O(n)$。 多数情况下，线性复杂度的算法是完全可以接受的。 对数线性复杂度归并排序，堆排序，快速排序的平均情况的复杂度都是 $O(log_n)$。 多项式复杂度常见的是平方复杂度，比如对两个列表进行嵌套的遍历时，可能涉及平方复杂度。 指数复杂度所需时间随着输入规模呈指数增长。对于指数复杂度问题，可以使用某种算法找出近似解，或对于这些问题中的某些特殊实例，可以求出最优解。 像集合覆盖问题中，比如每个广播台都覆盖特定的区域，不同广播台的覆盖区域可能重叠。如何找出覆盖全美 50 个州的最小广播台集合呢？如果有 n 个广播台，每个广播台都有选或不选两种选择，列出每种可能便是类似下面代码展示的幂集问题，复杂度为 $O(2^n)$。 12345678910111213141516171819202122232425262728293031In [9]: def get_binary_rep(n, array_lenhth): ...: # result 为 0 和 1 组成的字符串 ...: result = '' ...: while n &gt; 0: ...: result = str(n % 2) + result ...: n //= 2 ...: # 用 0 在前面补齐至 array_length ...: for i in range(array_lenhth - len(result)): ...: result = '0' + result ...: return result ...:In [10]: def gen_power_set(L): ...: powerset = [] ...: for i in range(2 ** len(L)): ...: bin_str = get_binary_rep(i, len(L)) ...: subset = [] ...: ...: for i in range(len(bin_str)): ...: if bin_str[i] == '1': ...: subset.append(L[i]) ...: ...: powerset.append(subset) ...: ...: return powerset ...:In [11]: L = [1, 2, 3]In [12]: gen_power_set(L)Out[12]: [[], [3], [2], [2, 3], [1], [1, 3], [1, 2], [1, 2, 3]] NP 问题我们把时间复杂度为非多项量级的算法问题称为 NP 问题，像 $O(2^n)$ 和 $O(n!) $ 等。 像 NP 完全问题，必须考虑所有组合的情况，不能将问题分解成子问题，比如集合覆盖问题（$O(2^n)$）和旅行商问题（$O(n!)​$），随着元素数量增加，运行时间会变得非常慢，这类问题的最佳算法是使用近似算法。 chapter 10程序效率的关键是好的算法，而不是靠小聪明在编码时耍花招。 在面对具体问题时，我们需要： 理解问题的内在复杂度 思考如何将问题分解为多个子问题 将子问题与已有高效算法的其他问题联系起来 搜索算法线性搜索复杂度至多与列表长度 L 成线性关系。 Python 中的列表由表示列表长度的整数及固定长度的对象指针序列构成，下图的列表包含 4 个元素，最左侧方块则存储了指向整数的指针。因为列表中目标元素的引用（指针）是连续存储，这也使得访问列表中的元素可以在常数时间内完成。支持随机访问也使得数组区别于链表、栈和对列。 二分搜索假设是有序排列的，不论是列表有序还是数值固有的有序性，都可以采用二分查找的方法进行搜索。可以显著改善最差情形下的复杂度。在每次调用时，high - low的值都被减半，直至达到 high = low 。复杂度为 $O(log_n)$。 递归12345678910111213141516171819202122232425262728In [13]: def bsearch(arr, item): ...: if len(arr) == 0: ...: return None ...: else: ...: return binary_search(arr, item, 0, len(arr) - 1) ...:In [14]: def binary_search(arr, item, low, high): ...: mid = (low + high) // 2 ...: if low == high: ...: if arr[low] == item: ...: return mid ...: else: ...: return arr[low] == item ...: if arr[mid] == item: ...: return mid ...: elif arr[mid] &gt; item: ...: if low == mid: # nothing left to search ...: return False ...: return binary_search(arr, item, low, mid - 1) ...: else: ...: return binary_search(arr, item, mid + 1, high) ...:In [15]: arr = [2, 4, 5, 7, 8, 9, 13, 23, 34, 45]In [16]: bsearch(arr, 22)Out[16]: False 循环12345678910111213141516171819In [20]: def binary_search(list, item): ...: low = 0 ...: high = len(list) - 1 ...: while low &lt;= high: ...: mid = int((low + high) / 2) ...: guess = list[mid] ...: if guess == item: ...: return guess ...: if guess &gt; item: ...: high = mid - 1 ...: else: ...: low = mid + 1 ...: return None ...:In [21]: arr = [2, 4, 5, 7, 8, 9, 13, 23, 34, 45]In [22]: binary_search(arr, 9)Out[22]: 9 使用 mid-1 而非 mid 的原因 当被查找元素不在列表中时，low 和 high 永远不会相遇，会无限循环下去 优势k 次查找乱序列表时，常常先对列表进行排序，然后采用 k 次二分查找的方式，而非直接进行 k 次线性搜索。 常用的排序算法复杂度为 $O(nlog_n)$，当 k 越来越大时，$O(nlog_n) + k \times O(log_n)$ 远小于 $k \times O(n)$。 牛顿-拉弗森算法 每次更新值 $x$ 的坐标是穿过点 $(x_0, f(x_0))$ 并且斜率为 $f’(x_0)$ 的直线与 $x$ 轴的交点。也就是如下方程的解： $0 = (x - x_0)f’(x_0) + f(x_0)$ $x_{n+1} = x_n - \frac{f(x_n)}{f’(x_n)}$ 下降速度快。 排序算法选择排序原理是维持循环不变式，将列表分成前缀部分（L[0: i]）和后缀部分（L[i+1: len(L)]），前缀部分已经排好序。开始时前缀部分是空列表，后缀部分是整个列表；结束时前缀部分是整个列表，后缀部分是空的。复杂度为 $O(n^2)$。 12345678910111213141516171819In [1]: def sel_sort(arr): ...: for i in range(len(arr)): ...: for j in range(i, len(arr)): ...: if arr[i] &lt; arr[j]: ...: arr[i], arr[j] = arr[j], arr[i] ...: return arr ...:In [2]: arr = np.random.randint(30, size=(20))In [3]: arrOut[3]:array([ 6, 27, 29, 25, 26, 26, 7, 28, 27, 25, 0, 3, 1, 26, 28, 0, 25, 15, 3, 25])In [4]: sel_sort(arr)Out[4]:array([29, 28, 28, 27, 27, 26, 26, 26, 25, 25, 25, 25, 15, 7, 6, 3, 3, 1, 0, 0]) 归并排序基本思想利用分治思想，先找到初始问题的简单实例的解，再将这些解组合起来作为初始问题的解。 列表长度为 0 或 1 时，认为已经排好序了 如果多于 1 个元素，将其分成两个列表，利用归并排序法排序 合并结果 普通归并排序前提是两个有序的列表，比较两个列表中的第一个元素，将较小的值追加至目标列表。当其中一个列表为空时，将另一个列表中的剩余元素追加至目标列表。 每一层 需要merge 的元素总数都是 len(L)，时间复杂度为 $O(n)$，共有 $log_2^{len(L)}$ 层，时间复杂度为 $O(nlog_n)$。 普通归并排序需要复制列表，空间复杂度为 $O(n)$。 1234567891011121314151617181920212223242526272829303132333435363738In [20]: def merge(left, right): ...: # left, right 都是已排序好的 ...: results = [] ...: i, j = 0, 0 ...: while i &lt; len(left) and j &lt; len(right): # 等号保证了排序的稳定性 ...: if left[i] &lt;= right[j]: ...: results.append(left[i]) ...: i += 1 ...: else: ...: results.append(right[j]) ...: j += 1 ...: ...: # 排序结束后, 把未完成的排序加进去 ...: while i &lt; len(left): ...: results.append(left[i]) ...: i += 1 ...: ...: while j &lt; len(right): ...: results.append(right[j]) ...: j += 1 ...: return results ...:In [21]: def merge_sort(arr): ...: if len(arr) &lt; 2: ...: return arr ...: else: ...: mid = len(arr) // 2 ...: left = merge_sort(arr[:mid]) ...: right = merge_sort(arr[mid:]) ...: return merge(left, right) ...:In [22]: arr = [4, 5, 3, 2, 1, 7]In [23]: merge_sort(arr)Out[23]: [1, 2, 3, 4, 5, 7] 原地归并排序利用手摇算法可以实现原地归并排序，空间复杂度为 $O(1)$，但同时牺牲了时间复杂度。 手摇算法 可以通过三次逆序交换的手段实现从 ABCDEF 到 CDEFAB BACDEF（反转后一段） BAFEDC（反转前一段） CDEFAB（反转整体） 原地归并排序 开始时，i = 0, j = mid + 1，使 i 前进； 当 L[i] &gt; L[j] 时，使 j 前进，直至 L[j] &lt;L[i]； 对 L[i: index] 和 L[index: j+1] 利用手摇算法进行交换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051In [8]: def reverse(A, start, end): ...: """ ...: 将长度为 n 的数组逆序 ...: """ ...: while start &lt; end: ...: A[start], A[end - 1] = A[end - 1], A[start] ...: start += 1 ...: end -= 1 ...: ...: ...: def exchange(A, index, i, j): ...: """ ...: 三次逆序 ...: """ ...: reverse(A, i, index) ...: reverse(A, index, j) ...: reverse(A, i, j) ...: ...: ...: def merge(A, begin, mid, end): ...: # print(begin, mid, end, A) ...: i, j, k = begin, mid, end ...: while i &lt; j &lt;= k: ...: step = 0 ...: index = j ...: while i &lt; j and A[i] &lt;= A[j]: ...: i += 1 ...: while j &lt;= k and A[j] &lt; A[i]: ...: j += 1 ...: step += 1 ...: exchange(A, index, i, j) ...: ...: ...: def merge_sort(A, l, r): ...: if l &lt; r: ...: mid = (l + r) // 2 ...: merge_sort(A, l, mid) ...: merge_sort(A, mid + 1, r) ...: merge(A, l, mid + 1, r) ...:In [9]: def test(): ...: import numpy as np ...: for i in range(100): ...: A = np.random.randint(0, 100, size=100) ...: merge_sort(A, 0, len(A) - 1) # 检查是否已排序好 ...: assert (np.diff(A) &gt;= 0).all() ...:In [10]: test() # 没有引起 AssertionError 扩展—函数作为实参在普通归并排序中，可以自定义排序规则，只需要将表示排序规则的函数作为实参传入 merge 函数，在比较时调用这个函数即可。可以应用按不同规则对字符串进行排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253In [15]: def merge(left, right, compare): ...: i, j = 0, 0 ...: result = [] ...: ...: while i &lt; len(left) and j &lt; len(right): ...: if compare(left[i], right[j]): ...: result.append(left[i]) ...: i += 1 ...: else: ...: result.append(right[j]) ...: j += 1 ...: ...: while i &lt; len(left): ...: result.append(left[i]) ...: i += 1 ...: ...: while j &lt; len(right): ...: result.append(right[j]) ...: j += 1 ...: ...: return result ...: ...: ...: def merge_sort(arr, compare): ...: if len(arr) &lt; 2: ...: return arr[:] ...: ...: else: ...: mid = len(arr) // 2 ...: left = merge_sort(arr[:mid], compare) ...: right = merge_sort(arr[mid:], compare) ...: return merge(left, right, compare) ...: ...: ...: def lastname_firstname(name1, name2): ...: spl_n1 = name1.split() ...: spl_n2 = name2.split() ...: ...: if spl_n1[1] == spl_n2[1]: ...: return spl_n1[0] &lt; spl_n2[0] ...: else: ...: return spl_n1[1] &lt; spl_n2[1] ...:In [16]: left = [2, 3, 5, 1, 4, 7]In [17]: merge_sort(left, compare=lambda x, y: x &lt; y)Out[17]: [1, 2, 3, 4, 5, 7]In [18]: s = ['Chris Terman', 'Tom Brady', 'Eric Grimson']In [19]: merge_sort(s, lastname_firstname) # 先对 lastname 排序，随后对 firstname 排序Out[19]: ['Tom Brady', 'Eric Grimson', 'Chris Terman'] 快速排序分治思想 选择一个基准值 将数组划分成两个子数组，小于基准值的元素和大于基准值的元素 对这两个子数组进行快速排序 我们认为如果是空数组或只含一个元素的数组是已经排序好的数组。 下面这种快排的方法的空间复杂度是 $O(n)$，并非原地排序。 12345678910111213141516In [10]: def quicksort(arr): ...: if len(arr) &lt; 2: ...: return arr ...: else: ...: less, greater = [], [] ...: pivot = arr[0] ...: for i in arr[1:]: ...: if i &lt; pivot: ...: less.append(i) ...: else: ...: greater.append(i) ...: return quicksort(less) + [pivot] + quicksort(greater) ...: In [21]: quicksort([10, 5, 12, 2, 5])Out[21]: [2, 5, 10, 12] 快速排序（小兵移动）选择序列最左侧的数作为基准值，从右边寻找小于基准值的数，从左边寻找大于基准值的数 因为基准值在左侧，必须是右边的哨兵先向左移动。当找到比基准值小的数时，暂停移动。这时左边的哨兵向右移动，当找到比基准值大的数时，暂停移动。交换数据。 当左右两边的哨兵相遇时，这一次的探测结束，我们将基数与它们相遇的数据交换位置 这时，6 左边的数据都是小于 6，右边的数据都是大于 6。快排的目的也是使小于基准值的位于基准值左边，大于基准值的位于基准值右边。随后用同样的方法再对基准值 6 左侧和右侧的序列分别进行快排。 1234567891011121314151617181920def quick_sort(arr, start, end): if start &gt;= end: return low = start high = end mid = arr[start] while low &lt; high: while low &lt; high and arr[high] &gt; mid: high -= 1 while low &lt; high and arr[low] &lt;= mid: low += 1 # 小兵相遇交换位置 arr[high], arr[low] = arr[low], arr[high] # 交换相遇位置与基准值 arr[start], arr[low] = arr[low], arr[start] # 对基准值左侧的序列快排 quick_sort(arr, start, low - 1) # 对基准值右侧的序列快排 quick_sort(arr, low + 1, end) 快速排序（小兵移动）优化三数取中法优化快速排序：排序时，不选择最左侧的值作为枢纽值，而是取序列中最左侧元素，中间元素和最右侧元素中的中间值作为枢纽值以优化快速排序。 1234567891011121314151617181920212223242526272829303132333435def sorted_short(arr, low, high): """ 对三值进行排序, 并把枢纽值放到倒数第二位 """ mid = (low + high) // 2 if arr[low] &gt; arr[high]: arr[low], arr[high] = arr[high], arr[low] if arr[low] &gt; arr[mid]: arr[low], arr[mid] = arr[mid], arr[low] if arr[mid] &gt; arr[high]: arr[high], arr[mid] = arr[mid], arr[high] arr[high - 1], arr[mid] = arr[mid], arr[high - 1]def quick_sort(arr, start, end): if start &gt;= end: return low, high = start, end if low &lt; high: sorted_short(arr, low, high) pivot = arr[high - 1] while low &lt; high: # 参考值在右侧, 先从左侧开始判断 while low &lt; high and arr[low] &lt; pivot: low += 1 while low &lt; high and arr[high] &gt;= pivot: high -= 1 arr[high], arr[low] = arr[low], arr[high] arr[low], arr[end - 1] = arr[end - 1], arr[low] quick_sort(arr, start, low - 1) quick_sort(arr, low + 1, end) C 语言标准库中的函数 qsort 使用的是快速排序，而 Python 中的 sort() 与 sorted() 函数使用的是 timesort，利用很多数据集中数据已部分有序的事实，最差情形下的性能与归并排序一样，平均性能远超归并排序。 下图是不同排序方法的比较 散列表散列表可以由数组和散列函数构成，散列函数指出在数组中的存储位置，数组则用来存储值。下面代码中取余操作 key % self.num_buckets 可以看作一个散列函数。 散列函数特性 散列函数必须是一致的。对于同样的输入，每次都映射到相同的索引，比如取余操作。 散列表要知道数组的大小，只返回有效索引，而非返回超过数组大小的索引。 最理想情况下，对于不同的输入，散列表映射到不同的索引。如果两个键映射到同一位置，那么在这个位置上存储一个链表，这种情况称为冲突（碰撞）。 好的散列函数很重要，会使键均匀的分布在散列表的不同位置，而不是使散列表中的链表变得很长，在查找时十分低效。 填装因子是评价散列表的另一指标。由于散列表使用数组存储数据，数组中有些位置填充上数据，有些位置是空的，填装因子等于散列表中的元素数/位置总数。 通常情况，一旦填装因子大于 0.7 时，需要调整散列表的长度，需要创建一个更长的数组。 由于散列函数直接映射到数组索引上，理想情况下，散列表的查询，插入和删除复杂度都是 $O(1)$。但是，最糟情况下，所有的键都映射到同一链表中，使得查询，插入和删除的复杂度变成 $O(n)​$。 12345678910111213141516171819202122232425262728293031323334class IntDict(object): def __init__(self, num_buckets): self.buckets = [] self.num_buckets = num_buckets for i in range(num_buckets): self.buckets.append([]) def add_entry(self, key, dict_val): hash_bucket = self.buckets[key % self.num_buckets] for i in range(len(hash_bucket)): # 如果 key 已经存在，则更新值 if hash_bucket[i][0] == key: hash_bucket[i] = (key, dict_val) return hash_bucket.append((key, dict_val)) def get_value(self, key): hash_bucket = self.buckets[self.num_buckets % key] for i in range(len(hash_bucket)): if hash_bucket[i][0] == key: return hash_bucket[i][1] return None def __str__(self): result = '&#123;' for b in self.buckets: for e in b: result += str(e[0]) + ':' + str(e[1]) + ',' return result[:-1] + '&#125;' 参考文章 https://blog.csdn.net/ltyqljhwcm/article/details/52155097 https://www.cnblogs.com/Yogurshine/archive/2013/05/02/3054329.html https://blog.csdn.net/yangquanhui1991/article/details/52070777]]></content>
      <categories>
        <category>Books</category>
        <category>python 编程导论</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3Blue1Brown-微积分的本质读书笔记]]></title>
    <url>%2Farchives%2F7830505c.html</url>
    <content type="text"><![CDATA[微积分的本质微积分的三个中心思想：微分，积分和微分积分互逆。 将半径为 $R$ 的圆剪成同心圆环，并计算每个圆环的面积后，相加可得。圆环半径为 $r$，厚度为 $dr$，展开后面积等于 $2\pi rdr$。 $dr$ 越小，矩形越多，近似越准确 $0\le r\le R$ $\pi R^2$ 的由来，即三角形的面积 原因$dr$ 不仅是 $2\pi rdr$ 中的因子，更是不同 $r$ 值的间隔，且 $dr$ 越小，矩形面积加和的近似越精确。 核心圆环面积的和是对原问题的近似，等价于求图像的面积。如果是不规则图形，用积分的思想。 微积分基本定理将曲线下面积的变化看作关于 $x$ 的函数 $A(x)$$\frac{dA}{dx}\approx x^2$ 当 $dx$ 越来越小时，根据导数的概念，$\frac{dA}{dx} = f(x)$，$A(x)$ 的导数等于 $f(x)$。 导数衡量的是函数对取值的微小变化有多敏感。 导数的悖论速度的定义：单位时间内运动的距离。 矛盾我们想在某个时间点上关联一个速度值，但计算速度却需要比较两个时间点上的距离。现实中，并不是计算瞬时变化率，而是非常小的一段时间 $dt$ 内的距离的变化 $ds$，$ds/dt$ 即为 $t$ 时刻速度。 导数的意义$\frac{ds}{dt}$ 是关于时间 $t$ 的函数，也就把速度看作关于时间的函数。当 $dt$ 趋近于 0 时，$t$ 时刻的导数逼近于 $t$ 时刻图像切线的斜率。 $dt$ 并不是趋近于无穷小，$dt$ 也不是 0，只是非常接近 0。 并非瞬时变化率，而是某一点附近的变化率的最佳近似，即该点处函数的斜率。 导数的求法将 $t+dt$ 和 $t$ 代入 $s(t)$函数中, 求解 $\lim_{dt \to 0}\frac{(t+dt)^3 - {t}^3}{dt}$,将右侧展开，利用 $dt$ 趋近于0，即得导数值。 导数不是来强调瞬时变化的，比如，$t=0$ 的速度仅仅是近似于 0。 用几何来求导导数本质是微小变化量！ 正方形面积正方形面积表示为 $f(x) = x^2$$x$ 代表边长，$f(x)$ 代表面积$dx$ 表示微小的增量，$df$ 表示面积的增量${dx}^2$ 可以小到忽略不计 $\frac{df}{dx} = 2x$ 立方体体积立方体体积表示 $f(x) = x^3$ 当增加 $dx$ 时，面积的增量 $df = 3x^2dx + 3x dx^2 + dx^3$ $dx​$ 趋向于 0 时，$\frac{df}{dx}​$ 只剩下 $3x^2​$ 项 $x^n$ 的导数意义将 $f(x) = x^n$ 展开，得到如下公式。 ${(x+dx)}^n - {x}^n$ 的增量除以 $dx$ 后只保留了 $nx^{n-1}$ $\frac{1}{x}$ 的导数意义$f(x) = \frac{1}{x}$ 的图像如下，矩形面积等于 $x * f(x) = 1$ 为使面积为 1，横坐标 $x$ 变化时，纵坐标相应的变化为 $\frac{1}{x}$当 $x$ 增加 $dx$ 时，为使面积不变，把高度的变化量记为 $\frac{1}{x}$$\frac{d(\frac{1}{x})}{dx} = \frac{\frac{1}{x+dx} - \frac{1}{x}}{dx} = -\frac{1}{x^2}$ $\frac{1}{\sqrt{x}}$ 的导数意义$f(x) = \frac{1}{\sqrt x}$令 $\sqrt x = t$$df = \frac{2\sqrt tdt}{d\sqrt{t}}$$\frac{d\sqrt{t}}{dt} = \frac{1}{2\sqrt t}$ $sin\theta$ 的导数意义$f(\theta) = sin\theta$$\frac{d (sin\theta)}{d \theta} = $邻边/斜边 = $cos \theta$ 链式法则和乘积包含函数相加，相乘和复合函数 加法法则$f(x) = sin(x) + x^2$$df = d(sin(x)) + d(x^2)$$\frac{df}{dx} = cos(x) + 2x$ $f(0.5)$ 的变化等于 $sin(0.5)$ 的变化加上 $0.5^2$ 的变化 乘法法则$f(x) = sin(x)x^2$用盒子面积 $sin(x) * x^2$ 定义乘积 $f(x)$ 当 $x$ 增加 $dx$ 时，$sin(x)$ 增加 $d(sin(x))$，$x^2$ 增加 $d(x^2)$ $d(x^2)d(sin(x))$ 面积忽略不计 总结：左乘右导 + 右乘左导 复合函数$f(x) = sin(x^2)$$h(x) = x^2$$d(sin(h)) = cos(h)dh= cos(x^2)d(x^2) = cos(x^2)2xdx$ 从上到下三条轴分别表示 $x$，$x^2$，$sin(x^2)$ 第三条轴的变化量 $d(sin(h)) = cos(h)dh$，只与中间变量 $h$ 有关。 第二条轴的变化量 $d(h)$，只与变量 $x$ 有关。 链式法则:$\frac{d}{dx}g(h(x)) = \frac{dg}{dh}\frac{dh}{dx}$ 指数函数求导$M(t) = 2^t$$\frac{dM}{dt}(t) = \frac{2^{t+dt}-2^t}{dt} = 2^t\frac{2^{dt}-1}{dt}$当 $t$ 代表离散变量天数时，导数等于其本身。当 $dt$ 趋近于 0 时，$\frac{2^{dt}-1}{dt}$ 趋近于一个常数，这使得导数 $\frac{dM}{dt}(t)$ 与其自身 $2^t$ 成正比例，这个常数便是 $ln2$。 关于 e 的定义把右侧分式 $\frac{2^{dt}-1}{dt}$ 的底数 2 换成 e 变成 $\frac{e^{dt}-1}{dt}$ 我们定义 $\lim_{dt \to 0}\frac{e^{dt}-1}{dt}=1$，就好像把圆的周长比上直径等于 $\pi$ 一样 所以 $e^t$ 的导数 $\frac{d(e^t)}{dt} = e^t * 1$ $2^t$ 的导数$\frac{d(e^{ct})}{dt} = ce^{ct}$$2^t = e^{ln(2)t}$ 利用复合函数求导 $\frac{d(2^t)}{dt} = \frac{de^{ln(2)t}}{dt} = ln(2)e^{ln(2)t} = ln(2)2^t$ 指数函数常写成 $e^{ct}$形式 积分与微积分基本定理已知 $v(t)$ 求 $S(t)$已知速度时间函数 $v(t) = t(8-t)$，将行使过的距离用函数图像下面积表示。之前求路程时，近似成车在做匀速运动。但这次速度会不连续地“跳跃”。 每个间隔称为 $dt$，$\int$ 代表每个小矩形的加和，跳跃间隔 $dt$ 逐渐趋近于 0。$\int_0^8v(t)dt$ 表示 $dt \to 0$ 时，加和的趋近值就是曲线下的面积。 $s(T) = \int_0^Tv(t)dt$我们将曲线下的面积看作是关于 $T$ 的函数。那么，面积函数 $s$ 的导数 $\frac{ds}{dT}$ 等于 $T$ 时刻的速度 $v(T)$。 所以，任意函数图像下方面积的导数等于原先函数的本身。距离函数 $s(T)$ 即 $v(T)$ 关于 $T$ 的积分。 关于积分的常数项 C常数 $C$ 的理解：上下移动距离函数 $s(T)$ 的图像并不影响时刻 $T$ 处的导数。定积分的常数项 $C$ 会被消掉。 微积分基本定理意义积分可理解为极限条件下，一定范围内的小矩形面积和。 即将 $x$ 在一定范围内将 $f(x) \times dx$ 的值加起来，求 $dx \to 0 $ 时，加和趋近的值就是 $f(x)$ 在这个范围上积分。$\frac{dF}{dx}(x) = f(x)$$\int_a^bf(x)dx = F(b) - F(a)$ 负面积图像如果出现在横轴下方定义的面积为负数。 泰勒级数泰勒级数可以理解为是一个强大的函数近似工具，在某个点附近用多项式函数去近似其他函数。只要这个函数是可导的。 多项式近似为保证多项式 $p(x) = c_0 + c_1x + c_2x^2$ 与 $cos(x)$ 足够近似。分别令原函数，一阶导，二阶导（弯曲程度）在 $0$ 处值均相等，也就是求解 $cos(0) = p(0) = c_0 = 1$ $-sin(0) = p’(0) = c_1 + c_2 \times 0 = 0$ $-cos(0) = p’’(0) = 2c_2 = -1$ 分别得到 $c_0, c_1, c_2$，也就定义了 $p(x) = 1-\frac{1}{2}x^2$ 依次类推，可以在多项式中得到 $c_3x^3, c_4x^4 …$ 项 要点 阶乘 $n!$ 的形式实际上是多项式 $x^n$ 求导法则一层套一层的结果比如求 $x^4$ 前面的系数时，分母是阶乘形式 $4\times3\times2\times1$，分子是在 $x=0$ 处的高阶导数，比如 $cos(0) = 1$ 当 $x = 0$ 时，添加高次项时，低次项的系数不会发生改变，每一阶系数都是固定且唯一的。 当 $p(x) = c_0 + c_1x + c_2x^2$ 时，$p’(x) = c_1 + 2c_2x$，当 $x = 0$ 时，$2c_2x$ 并不影响 $c_1$ 的值。 在 $x = x_0$ 处，写成 $p(x)= c_0 + c_1(x-x_0)^1 + c_2(x-x_0)^2 + c_3(x-x_0)^3 + c_4(x-x_0)^4$。代入 $x=x_0$，求导后可以消掉多项式除目标项外的所有部分。 本质本质上，泰勒级数将某一点处的导数值信息转换成这一点附近的函数值信息。在任一点 $a$ 处，可以调整多项式函数在 $a$ 处近似成原始函数 $P(x)$，并使得多项式的高阶导数与原始函数的高阶导数在 $a$ 处相等。 通用的写法 $f(x)$ 在 $x = 0$ 处的泰勒级数可以表示成 $P(x) = f(0) + f’(0)\frac{x^1}{1!} + f’’(0)\frac{x^2}{2!} + f’’’(0)\frac{x^3}{3!} + …$ $f(x)$ 在 $x = a$ 处的泰勒级数可以表示成 $P(x) = f(a) + f’(a)\frac{(x-a)^1}{1!} + f’’(a)\frac{(x-a)^2}{2!} + f’’’(a)\frac{(x-a)^3}{3!} + … + f^n(a)\frac{(x-a)^n}{n!} + R_n(x)$， 皮亚诺余项 $R_n(x)$ 代表误差。 几何解释泰勒多项式的二次项 已知面积函数 $f_{area}$ 在 $a$ 附近的导数，要近似在 $x$ 处的面积 $f(x)$横坐标的变化量 $(x-a)$ 黄色矩形面积等于 $\frac{df}{dx}(a)(x-a)$（微积分基本定理 $df$ = 矩形的高 $\times dx$）粉红色三角形高等于图像斜率 $\times (x-a)$，图像斜率等于面积函数 $f$ 二阶导 $\frac{d^2f}{dx^2}(a)$，三角形面积等于 $\frac{1}{2}\frac{d^2f}{dx^2}(a)(x-a)^2$，也就是泰勒多项式的二次项。 泰勒级数级数收敛：累加无限项和后，收敛到某一个值收敛半径：原始函数近似点周围能让多项式的和收敛的最大取值范围 总结泰勒公式实际用多项式函数 $p(x)$ 近似原函数在某一点附近的函数值，利用的是在该点处多项式和原函数的高阶导数相等。阶乘是多项式求高阶导的结果，$(x-a)^n$ 使得在 $x = a$ 处求高阶项系数时，低阶项和更高阶项不受影响，仅与目标项有关。]]></content>
      <categories>
        <category>数学</category>
        <category>微积分</category>
      </categories>
      <tags>
        <tag>微积分</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 编程导论（第 2 版）读书笔记 1~8 章]]></title>
    <url>%2Farchives%2F578b18fe.html</url>
    <content type="text"><![CDATA[本书目的是思考和解决计算相关的问题。与这个目标无关的不会涉及，整本书也是围绕这个核心展开。 chapter 2 变量Python 中变量仅仅是名称，一个对象可以关联多个名称。万物皆对象，当对象没有引用计数时，内存释放。 交换变量位置x, y = y, x 在排序中大量用到。 关键字nonlocal 使用作用域外层变量。 1234567891011In [5]: def outside(): ...: msg = 'Outside' ...: def inside(): ...: nonlocal msg ...: msg = 'Inside' ...: print(msg) ...: inside() ...: print(msg)In [6]: outside()InsideInside 常数时间复杂度程序运行时间由程序长度决定，与程序输入量无关。 类型检查&#39;4&#39;&lt;3 在 Python 3 中会引发 TypeError，而 Python 2 中则会返回 False。 print() 多个参数之间逗号隔开，会自动加空格 1234In [3]: print('a', 'b') a bIn [4]: print('a'+'b') ab 类型转换input() 会将输入值绑定到字符串类型上，整数或浮点数时需进行类型转换。 chapter 3求近似解穷举法从某一处开始，每次变化步长值，逐渐接近答案；步长太小，耗时长，步长太大，跳过正确答案，有点像梯度下降中的学习率参数。 二分查找一种应用于有序数组中的查找方法，每次查找都会将范围缩小一半，时间复杂度 $O(log_n)$。 牛顿法过 $(x_0, f(x_0))$，斜率为 $f’(x_0)$ 的直线等于 $0$ 处的 $x$ 作为下次逼近的 $x$， x := x - f(x)/f'(x)下降速度比二分法快。 浮点数的表示计算机在表示浮点数时，只能以二进制的形式存储，由于有效数字是有限的，用二进制表示浮点数只能在一定的精度范围内。 比如数对 $(101, -11)$ 代表 $5*2^{-3} = 0.625$； 数对 $(0011， -101) = 3 * 2^-5 = \frac{3}{32} = 0.09375$，在 4 位有效数字精度下，与 $0.1$ 相等。 chapter 4图灵完备：如果一个问题可以通过计算来解决，那么一定满足可以通过「数值」，「赋值语句」，「输入输出」，「比较语句」，「循环结构」来解决，也是编程语言的共性。 函数与作用域使程序清晰易读的关键是局部性，能够相对容易的扩展重用代码；函数通过分解和抽象使程序便捷；分解使各个子程序独立，方便重用；抽象使不需要被看见的细节忽略掉。 形参和实参这两个概念可能在看其他资料时会遇到，函数括号里面的叫形参，调用函数过程中的参数叫实参。这两个概念感觉很绕口。 参数位置关键字参数（实参中赋值）应放在位置参数后面，默认参数（形参中赋值）常常与关键字参数结合使用。 作用域 函数内部都有自己的命名空间，函数内的赋值语句并不影响作用域外的变量 即使是全局变量，只要在函数体内部任何一个地方有对象与该名称进行绑定，那么这个名称便是局部变量。 12345678910111213141516171819202122232425In [16]: def f(): ...: print(x)In [18]: def g(): ...: print(x) ...: x = 1In [19]: x = 3In [20]: f()3In [21]: x = 3In [22]: g()---------------------------------------------------------------------------UnboundLocalError Traceback (most recent call last)&lt;ipython-input-22-d65ffd94a45c&gt; in &lt;module&gt;()----&gt; 1 g()&lt;ipython-input-18-a762797aa895&gt; in g() 1 def g():----&gt; 2 print(x) 3 x = 1 4UnboundLocalError: local variable 'x' referenced before assignment g(x) 中的 x 是局部变量 函数的调用与栈帧有关（有助于理解递归） 函数调用时，会建立新的栈帧 栈帧记录了形参及函数体内被绑定到一个对象的变量名称（局部变量） 函数调用内部函数时，会建立新的栈帧 如果出现没有和函数体内任何对象绑定的名称时，解释器会搜索函数的作用域上层的栈帧，如果没有就会产生错误消息 函数运行结束后，栈帧消失，只能从栈顶弹出栈帧，不能从中间移除 函数是对象，可以被返回，绑定到新名称，可以利用新名称调用函数 函数体内部的函数调用结束后，会从原函数离开的位置继续执行 编写测试的重要性常常是“一本万利” 规范模块的规范化，抽象作用的体现，让其他人按照这个规范编写或使用，不用过于担心其他程序员在干的事或模块如何实现。 递归 递归只是让解决方案更清晰，并没有性能上的优势，使用循环，程序的性能可能更高，使用递归，程序可能更容易理解。 每个递归都有基线条件和递归条件；递归条件是函数调用自己；基线条件是函数不再调用自己，避免无限循环。 尾递归 函数每次调用前已经计算好结果，传入函数 return 语句中不能含有表达式，仅返回函数本身 效果与循环等价 分治算法 子问题比原始问题更容易解决比如回文字符串中，一个字母或空字符串直接可以判断是否是回文；快排中，只包含一个元素的列表或空列表，直接可以返回排序结果。 将子问题的解决方案组合解决初始问题比如回文字符串 and 连接的结果；快排中 + 连接结果。 全局变量尽量少的使用全局变量，会破坏程度的可读性和结构性，但有时全局变量很好用。 模块 importimport module 后，使用点标记法代表引用的名称是限定在模块内的，避免名称冲突造成的程序损害。所以，并不推荐 from module import * 这种导入方式，尤其在画图模块中，不要用 from pylab import *，用到什么模块导入什么，使程序可控，也使程序易读。比如 random.randint() 与 from pylab import * 后的 randint() 结果并不相同。 12345678910111213In [31]: from pylab import *In [32]: from random import randintIn [33]: max([randint(0, 5) for _ in range(100)])Out[33]: 5In [34]: from random import randintIn [35]: from pylab import *In [36]: max([randint(0, 5) for _ in range(100)])Out[36]: 4 reload在一个解释器中如果要重新载入模块，需要引入 imp 模块 12import impimp.reload(module) 否则，只会继续使用导入模块的初始版本。 文件 打开文件一定记住 f.close() 关闭文件，这是代码的严谨性 f.writelines(S)，将序列 S 中的每个元素作为单独的行写入 chapter 5结构化类型(str, tuple, list, dict, range)多重赋值 这些结构化类型都是遵循迭代器协议，可以用 next() 获取到下一项，可以被 for 循环遍历，但不一定都支持索引。 多重赋值（可以看作拆包的过程）与 for 循环遍历是一样的，也遵循迭代器协议。 迭代器协议 可以传递给 next() 函数，它将给出下一项，如果没有下一项，那么它将会引发 StopIteration 异常。 可以传递给 iter() 函数，它会返回一个自身的迭代器。 共性与差异 索引 seq[i]，(不适用于 dict) 长度 len(seq) 连接 “+” (不适用于 range 和 dict) 重复 n*seq (不适用于 range 和 dict) 切片 seq[start:end] (不适用于 dict) 包含 in / 不包含 not in 遍历 for ... in ... 元组 其中的单个元素可以是任意类型 只含 1 个元素的元组请这样书写成 (1, ) 元组不可变，可以作为字典的键 range range(start, end, step)step 是负数时, 如果 start &lt; end, 那么返回[]，例如 12In [1]: list(range(2, 6, -1))Out[1]: [] python 3 中 range 存储仅由起始值，结束值和步长决定，占用的空间与长度不成正比。 支持切片 12In [2]: range(6)[:3]Out[2]: range(0, 3) list可变性 变量仅仅是名称，是贴在变量上的标签 id() 可以检查两个对象是否相等 切片列表切片与数组切片的不同 列表切片返回浅拷贝，对于可变对象仍然是对原对象的引用。 12345678910In [3]: a = [[1], 2, 3, 4]In [4]: b = a[:2]In [5]: b[0].append(999)In [6]: b[1] = 888In [7]: aOut[7]: [[1, 999], 2, 3, 4] numpy.array() 的切片返回数组的视图对象，对视图对象修改会直接改变原对象。 12345678910In [8]: a = np.array([[1], 2, 3, 4])In [9]: b = a[:2]In [10]: b[0].append(999)In [11]: b[1] = 888In [12]: aOut[12]: array([list([1, 999]), 888, 3, 4], dtype=object) 数组 a 的第二个元素发生了改变。 可以参考 stack overflow 上的回答及翻译。同样，dict.keys()，dict.values()，dict.items() 均返回视图对象。 列表切片 list[start, end, step] 当 step 为负数时，切片方向从右向左，start 和 end 仍代表 index，当 start &lt; end 时，返回 []。 1234567In [14]: a = [1, 2, 3, 4, 5, 6]In [15]: a[:5:-2]Out[15]: []In [16]: a[5::-2]Out[16]: [6, 4, 2] 具体参考 https://segmentfault.com/q/1010000001636741/a-1020000001637729 如果在想要修改一个正在遍历的列表，可以使用切片 list[:]，list.copy()，copy.copy() 的方式。 123for i in l1[:]: if e1 in l2: l1.remove(e1) 如果上述列表 l1 中包含可变对象，可以用 copy.deepcopy() 进行复制。 浅拷贝和深拷贝 浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。常见的浅拷贝有：列表切片操作、工厂函数、对象的 copy() 方法、copy 模块中的 copy() 函数。 深拷贝：是指创建一个新的对象，然后递归的拷贝原对象所包含的子对象。深拷贝出来的对象与原对象没有任何关联。深拷贝只有一种方式：copy 模块中的 deepcopy() 函数。 123456789101112131415161718192021222324252627282930In [18]: import copyIn [19]: a = [[1, 2], [5, 6], [8, 9], 'a', 1]In [20]: b = copy.copy(a)In [21]: c = copy.deepcopy(a)In [22]: [id(i) for i in [a, b, c]]Out[22]: [279589512, 304508616, 279730760] In [21]: for x, y in zip(a, b): ...: print(id(x), id(y)) ...: ...:296105992 296105992 # 可变对象 id 相同278258440 278258440273903240 27390324031933640 31933640 # 不可变对象 id 相同1381362800 1381362800In [22]: for x, y in zip(a, c): ...: print(id(x), id(y)) ...: ...:296105992 273960456 # 可变对象 id 不同278258440 273916616273903240 27390042431933640 31933640 # 不可变对象 id 相同1381362800 1381362800 实质上，浅拷贝和深拷贝的不同仅仅是对可变对象来说。而对于数字、字符串以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。 在深拷贝中，当两个元素指向同一个不可变对象时，对其中一个赋值不会影响另外一个，并不会影响 a 和 b 的相互独立性，所以不可变对象的 id 仍然是相同的。具体请参考 https://songlee24.github.io/2014/08/15/python-FAQ-02/ 列表推导 可以有多个 if 或 for 语句 并不提倡写复杂的列表推导式 不会就地修改列表的操作list.count(item) list.index(item) 函数 函数是一等对象，具有类型，返回 &lt;type &#39;build-in_function_or_method&#39;&gt; 12In [24]: type(abs)Out[24]: builtin_function_or_method 可以作为赋值语句的右侧项，作为函数实参，作为列表中元素等。 高阶函数函数作为实参传入，在列表推导和 map 函数中可以使用。 map() map(func, 值集合)， 这里的值集合包括 range、set、dict、str、list、tuple都可以。 map(func, list1, list2, ...) 用法 func 是具有 n 个参数的函数，后面跟 n 个有序集合 list1 和 list2 长度必须相等 map(min, list1, list2) 返回对应元素的最小值 123456In [27]: ls1 = [1, 3, 5, 7]In [28]: ls2 = [0, 2, 6, 8]In [30]: list(map(min, ls1, ls2))Out[30]: [0, 2, 5, 7] lambda 匿名函数list1 和 list2 对应元素相加 12In [31]: list(map(lambda x, y: x+y, ls1, ls2))Out[31]: [1, 5, 11, 15] 字符串很多内置函数 rindex()，rfind()，rsplit() 等，split() 可以指定分隔次数。 dict 散列表 由散列函数和数组构成 散列函数需保持一致性，对于相同的输入，总是能映射到相同数组索引 查询，插入，删除时间复杂度均是 $O(1)$ 好的散列表要求 良好的散列函数，使数组中的值能均匀分开 具有较低的填装因子。 view 对象dict.keys()，dict.values()，dict.items() 均返回字典的 view 对象。类似数据库中的 view，当 dict 改变时，该 view 对象也跟着改变。 1234567891011In [39]: d = &#123;'a': 1, 'b': 2&#125;In [40]: k = d.keys()In [41]: list(k)Out[41]: ['a', 'b']In [42]: d['c'] = 3In [43]: list(k)Out[43]: ['a', 'b', 'c'] 键必须是可散列类型的对象 具有 __hash__ 方法，将对象映射为一个 int 值，而且由 __hash__ 返回的值在这个对象的生命周期中不变。 具有 __eq__ 方法，可以比较两个对象是否相等。python 中内置的所有不可变类型都是可散列的， 所有可变类型都是不可散列的 chapter 6测试 证明错误存在，而不是程序没有错误。 测试时注意边界条件，例如，列表对应的空列表，只含一个元素的列表 回归测试：不论修改多么小，都应该确保程序通过以前的测试 调试 编写程序时，尽量使程序错误显性和持续，又称防御性编程。防御性编程的例子是使用异常和断言。 避免隐形错误（给出一个错误答案，我们却意识不到）。 调试的方法：print 语句；记录实验过程；确定代码具体区域；减小测试数据量。 注意点 忘记使用()对函数类型对象进行调用 判断相等时用 == 还是 is? 如 np.nan。会有一篇博客介绍 np.nan，float(‘nan’)与 None 的区别。 想清楚“修复后的结果是否影响到其他程序”，注意保存修改前的版本 chapter 7处理异常的方法1234try: ...except (ValueError, TypeError) as msg: print(msg) 1234try: ...except: raise ValueError(msg) 使用断言确保中间值符合预期或函数返回可接受的值1assert boolean expression, argument 等价于 12if not expression: raise AssertionError(argument) argument 为输出的信息 chapter 8面向对象编程的关键是将对象看作数据和可以在数据上执行的方法的集合。 接口则建立了一个抽象边界，将程序的其他部分与实现类型对象的数据结构、算法和代码隔离开来。要使程序易于修改，控制程序的复杂度，需要分解和抽象。分解使程序具有结构，抽象则隐藏细节。 Python 中用类实现数据抽象。 抽象数据类型与类类定义创建一个 type 类型的对象，并将这个类的对象与一组 instance method 类型的对象关联起来。 123456789101112In [2]: class Insert(object): ...: def __init__(self): ...: self.vals = [] ...: def insert(self, e): ...: self.vals.append(e) ...:In [3]: type(Insert)Out[3]: typeIn [4]: type(Insert.insert)Out[4]: function __init__ 方法只要有类被实例化，便会调用 __init__ 方法。执行 s = Inset() 时，对使用新创建的对象 s 绑定到形参 self 上，而对象 vals 成为 Inset 实例的数据属性，每个实例化 Inset 类型的对象都有不同的列表 vals。 __init__ 与 __new__创建类实例并初始化时，__new__ 函数首先被调用，构造一个实例，接着 __init__ 函数在 __new__ 函数返回一个实例时被调用，并将这个实例作为 self 参数传入 __init__ 函数 123456789101112131415161718In [1]: class newStyleClass(object): ...: # In Python2, we need to specify the object as the base. ...: # In Python3 it's default. ...: ...: def __new__(cls): ...: print("__new__ is called") ...: return super(newStyleClass, cls).__new__(cls) ...: ...: def __init__(self): ...: print("__init__ is called") ...: print("self is: ", self) ...:In [2]: newStyleClass()__new__ is called__init__ is calledself is: &lt;__main__.newStyleClass object at 0x000000001009E588&gt;Out[2]: &lt;__main__.newStyleClass at 0x1009e588&gt; 只有在__new__返回一个新创建属于该类的实例时当前类的__init__才会被调用。如果 __new__ 函数返回一个已经存在的实例或不返回任何对象，__init__ 函数不会被调用。 方法属性与实例属性方法属性在类定义中，实例属性只有被实例化时才被创建 Insert.member 与 s.member 是不同的对象 12345678910111213141516In [6]: class Insert(object): ...: def __init__(self): ...: self.vals = [] ...: def insert(self, e): ...: self.vals.append(e) ...: def member(self, e): ...: return e in self.vals ...:In [7]: type(Insert.member)Out[7]: function In [9]: s = Insert() In [10]: type(s.member)Out[10]: method 类变量与实例变量1234In [11]: class Insert(object): ...: m = 'class variable' ...: def __init__(self): ...: self.vals = [] m 被称为类变量，vals 被称为实例变量。 实例化的对象可以访问类变量，但类不能直接访问实例变量。 123456789101112131415161718In [13]: s = Insert() In [15]: Insert.mOut[15]: 'class variable'In [16]: s.mOut[16]: 'class variable'In [17]: s.valsOut[17]: []In [18]: Insert.vals---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-18-96ca128642d5&gt; in &lt;module&gt;()----&gt; 1 Insert.valsAttributeError: type object 'Insert' has no attribute 'vals' __str__ 方法执行 print 命令时，会调用与待输出对象相关联的 __str__ 方法； 程序调用 str 函数将实例转换为字符串时，也会调用这个类的 __str__ 方法。 可散列所有自定义类的实例都可以作为字典的键。 如果没有 __hash__ 方法，那么散列值由 id() 给出，如果有 __hash__ 方法，这个方法必须保证对象的散列值是不变的。 使用类记录学生与教师尽量避免通过表达式直接访问类的数据属性（如实例变量）因为直接访问类的属性时，类如果发生变化（比如名字在需要时才提取），客户的代码便会出错。 而且 Python 允许程序直接从类的外部读取及改写实例变量和类变量，甚至允许为类的实例新建一个类定义中没有的实例变量。 所以推荐采用与实例关联的方法访问类的信息。如下的 get_name()和 get_lastname() 1234567891011121314151617181920class Pearson(object): def __init__(self, name): self.name = name try: last_blank = name.rindex(' ') self.lastname = name[last_blank + 1:] except: self.lastname = name self.birthday = None def get_name(self): return self.name def get_lastname(self): return self.lastname def __lt__(self, other): if self.lastname == other.lastname: return self.name &lt; other.name return self.lastname &lt; other.lastname __lt__ 重载了 &lt; 操作符 self.name &lt; other.name 是 self.name.__lt__(other.name) 的简写，调用哪种 __lt__ 方法是由表达式的第一个参数决定的，如 p1 &lt; p4，会调用 p1 类型中关联的 __lt__ 方法，如果 p1 中的 __lt__ 方法涉及的属性在 p4 中没有，就会导致 AttributeError。 如果 plist 是 Pearson 类组成的列表，plist.sort() 会调用 Pearson 类中的 __lt__ 方法。 继承 子类继承超类的属性 可以添加新的属性 子类可以覆盖超类的属性 实现每个实例具有唯一 idNum 的方法1234567class MITPearson(Pearson): next_id_num = 0 def __init__(self, name): Pearson.__init__(self, name) self.id_num = MITPearson.next_id_num MITPearson.next_id_num += 1 通过设置类变量 next_id_num，每次创建实例时，并不会创建类变量的实例，这样保证每个实例都有唯一的 id_num。 type() 与 isinstance() 的不同type() 实例返回当前所属的类 isinstance() 则当第一个参数是第二个参数中的一个实例时，返回 True，当然，绑定在子类上的实例也被认为是父类的一个实例。 12345678910111213141516171819202122232425In [23]: class Student(MITPearson): ...: pass ...: ...: ...: class UG(Student): ...: def __init__(self, name, class_year): ...: MITPearson.__init__(self, name) ...: self.year = class_year ...: ...: def get_class(self): ...: return self.year ...: ...: ...: class Grad(Student): ...: pass ...: In [24]: p6 = UG('Billy Beaver', 1984)In [25]: isinstance(p6, Student)Out[25]: TrueIn [26]: type(p6)Out[26]: __main__.UG 引入中间类 Student 可以使在利用 isinstance() 判断新的类型是否属于 Student 时，修改的代码量减少。 隐藏信息如果一个属性名称以 __ 开头，不以 __ 结尾时，这个属性在类外是不可见的。 12345678910111213141516171819202122232425262728293031In [27]: class infoHiding(object): ...: def __init__(self): ...: self.visible = 'look at me' ...: self.__alsovisible__ = 'look at me too' ...: self.__invisible = 'Can\'t look at me' ...: ...: def printVisible(self): ...: print(self.visible) ...: ...: def printInvisible(self): ...: print(self.__invisible) ...: ...: def __printInvisible(self): ...: print(self.__invisible) ...: ...: def __printInvisible(self): ...: print(self.__invisible) ...:In [28]: test = infoHiding()In [29]: test.__alsovisible__Out[29]: 'look at me too'In [30]: test.__invisible---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-30-37122f35b3ec&gt; in &lt;module&gt;()----&gt; 1 test.__invisibleAttributeError: 'infoHiding' object has no attribute '__invisible' 子类想要使用超类中的隐藏属性时，会引发 AttributeError 异常，因为这点，Python 程序员不愿意使用 __ 隐藏属性。 1234567891011121314151617In [31]: class subClass(infoHiding): ...: def __init__(self): ...: print('from subclass', self.__invisible) ...:In [32]: testsub = subClass()---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-32-c6b153745dd5&gt; in &lt;module&gt;()----&gt; 1 testsub = subClass()&lt;ipython-input-31-0efe6d9a7993&gt; in __init__(self) 1 class subClass(infoHiding): 2 def __init__(self):----&gt; 3 print('from subclass', self.__invisible)AttributeError: 'subClass' object has no attribute '_subClass__invisible' 生成器为避免信息隐藏而产生的低效算法，比如在返回一个庞大的列表时，常采用生成器的做法。 如果一个函数中包含 yield 语句，这个函数则会被看作生成器，一般与 for 语句一起使用。 for 循环进行一次迭代时，执行到 yield 语句时，会返回 yield 语句表达式中的值。在下一次迭代时，会紧接着 yield 语句继续进行，直到代码运行完毕或执行到 return 语句，循环结束。 每次生成一个值，效率更高。不需要再创建新的空间去储存庞大的列表。 12345678910111213In [33]: def get_students(self): ...:if self.sorted is False: self.students.sort() self.sorted = True for s in self.students: yield sIn [35]: for s in six_hundred.get_students(): ...: print(s) ...:Jane DoeJohn DoeBilly BucknerBucky F. Dent 关于生成器，将会有一篇博客进行详细的说明。 参考文章： https://www.jianshu.com/p/14b8ebf93b73]]></content>
      <categories>
        <category>Books</category>
        <category>python 编程导论</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用 Hexo，GitHub page 和 Coding.net 搭建博客]]></title>
    <url>%2Farchives%2Fbe879d03.html</url>
    <content type="text"><![CDATA[安装环境 Node.js Git 安装步骤初次运行 Gitgit config --global user.email &quot;Your email &quot; git config --global user.name &quot;Your name&quot; Hexo 常用命令123456hexo g # 生成网页hexo s # 启动本地服务hexo d # 将网站递交到远程仓库hexo s -g # 组合命令，生成网页并启动本地服务hexo d -g # 组合命令，生成网页并递交到远程仓库hexo clean # 清除刚生成的网页 安装 Hexo 及相关组件安装 Hexo新建 Blog 目录，必须是空目录，否则 hexo init 报错。 npm install -g hexo-cli 生成 Hexo 项目12hexo init # 初始化 blog 目录npm install hexo-deployer-git --save # 安装上传组件 本地调试运行12hexo ghexo s 此时，在浏览器输入「localhost:4000」可以看到 Hello world 页面。 目录结构12345678910111213├── .deploy├── public├── scaffolds| ├── draft.md| ├── page.md| └── post.md├── scripts├── source| ├── _drafts| └── _posts├── themes├── _config.yml└── package.json scaffolds 目录 新建页面的模板，执行新建命令时，是根据这里的模板页来完成的。 主要配置文件 _config.yml 站点配置文件 themes/*/_config.yml 主题配置文件 安装 next 主题 next 官方网站上下载 git clone https://github.com/iissnan/hexo-theme-next themes/next 新版本的 NexT 6.x 还对部分功能的使用做了优化。把很多功能比如字数统计，顶部进度条等放到了主分支里。具体可以参考Hexo NexT 主题6.x版本的使用配置与美化。 修改站点配置文件中的 themes 字段为 next 使用组合命令运行 hexo s -g 添加文章 添加文章 hexo new &quot;文章名&quot; 添加内容 在 blog/source/_post 文件夹下有文章名.md文件。 12345---title: 利用 Hexo 和 Gitpage 搭建博客date: 2018-09-16 10:47:55tags: Hexo--- 将要写的内容添加到 --- 下 本地提交 hexo s -g 刷新浏览器可以看到刚写的文章了 利用 Github Pages 实现远程访问新建仓库 在 github 中创建新仓库，仓库名字格式「username.github.io」，勾选用初始化 README 在 Setting 页面的 Github Pages 中，Source 显示为 master branch 绑定 SSH Key 在本地找到 id_rsa.pub 文件，进入 github，Settings $\rightarrow$ SSH and GPG keys $\rightarrow$ New SSH key，将 ssh key 粘贴进去 ssh -T git@github.com验证 ssh key 是否添加成功 部署网站 修改 Blog 目录下站点配置文件，repository 改为自己的仓库名 1234deploy: type: git repository: git@github.com:hotheat/hotheat.github.io.git branch: master repository 使用 ssh 方式进行复制 每个冒号后面都有空格 执行命令 12npm install hexo-deployer-git --savehexo d -g # 部署网站 在浏览器中输入「hotheat.github.io」即可访问 有时本地文件会出错，执行 hexo clean 即可清除 public 文件夹，再重新提交 绑定域名 注册域名，推荐 godaddy.com 和「万网」，开始时注册的腾讯云域名，在同时绑定 coding.net 后子域名负载均衡超限需要花钱 添加主机记录 www， 记录类型 CNAME，记录值仓库名：hotheat.github.io，保存 再添加主机记录 @， 记录类型 CNAME，记录值仓库名：hotheat.github.io，保存 Github setting 里 Github page 页面中填入新的域名。保存后，此时仓库中会多出 CNAME 文件，里面是域名信息 下次执行 hexo d 时，CNAME 文件会被删掉。需要本地在 source 目录中新建 CNAME 文件，里面写上域名地址 「www.hotheat.top」，下次执行 hexo d 时会自动复制到 post 文件夹下 同时部署到 Coding.net因为 Github 屏蔽了百度的爬虫，国内流量的解析到 Coding，国外的解析到 Github。Coding.net 在国内的访问速度也比较快。 注册 Coding.net $\rightarrow$ 绑定 ssh 证书 $\rightarrow$ 站点配置文件中配置 deploy 方式（添加内容如下） $\rightarrow$ hexo d -g 推送 $\rightarrow$ 绑定域名（主机记录同样填写 www 和 @） 123456deploy: type: git repository: github: git@github.com:hotheat/hotheat.github.io.git coding: git@git.coding.net:HotHeat/hotheat.coding.me.git branch: master Coding.net 绑定腾讯云后免费升级到银牌会员，可以像 Github Page 一样对自定义域名进行绑定了 在 Coding Pages 里申请 SSL 证书时，会同时解析国外线路和国内线路，当解析到国外线路时，申请证书无法通过，需要将 github.io 线路暂停，待申请通过后再启用。DNS 有缓存，修改后约有 5 分钟生效时间。在域名解析时尤其不能心急 不需要选中「强制 HTTPS 访问」 coding.me 跳转页的广告删除请参考文章 http://stevenshi.me/2017/12/22/hexo-codingNet/。再点击头像右下角，切换旧版，勾选 已放置 Hosted By Coding Pages 提交审核。 next 主题优化配置设置语言和网站信息在站点配置文件中 Site 模块设置 12345678# Sitetitle: Hotheatsubtitle: Hotheat Homedescription: Studying and Processingkeywords:author: Hotheatlanguage: zh-Hans ## 中文timezone: 菜单设置在主题配置文件 Menu 中，将需要的菜单取消注释。相应的按钮会在网站上显示。 123456789menu: home: / || home tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive about: /about/ || user #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 分类和标签分类 执行命令 hexo new page categories 在提示生成的 index.md 中添加 type: “categories” 12345---title: categoriesdate: 2018-09-16 13:11:19type: "categories"--- 在要添加分类的文章 Header 中添加 categories 属性值。一篇文章只能属于一个分类，如果在下一行添加 -xxx，那么 -xxx 将属于上一个分类的子类 123456title: 利用 Hexo 和 Gitpage 搭建博客date: 2018-09-16 10:47:55tags: - Hexocategories: - 使用教程 标签 标签和分类同样的设置，执行 hexo new page tags 在 index.md 中添加 type: “tags” 123456---title: tagsdate: 2018-09-16 14:24:24type: "tags"comments: false--- 在要添加分类的文章 Header 中添加 tags 属性值。与 categories 不同，一篇文章可以有多个 tags 新建文章时自动生成 categories在 scaffold/post.md 中的 tags: 上面加入 categories:，在 hexo new &quot;文章名&quot;时，就可以自动添加 categories 项。 关于（About） 执行 hexo new page about 在主题配置文件中将 About 取消注释，在 source\about\index.md 编辑 About 内容 LocalSearch 搜索 安装 hexo-generator-searchdb npm install hexo-generator-searchdb --save 编辑站点配置文件，添加 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件 123# Local searchlocal_search: enable: true algolia search参考文章 hexo Next 主题配置—-补充—-Algolia 搜索 文章首页只显示部分内容在 .md 文章内容中加上 &lt;!--more--&gt;，首页展示的就是 &lt;!--more--&gt; 之前的文字 。 修改文章内链接文本样式修改文件 themes\next\source\css\_common\components\post\post.styl，在末尾添加如下 css 样式 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 设置代码高亮在主题配置文件中修改，共有 5 款主题 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night 12def test(): print(a * 2) 网站字数统计主题配置文件中修改 123456post_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true 访问统计主题配置文件修改 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class="fa fa-user"&gt;&lt;/i&gt; site_uv_footer: 总访客 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class="fa fa-eye"&gt;&lt;/i&gt; site_pv_footer: 总访问 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class="fa fa-file-o"&gt;&lt;/i&gt; page_pv_footer: 文章阅读数 搜索引擎优化（SEO）验证网站所有权谷歌进入 Google Search Console，选择 HTML 标记，将字段复制到主题配置文件 google_site_verification 位置。 1google_site_verification: SHONCNF9wVKYUwUP... 百度 登录百度站长平台，选择 CNAME 验证 主机记录添加 rnv4n…，记录值 ziyuan.baidu.com，保存。 安装 sitemap 插件12npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save 修改站点配置文件为博客首页 12url: https://www.hotheat.top/root: / 站点配置文件中添加 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 执行 hexo d -g 后， public 目录下发现生成了 sitemap.xml和baidusitemap.xml 表示配置成功了。 提交站点地图谷歌 谷歌站点地图找到抓取$$\rightarrow$$站点地图，输入 sitemap.xml 这步很重要！很重要！ 提交站点地图之后，点击Google 抓取方式，点击抓取$$\rightarrow$$请求编入索引 百度 点击 数据引入$$\rightarrow$$链接提交，在 自动提交中选择 sitemap，输入域名加 baidusitemap.xml 即可 点击抓取诊断，选择PC UA点击抓取，显示抓取成功 next 主题中部署自动推送代码，主题配置文件中修改 baidu_push: true 其他配置添加 robots.txt在 source 目录下增加 robots.txt 文件 123456789101112131415User-agent: *Allow: /Allow: /archives/Allow: /categories/Allow: /tags/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://www.hotheat.top/sitemap.xmlSitemap: https://www.hotheat.top/baidusitemap.xml URL 持久化安装 hexo-abbrlinknpm install hexo-abbrlink --save 修改站点配置文件1234permalink: archives/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 这步设置也很重要，避免了文章名称经 URL 编码后过长（超过 50 个字符）引起 Gitalk 的 Error: Validation Failed.，而网上多采用对 label 进行 md5 加密的方法解决。 添加 nofollow 标签 修改 themes\next\layout\_partials\footer.swig 12&#123;&#123; __('footer.powered', '&lt;a class="theme-link" href="http://hexo.io" rel="external nofollow"&gt;Hexo&lt;/a&gt;') &#125;&#125;&lt;a class="theme-link" href="https://github.com/iissnan/hexo-theme-next" rel="external nofollow"&gt; 修改 themes\next\layout\_macro\sidebar.swig 1&lt;a href="&#123;&#123; link &#125;&#125;" title="&#123;&#123; name &#125;&#125;" target="_blank" rel="external nofollow"&gt;&#123;&#123; name &#125;&#125;&lt;/a&gt; 注意: 使用hexo g生成文章的时候，当文章中有}}时,且这两个括号未被代码块包含，解析会出问题。这部分并没有进行设置。 页面关键字优化 编辑 themes\next\layout\index.swig 123&#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;改为：&#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;% endif %&#125;&#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125; 站点配置文件中添加 keywords 和 description Next 主题自带 SEO 优化修改主题配置文件 seo: true 支持 Latex 安装 Mathjax 插件 npm install hexo-math --save 更换 markdown 渲染引擎 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 解决语义冲突（这步我没做） 进入node_modules\kramed\lib\rules\inline.js，修改 11 行 12//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 修改 20 行 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 修改主题配置文件，将 enable 修改为 true，更换 url 123456# MathJax Supportmathjax: enable: true per_page: true #cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML 打开 mathjax 开关，在文章的 Front-matter 中打开 mathjax 开关。类似 categories，在 scaffold/post.md 中添加 mathjax: true，可以新建文章时自动打开 mathjax 开关。 12345title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:mathjax: true Gitalk 评论系统注册 在 GitHub 上注册新应用，https://github.com/settings/applications/new，这里 Homepage 和回调网址都用自定义域名 https://www.hotheat.top。有自定义域名的填写自定义域名，没有的填写仓库地址链接。 记录 Cliet ID 和 Client Secret gitalk 配置gitalk 是这样写，而不是这样 gittalk。 新建 themes\next\layout\_third-party\comments\gitalk.swig，并添加内容 12345678910111213141516&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"&gt; &lt;script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: location.pathname, distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; themes\next\layout\_partials\comments.swig 添加内容，与上一个 elseif 同级 12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id="gitalk-container"&gt;&lt;/div&gt; themes\next\layout\_third-party\comments\index.swig 添加内容 1&#123;% include 'gitalk.swig' %&#125; 新建 themes\next\source\css\_common\components\third-party\gitalk.styl，添加内容 1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; themes\next\source\css\_common\components\third-party\third-party.styl 最后一行添加 1@import "gitalk"; 主题配置文件添加 12345678gitalk: enable: true ClientID: *** #填写步骤一中申请应用后网页上的的对应参数 ClientSecret: *** #同上 repo: gitalk-comments #填写你准备用来存放评论的仓库，只需要写名称，一定确保已经创建该仓库 githubID: hotheat #你的 Github ID adminUser: hotheat # 你的 Github ID或者其他有权限的管理，官方注释"support multiple admins split with comma, e.g. foo,bar" distractionFreeMode: true about/index.md、categories/index.md、tags/index.md的 header 里加上 comments: false，在关于、分类和标签中禁用评论。 出现问题 点击 使用 GitHub 登录后跳转到主页 进入控制台发现报错 [VM646:1 GET https://api.github.com/user **401** (Unauthorized)]，怀疑是开始注册时填写的回调网站 hotheat.github.io有问题。 解决：将注册时的 callback URL改成 https://www.hotheat.top，在主题配置文件中修改相应的 ClientID 和 ClientSecret，而 repo 不用改，就可以点击登录评论。 Chrome 中点击评论按钮捕捉不到 mousedown 事件，Firefox 则可以正常添加评论 报错信息： 12Uncaught TypeError: Cannot read property 'keydown' of undefinedUncaught TypeError: Cannot read property 'mousedown' of undefined 解决：最终测试发现，是 Chrome 浏览器中 Tampermonkey（油猴）插件中「网页限制解除」脚本导致的。debug 过程满满地都是泪啊~ 部署、配置和优化主要参考： https://www.bgrc.fun/posts/hexo/index.html#more https://www.bgrc.fun/posts/next/index.html https://www.jianshu.com/p/c20bb9df1867 https://www.jianshu.com/p/86557c34b671 https://blog.csdn.net/lewky_liu/article/details/82432045 支持 Latex 参考：https://www.jianshu.com/p/d95a4795f3a8 评论系统参考https://asdfv1929.github.io/2018/01/20/gitalk/ https://iochen.com/2018/01/06/use-gitalk-in-hexo/ 折腾酷炫的网站意义不大，最主要的是多写文章。]]></content>
      <categories>
        <category>使用教程</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Typora 使用]]></title>
    <url>%2Farchives%2Fedddb6f8.html</url>
    <content type="text"><![CDATA[目录 [TOC] [toc] + Enter 直接生成目录 对文字的特殊标注标题1-6级标题（#~######） 下划线麻烦的下划线 Control + u \ 强调我是重要的 Control + b 斜体斜体 Control + i 删除不要啦 Shift + Alt + 5 ~~ ~~ 方框框起来 ` ` 引用今天吃啥呀1 1. 吃鱼吧 &#8617; 通过培养试验研究了有机酸对铅、镉的毒害影响，结果表明柠檬酸对铅2，酒石酸对镉有较明显解毒作用3。用逐步提取法研究萝卜根叶内重金属存在的化学形态 ，有机酸处理并不影响各形态铅的优势顺序 ，但各形态铅的浓度或相对百分率发生了变化。 2. 陈苏. 污染土壤中镉、铅的活化及植物有效性研究[D]. 中国科学院沈阳应用生态研究所, 2007. &#8617; 3. 陈英旭, 林琦, 陆芳,等. 有机酸对铅、镉植株危害的解毒作用研究[J]. 环境科学学报, 2000, 20(4):467-472. &#8617; 1234通过培养试验研究了有机酸对铅、镉的毒害影响，结果表明柠檬酸对铅[^1]，酒石酸对镉有较明显解毒作用[^2]。用逐步提取法研究萝卜根叶内重金属存在的化学形态 ，有机酸处理并不影响各形态铅的优势顺序 ，但各形态铅的浓度或相对百分率发生了变化。[^1]: 陈苏. 污染土壤中镉、铅的活化及植物有效性研究[D]. 中国科学院沈阳应用生态研究所, 2007.[^2]: 陈英旭, 林琦, 陆芳,等. 有机酸对铅、镉植株危害的解毒作用研究[J]. 环境科学学报, 2000, 20(4):467-472. 引文序号 [^num]来源 [^num]: 注释 高亮==高亮== == == 需在偏好设置中事先勾选 角标上角标： x^2^ # ^ ^ 下角标：H~2~O # ~ ~ $x^2 + x^1$ # Latex 公式 转义# \ 代表转义 List有序 神奇动物在哪里 死侍2 八罗汉 无序- * + 都可以使用 海伯利安 阿西莫夫 茨威格 Todo list [ ] 背单词 [ ] 口语 - [ ] + Content TableControl + t 生成表格 Control + Enter 自动添加一行 支持行列拖拽移动 支持网页表格复制粘贴，且保留超链接 车次 出发站 到达站 出发时间 到达时间 历时 商务座 特等座 一等座 二等座 高级 软卧 软卧 动卧 硬卧 软座 硬座 无座 其他 备注 D3126深圳北**杭州东07:0017:25**10:25当日到达 — 有 有 — — — — — — 有 — 预订 D3108深圳北**杭州东08:2219:01**10:39当日到达 — 有 有 — — — — — — 有 — 预订 D2284深圳北**杭州东08:4619:35**10:49当日到达 — 有 有 — — — — — — 有 — 预订 D2282深圳北**杭州东09:1819:58**10:40当日到达 — 10 有 — — — — — — 有 — 预订 D2286深圳北**杭州东09:5421:03**11:09当日到达 — 有 有 — — — — — — 有 — 预订 D2288深圳北**杭州东10:4221:38**10:56当日到达 — 20 有 — — — — — — 有 — 预订 D3112深圳北**杭州东11:2422:01**10:37当日到达 — 有 有 — — — — — — 有 — 预订 T212深圳**杭州东13:0805:39**16:31次日到达 — — — — 无 — 有 — 有 有 — 预订 T102深圳**杭州东16:0209:13**17:11次日到达 — — — — 无 — 有 — 有 有 — 油条 面条 早餐 牛肉 午餐 分割线—— 或 * 插入图片直接拖入或七牛云图床或网络地址图片 Control + Shift + i 支持 HTML img 标签，调整图片大小 调整图片比例的其他方法 打开 Typora 主题文件夹下的 ‘github.css’ 文件 在文件最后添加以下代码后 img { zoom: 50%; } 链接Control + k 按住 Control 可访问超链接 typora 使用方法 锚点：文本内链接到第一章 数学公式行间公式 $$$$ 回车 \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-u)^2}{2\sigma^2})行内公式用 $包含 代码Shift + Control + ` 或 Enter12345``` pythondef test(): for i in range(arr): print(abs(i)) 表情 emoji:happy: : + 特殊心情的词语 引用> >> 二级引用 按两次回车退出上次引用 这部分是引用 二级引用 流程图Flowchart12345678910111213```flowst=&gt;start: Start:&gt;http://www.google.com[blank]e=&gt;end:&gt;http://www.google.comop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yesor No?:&gt;http://www.google.comio=&gt;inputoutput: catch something...st-&gt;op1-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op1``` Mermaid123456789graph TD; 开始--&gt;条件B; 条件A--&gt;条件C; 条件B--&gt;条件D; 条件C--&gt;条件D; 条件A--&gt;条件D; 开始--&gt;条件C; 开始--&gt;条件A; 条件D--&gt;结束; Sequence12345Alice-&gt;&gt;John: Hello John, how are you?Note right of John: Rational thoughtsJohn--&gt;&gt;Alice: Great!John-&gt;&gt;Bob: How about you?Bob--&gt;&gt;John: Jolly good! Gantt123456789101112131415gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d 主题访问：https://theme.typora.io/，并下载好相应主题。 在偏好设置中找到主题文件夹，将下载好的主题 CSS 文件复制进去，重启 Typora，选择主题。 参考资料Typora 编辑器 ———— 书写即为美学 https://www.bilibili.com/video/av18508430/ https://www.bilibili.com/video/av20190823/]]></content>
      <categories>
        <category>使用教程</category>
      </categories>
      <tags>
        <tag>Typora</tag>
      </tags>
  </entry>
</search>
